name: Universal Clustering Model Evaluation Component
description: |
  Comprehensive evaluation metrics for clustering results designed for TEST DATA evaluation.
  Supports internal metrics (no ground truth needed) and external metrics (with ground truth).
  Calculates Silhouette Score, Davies-Bouldin Index, Calinski-Harabasz Score, ARI, AMI, and per-cluster statistics.

inputs:
  - name: data
    type: Data
    description: 'Data used for clustering (CSV or NPY, typically TEST data)'
  - name: labels
    type: Data
    description: 'Cluster labels/predictions (NPY or CSV from inference component)'
  - name: true_labels
    type: Data
    description: 'Ground truth labels for external metrics (NPY or CSV, optional)'
    optional: true
  - name: algorithm
    type: String
    description: 'Algorithm name for metadata'
    default: 'Unknown'
  - name: dataset_name
    type: String
    description: 'Dataset identifier (test, train, validation, etc.)'
    default: 'test'
  - name: per_cluster
    type: String
    description: 'Calculate detailed per-cluster statistics (true/false)'
    default: 'false'

outputs:
  - name: evaluation_results
    type: Data
    description: 'Complete evaluation metrics and statistics (JSON format)'
  - name: summary_report
    type: Data
    description: 'Human-readable evaluation summary (TXT format)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import warnings
        import numpy as np
        import pandas as pd
        from pathlib import Path
        
        warnings.filterwarnings('ignore')
        
        from sklearn.metrics import (
            silhouette_score,
            silhouette_samples,
            davies_bouldin_score,
            calinski_harabasz_score
        )
        
        try:
            from sklearn.metrics import (
                adjusted_rand_score,
                adjusted_mutual_info_score,
                normalized_mutual_info_score
            )
            EXTERNAL_METRICS_AVAILABLE = True
        except ImportError:
            EXTERNAL_METRICS_AVAILABLE = False
            print("Warning: sklearn version too old for external metrics (ARI, AMI)")
        
        
        def load_data(data_path):
            print('')
            print('Loading data from:', data_path)
            
            try:
                if data_path.endswith('.csv'):
                    df = pd.read_csv(data_path)
                    data = df.values
                elif data_path.endswith('.npy'):
                    data = np.load(data_path)
                else:
                    raise ValueError("Unsupported format. Use .csv or .npy")
                
                print('Loaded successfully')
                print('Shape:', data.shape[0], 'samples x', data.shape[1], 'features')
                
                return data
                
            except Exception as e:
                print('Error loading data:', str(e))
                raise
        
        
        def load_labels(labels_path):
            print('')
            print('Loading labels from:', labels_path)
            
            try:
                if labels_path.endswith('.npy'):
                    labels = np.load(labels_path)
                elif labels_path.endswith('.csv'):
                    df = pd.read_csv(labels_path)
                    if len(df.columns) == 1:
                        labels = df.iloc[:, 0].values
                    else:
                        labels = df.iloc[:, -1].values
                else:
                    raise ValueError("Unsupported format. Use .npy or .csv")
                
                print('Loaded successfully')
                print('Length:', len(labels))
                
                return labels
                
            except Exception as e:
                print('Error loading labels:', str(e))
                raise
        
        
        def validate_inputs(data, labels):
            print('')
            print('Validating inputs')
            
            if len(data) != len(labels):
                raise ValueError('Data and labels size mismatch: data has ' + str(len(data)) + ' samples, labels has ' + str(len(labels)))
            
            print('Shapes match:', len(data), 'samples')
            
            if not np.isfinite(data).all():
                n_invalid = (~np.isfinite(data)).sum()
                raise ValueError('Data contains ' + str(n_invalid) + ' NaN or Inf values')
            
            print('No NaN or Inf in data')
            
            unique_labels = np.unique(labels)
            n_clusters = len(unique_labels[unique_labels != -1])
            n_noise = np.sum(labels == -1)
            
            print('Found', n_clusters, 'clusters')
            if n_noise > 0:
                noise_pct = n_noise / len(labels) * 100
                print('Found', n_noise, 'noise points (' + str(round(noise_pct, 1)) + '%)')
            
            return n_clusters, n_noise
        
        
        def calculate_basic_statistics(labels):
            print('')
            print('[1/5] Calculating basic statistics')
            
            unique_labels = np.unique(labels)
            n_clusters = len(unique_labels[unique_labels != -1])
            n_noise = np.sum(labels == -1)
            
            cluster_sizes = {}
            for label in unique_labels:
                count = np.sum(labels == label)
                label_key = "noise" if label == -1 else 'cluster_' + str(int(label))
                cluster_sizes[label_key] = int(count)
            
            valid_sizes = [v for k, v in cluster_sizes.items() if k != 'noise']
            
            stats = {
                'n_samples': int(len(labels)),
                'n_clusters': int(n_clusters),
                'n_noise': int(n_noise),
                'noise_percentage': float(n_noise / len(labels) * 100) if len(labels) > 0 else 0.0,
                'cluster_sizes': cluster_sizes,
                'min_cluster_size': int(min(valid_sizes)) if valid_sizes else 0,
                'max_cluster_size': int(max(valid_sizes)) if valid_sizes else 0,
                'avg_cluster_size': float(np.mean(valid_sizes)) if valid_sizes else 0.0,
                'std_cluster_size': float(np.std(valid_sizes)) if valid_sizes else 0.0
            }
            
            print('Clusters:', n_clusters)
            print('Noise:', n_noise, '(' + str(round(stats['noise_percentage'], 1)) + '%)')
            if n_clusters > 0:
                print('Avg cluster size:', round(stats['avg_cluster_size'], 1), '+/-', round(stats['std_cluster_size'], 1))
            
            return stats
        
        
        def calculate_internal_metrics(data, labels):
            print('')
            print('[2/5] Calculating internal metrics')
            
            valid_mask = labels != -1
            data_valid = data[valid_mask]
            labels_valid = labels[valid_mask]
            
            n_clusters = len(np.unique(labels_valid))
            
            metrics = {}
            
            if n_clusters < 2:
                print('Need at least 2 clusters for metrics (found', n_clusters, ')')
                return {
                    'silhouette_score': None,
                    'davies_bouldin_score': None,
                    'calinski_harabasz_score': None,
                    'reason': 'Only ' + str(n_clusters) + ' cluster(s) found'
                }
            
            if len(labels_valid) < n_clusters:
                print('Not enough samples for metrics')
                return {
                    'silhouette_score': None,
                    'davies_bouldin_score': None,
                    'calinski_harabasz_score': None,
                    'reason': 'Insufficient samples'
                }
            
            try:
                sil_score = silhouette_score(data_valid, labels_valid)
                metrics['silhouette_score'] = float(sil_score)
                
                if sil_score > 0.7:
                    interp = 'Excellent'
                elif sil_score > 0.5:
                    interp = 'Good'
                elif sil_score > 0.25:
                    interp = 'Fair'
                else:
                    interp = 'Poor'
                
                print('Silhouette Score:', round(sil_score, 4), '(' + interp + ')')
            except Exception as e:
                print('Could not calculate Silhouette Score:', str(e))
                metrics['silhouette_score'] = None
            
            try:
                db_score = davies_bouldin_score(data_valid, labels_valid)
                metrics['davies_bouldin_score'] = float(db_score)
                print('Davies-Bouldin Score:', round(db_score, 4), '(lower is better)')
            except Exception as e:
                print('Could not calculate Davies-Bouldin Score:', str(e))
                metrics['davies_bouldin_score'] = None
            
            try:
                ch_score = calinski_harabasz_score(data_valid, labels_valid)
                metrics['calinski_harabasz_score'] = float(ch_score)
                print('Calinski-Harabasz Score:', round(ch_score, 4), '(higher is better)')
            except Exception as e:
                print('Could not calculate Calinski-Harabasz Score:', str(e))
                metrics['calinski_harabasz_score'] = None
            
            return metrics
        
        
        def calculate_external_metrics(pred_labels, true_labels):
            print('')
            print('[3/5] Calculating external metrics (vs ground truth)')
            
            if not EXTERNAL_METRICS_AVAILABLE:
                print('External metrics unavailable (sklearn version too old)')
                return {'available': False, 'error': 'sklearn version too old'}
            
            valid_mask = pred_labels != -1
            pred_valid = pred_labels[valid_mask]
            true_valid = true_labels[valid_mask]
            
            if len(pred_valid) == 0:
                print('All points marked as noise, cannot calculate external metrics')
                return {'available': False, 'error': 'All points are noise'}
            
            metrics = {'available': True}
            
            try:
                ari = adjusted_rand_score(true_valid, pred_valid)
                metrics['adjusted_rand_index'] = float(ari)
                
                if ari > 0.9:
                    interp = 'Excellent match'
                elif ari > 0.7:
                    interp = 'Good match'
                elif ari > 0.5:
                    interp = 'Moderate match'
                else:
                    interp = 'Poor match'
                
                print('Adjusted Rand Index:', round(ari, 4), '(' + interp + ')')
            except Exception as e:
                print('Could not calculate ARI:', str(e))
                metrics['adjusted_rand_index'] = None
            
            try:
                ami = adjusted_mutual_info_score(true_valid, pred_valid)
                metrics['adjusted_mutual_info'] = float(ami)
                print('Adjusted Mutual Info:', round(ami, 4))
            except Exception as e:
                print('Could not calculate AMI:', str(e))
                metrics['adjusted_mutual_info'] = None
            
            try:
                nmi = normalized_mutual_info_score(true_valid, pred_valid)
                metrics['normalized_mutual_info'] = float(nmi)
                print('Normalized Mutual Info:', round(nmi, 4))
            except Exception as e:
                print('Could not calculate NMI:', str(e))
                metrics['normalized_mutual_info'] = None
            
            return metrics
        
        
        def calculate_cluster_quality(data, labels):
            print('')
            print('[4/5] Calculating cluster quality metrics')
            
            valid_mask = labels != -1
            data_valid = data[valid_mask]
            labels_valid = labels[valid_mask]
            
            if len(labels_valid) == 0:
                print('No valid clusters (all noise)')
                return {'error': 'All points are noise'}
            
            metrics = {}
            
            intra_distances = []
            for label in np.unique(labels_valid):
                cluster_points = data_valid[labels_valid == label]
                if len(cluster_points) > 1:
                    center = np.mean(cluster_points, axis=0)
                    distances = np.linalg.norm(cluster_points - center, axis=1)
                    intra_distances.extend(distances)
            
            if intra_distances:
                metrics['mean_intra_cluster_distance'] = float(np.mean(intra_distances))
                metrics['std_intra_cluster_distance'] = float(np.std(intra_distances))
                metrics['max_intra_cluster_distance'] = float(np.max(intra_distances))
                print('Mean intra-cluster distance:', round(metrics['mean_intra_cluster_distance'], 4))
            
            unique_labels = np.unique(labels_valid)
            if len(unique_labels) > 1:
                centers = []
                for label in unique_labels:
                    cluster_points = data_valid[labels_valid == label]
                    centers.append(np.mean(cluster_points, axis=0))
                centers = np.array(centers)
                
                inter_distances = []
                for i in range(len(centers)):
                    for j in range(i+1, len(centers)):
                        dist = np.linalg.norm(centers[i] - centers[j])
                        inter_distances.append(dist)
                
                metrics['mean_inter_cluster_distance'] = float(np.mean(inter_distances))
                metrics['min_inter_cluster_distance'] = float(np.min(inter_distances))
                metrics['max_inter_cluster_distance'] = float(np.max(inter_distances))
                print('Mean inter-cluster distance:', round(metrics['mean_inter_cluster_distance'], 4))
                
                if metrics.get('mean_intra_cluster_distance'):
                    separation_ratio = metrics['mean_inter_cluster_distance'] / metrics['mean_intra_cluster_distance']
                    metrics['separation_ratio'] = float(separation_ratio)
                    
                    if separation_ratio > 2.0:
                        quality = 'Excellent'
                    elif separation_ratio > 1.5:
                        quality = 'Good'
                    elif separation_ratio > 1.0:
                        quality = 'Moderate'
                    else:
                        quality = 'Poor'
                    
                    print('Separation ratio:', round(separation_ratio, 4), '(' + quality + ')')
            
            cluster_sizes = [np.sum(labels_valid == label) for label in unique_labels]
            if cluster_sizes:
                metrics['cluster_size_std'] = float(np.std(cluster_sizes))
                metrics['cluster_size_cv'] = float(np.std(cluster_sizes) / np.mean(cluster_sizes))
                print('Cluster size variability (CV):', round(metrics['cluster_size_cv'], 4))
            
            return metrics
        
        
        def calculate_per_cluster_statistics(data, labels, calculate_detailed=True):
            print('')
            print('[5/5] Calculating per-cluster statistics')
            
            if not calculate_detailed:
                print('Skipped (disabled)')
                return None
            
            valid_mask = labels != -1
            data_valid = data[valid_mask]
            labels_valid = labels[valid_mask]
            
            if len(labels_valid) == 0:
                print('No valid clusters to analyze')
                return None
            
            per_cluster_stats = {}
            
            try:
                if len(np.unique(labels_valid)) >= 2:
                    cluster_silhouettes = silhouette_samples(data_valid, labels_valid)
                    has_silhouette = True
                else:
                    has_silhouette = False
            except:
                has_silhouette = False
            
            for label in sorted(np.unique(labels_valid)):
                cluster_mask = labels_valid == label
                cluster_points = data_valid[cluster_mask]
                cluster_key = 'cluster_' + str(int(label))
                
                size = len(cluster_points)
                percentage = size / len(labels_valid) * 100
                
                center = np.mean(cluster_points, axis=0)
                distances = np.linalg.norm(cluster_points - center, axis=1)
                
                cluster_silhouette = None
                if has_silhouette:
                    cluster_silhouette = float(np.mean(cluster_silhouettes[cluster_mask]))
                
                stats = {
                    'size': int(size),
                    'percentage': float(percentage),
                    'mean_distance_to_center': float(np.mean(distances)),
                    'max_distance_to_center': float(np.max(distances)),
                    'std_distance_to_center': float(np.std(distances)),
                    'silhouette_score': cluster_silhouette
                }
                
                per_cluster_stats[cluster_key] = stats
                
                sil_str = 'sil=' + str(round(cluster_silhouette, 3)) if cluster_silhouette is not None else "sil=N/A"
                print(cluster_key + ':', size, 'samples (' + str(round(percentage, 1)) + '%), ' + sil_str)
            
            return per_cluster_stats
        
        
        def generate_summary_report(evaluation_results):
            report = []
            report.append("\n" + "="*80)
            report.append("CLUSTERING EVALUATION SUMMARY")
            report.append("="*80)
            
            basic = evaluation_results['basic_statistics']
            report.append('\nDataset Overview:')
            report.append('  Dataset: ' + str(evaluation_results.get('dataset', 'Unknown')))
            report.append('  Algorithm: ' + str(evaluation_results.get('algorithm', 'Unknown')))
            report.append('  Total samples: ' + str(basic['n_samples']))
            report.append('  Clusters: ' + str(basic['n_clusters']))
            report.append('  Noise points: ' + str(basic['n_noise']) + ' (' + str(round(basic['noise_percentage'], 1)) + '%)')
            if basic['n_clusters'] > 0:
                report.append('  Avg cluster size: ' + str(round(basic['avg_cluster_size'], 1)) + ' +/- ' + str(round(basic['std_cluster_size'], 1)))
            
            if evaluation_results.get('internal_metrics') and not evaluation_results['internal_metrics'].get('reason'):
                internal = evaluation_results['internal_metrics']
                report.append('\nInternal Metrics (Clustering Quality):')
                
                if internal.get('silhouette_score') is not None:
                    sil = internal['silhouette_score']
                    if sil > 0.7:
                        quality = "Excellent"
                    elif sil > 0.5:
                        quality = "Good"
                    elif sil > 0.25:
                        quality = "Fair"
                    else:
                        quality = "Poor"
                    report.append('  Silhouette Score: ' + str(round(sil, 4)) + ' (' + quality + ')')
                
                if internal.get('davies_bouldin_score') is not None:
                    db = internal['davies_bouldin_score']
                    report.append('  Davies-Bouldin Score: ' + str(round(db, 4)) + ' (lower is better)')
                
                if internal.get('calinski_harabasz_score') is not None:
                    ch = internal['calinski_harabasz_score']
                    report.append('  Calinski-Harabasz Score: ' + str(round(ch, 4)) + ' (higher is better)')
            
            if evaluation_results.get('external_metrics') and evaluation_results['external_metrics'].get('available'):
                external = evaluation_results['external_metrics']
                report.append('\nExternal Metrics (vs Ground Truth):')
                
                if external.get('adjusted_rand_index') is not None:
                    ari = external['adjusted_rand_index']
                    if ari > 0.9:
                        match = "Excellent"
                    elif ari > 0.7:
                        match = "Good"
                    elif ari > 0.5:
                        match = "Moderate"
                    else:
                        match = "Poor"
                    report.append('  Adjusted Rand Index: ' + str(round(ari, 4)) + ' (' + match + ')')
            
            if evaluation_results.get('cluster_quality') and not evaluation_results['cluster_quality'].get('error'):
                quality = evaluation_results['cluster_quality']
                report.append('\nCluster Quality:')
                
                if quality.get('separation_ratio'):
                    ratio = quality['separation_ratio']
                    if ratio > 2.0:
                        quality_label = "Excellent"
                    elif ratio > 1.5:
                        quality_label = "Good"
                    elif ratio > 1.0:
                        quality_label = "Moderate"
                    else:
                        quality_label = "Poor"
                    report.append('  Separation ratio: ' + str(round(ratio, 4)) + ' (' + quality_label + ')')
            
            report.append("\n" + "="*80)
            
            return '\n'.join(report)
        
        
        def main():
            parser = argparse.ArgumentParser(
                description='Universal Clustering Model Evaluation Component'
            )
            
            parser.add_argument('--data', required=True,
                               help='Data file path')
            parser.add_argument('--labels', required=True,
                               help='Cluster labels path')
            parser.add_argument('--true_labels', default='',
                               help='Ground truth labels (optional)')
            parser.add_argument('--algorithm', default='Unknown',
                               help='Algorithm name')
            parser.add_argument('--dataset_name', default='test',
                               help='Dataset name')
            parser.add_argument('--per_cluster', default='false',
                               help='Calculate per-cluster stats')
            
            parser.add_argument('--output_evaluation_results', required=True,
                               help='Output path for evaluation results')
            parser.add_argument('--output_summary_report', required=True,
                               help='Output path for summary report')
            
            args = parser.parse_args()
            
            try:
                print("="*80)
                print("UNIVERSAL CLUSTERING MODEL EVALUATION")
                print("="*80)
                print("Important: Use this on TEST data predictions!")
                print("="*80)
                
                print('')
                print('Configuration')
                print('  Data:', args.data)
                print('  Labels:', args.labels)
                print('  True labels:', args.true_labels if args.true_labels else 'None')
                print('  Algorithm:', args.algorithm)
                print('  Dataset:', args.dataset_name)
                print('  Per-cluster:', args.per_cluster)
                
                data = load_data(args.data)
                labels = load_labels(args.labels)
                
                n_clusters, n_noise = validate_inputs(data, labels)
                
                evaluation_results = {
                    'algorithm': args.algorithm,
                    'dataset': args.dataset_name,
                    'data_shape': {
                        'n_samples': int(data.shape[0]),
                        'n_features': int(data.shape[1])
                    }
                }
                
                evaluation_results['basic_statistics'] = calculate_basic_statistics(labels)
                
                evaluation_results['internal_metrics'] = calculate_internal_metrics(data, labels)
                
                if args.true_labels and os.path.exists(args.true_labels):
                    true_labels = load_labels(args.true_labels)
                    
                    if len(true_labels) != len(labels):
                        raise ValueError('Ground truth length mismatch: labels has ' + str(len(labels)) + ', true_labels has ' + str(len(true_labels)))
                    
                    evaluation_results['external_metrics'] = calculate_external_metrics(
                        labels, true_labels
                    )
                else:
                    evaluation_results['external_metrics'] = None
                
                evaluation_results['cluster_quality'] = calculate_cluster_quality(data, labels)
                
                calculate_detailed = args.per_cluster.lower() == 'true'
                evaluation_results['per_cluster_statistics'] = calculate_per_cluster_statistics(
                    data, labels, calculate_detailed=calculate_detailed
                )
                
                os.makedirs(os.path.dirname(args.output_evaluation_results), exist_ok=True)
                os.makedirs(os.path.dirname(args.output_summary_report), exist_ok=True)
                
                with open(args.output_evaluation_results, 'w') as f:
                    json.dump(evaluation_results, f, indent=2)
                
                file_size = os.path.getsize(args.output_evaluation_results) / 1024
                print('')
                print('Evaluation results saved:', args.output_evaluation_results, '(' + str(round(file_size, 2)) + ' KB)')
                
                summary = generate_summary_report(evaluation_results)
                with open(args.output_summary_report, 'w') as f:
                    f.write(summary)
                
                print('Summary report saved:', args.output_summary_report)
                
                print(summary)
                
                print('')
                print('Quick Assessment')
                
                internal = evaluation_results.get('internal_metrics', {})
                if internal.get('silhouette_score') is not None:
                    sil = internal['silhouette_score']
                    if sil > 0.7:
                        print('  Excellent clustering (Silhouette > 0.7)')
                    elif sil > 0.5:
                        print('  Good clustering (Silhouette > 0.5)')
                    elif sil > 0.25:
                        print('  Fair clustering (Silhouette > 0.25)')
                    else:
                        print('  Poor clustering (Silhouette < 0.25)')
                
                if args.dataset_name == 'train':
                    print('')
                    print('Note: You evaluated TRAINING data.')
                    print('For TRUE performance, evaluate TEST data predictions!')
                else:
                    print('')
                    print('You evaluated', args.dataset_name.upper(), 'data - this is correct!')
                
                print("="*80)
                
            except Exception as e:
                print('')
                print('ERROR:', str(e))
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        
        if __name__ == '__main__':
            main()

    args:
      - --data
      - {inputPath: data}
      - --labels
      - {inputPath: labels}
      - --true_labels
      - {inputPath: true_labels}
      - --algorithm
      - {inputValue: algorithm}
      - --dataset_name
      - {inputValue: dataset_name}
      - --per_cluster
      - {inputValue: per_cluster}
      - --output_evaluation_results
      - {outputPath: evaluation_results}
      - --output_summary_report
      - {outputPath: summary_report}
