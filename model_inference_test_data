name: Universal Clustering Model Inference Component
description: |
  Universal inference component supporting ALL clustering algorithms with multiple inference methods.
  Handles algorithms with and without native predict() using unified logic.
  Supports: Native, Nearest Neighbor, Approximate Predict (HDBSCAN), Refit, and Function Call (Fuzzy C-Means).

inputs:
  - name: model
    type: Model
    description: 'Trained clustering model (PKL format from training component)'
  - name: metadata
    type: Data
    description: 'Training metadata (JSON format from training component)'
  - name: test_data
    type: Data
    description: 'Test dataset for prediction (CSV or NPY format)'
  - name: train_data_ref
    type: Data
    description: 'Training data reference (NPY, optional, needed for nearest_neighbor/refit modes)'
    optional: true
  - name: train_labels
    type: Data
    description: 'Training labels (NPY, optional, needed for nearest_neighbor/refit modes)'
    optional: true
  - name: inference_mode
    type: String
    description: 'Inference method: auto, native, nearest_neighbor, refit, approximate_predict, function_call'
    default: 'auto'

outputs:
  - name: test_predictions
    type: Data
    description: 'Test cluster predictions (NPY format)'
  - name: prediction_metadata
    type: Data
    description: 'Prediction statistics and metadata (JSON format)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import pickle
        import argparse
        import warnings
        import numpy as np
        import pandas as pd
        from pathlib import Path
        
        warnings.filterwarnings('ignore')
        
        
        def load_model(model_path):
            print('')
            print('Loading model from:', model_path)
            
            try:
                with open(model_path, 'rb') as f:
                    model = pickle.load(f)
                
                print('Model loaded successfully')
                
                if isinstance(model, dict):
                    if 'algorithm' in model and model['algorithm'] == 'FuzzyCMeans':
                        print('Model type: Fuzzy C-Means (functional)')
                    else:
                        print('Model type: Custom dictionary')
                else:
                    print('Model type:', type(model).__name__)
                
                return model
                
            except Exception as e:
                print('Error loading model:', str(e))
                raise
        
        
        def load_metadata(metadata_path):
            print('')
            print('Loading metadata from:', metadata_path)
            
            try:
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)
                
                print('Metadata loaded successfully')
                print('Algorithm:', metadata.get('algorithm', 'Unknown'))
                print('Category:', metadata.get('category', 'Unknown'))
                
                capabilities = metadata.get('capabilities', {})
                has_predict = capabilities.get('has_predict', False)
                preferred_inference = capabilities.get('preferred_inference', 'unknown')
                
                print('Has native predict:', has_predict)
                print('Preferred inference:', preferred_inference)
                
                return metadata
                
            except Exception as e:
                print('Error loading metadata:', str(e))
                raise
        
        
        def load_data(data_path):
            print('')
            print('Loading test data from:', data_path)
            
            try:
                if data_path.endswith('.csv'):
                    df = pd.read_csv(data_path)
                    data = df.values
                elif data_path.endswith('.npy'):
                    data = np.load(data_path)
                else:
                    raise ValueError("Unsupported format. Use .csv or .npy")
                
                print('Loaded successfully')
                print('Shape:', data.shape[0], 'samples x', data.shape[1], 'features')
                
                if not np.isfinite(data).all():
                    n_invalid = (~np.isfinite(data)).sum()
                    raise ValueError('Test data contains ' + str(n_invalid) + ' NaN or Inf values')
                
                return data
                
            except Exception as e:
                print('Error loading data:', str(e))
                raise
        
        
        def load_training_data(train_data_path, train_labels_path):
            print('')
            print('Loading training reference data')
            
            try:
                if not train_data_path or not os.path.exists(train_data_path):
                    return None, None
                if not train_labels_path or not os.path.exists(train_labels_path):
                    return None, None
                
                if train_data_path.endswith('.csv'):
                    df = pd.read_csv(train_data_path)
                    X_train = df.values
                elif train_data_path.endswith('.npy'):
                    X_train = np.load(train_data_path)
                else:
                    raise ValueError("Unsupported format for training data")
                
                print('Training data:', X_train.shape)
                
                train_labels = np.load(train_labels_path)
                print('Training labels:', len(train_labels), 'samples')
                
                if len(X_train) != len(train_labels):
                    raise ValueError('Training data and labels size mismatch: data=' + str(len(X_train)) + ', labels=' + str(len(train_labels)))
                
                return X_train, train_labels
                
            except Exception as e:
                print('Warning: Could not load training data:', str(e))
                return None, None
        
        
        def validate_test_data(X_test, metadata):
            print('')
            print('Validating test data')
            
            expected_features = metadata['training_results']['n_features']
            actual_features = X_test.shape[1]
            
            if actual_features != expected_features:
                raise ValueError('Feature mismatch: model expects ' + str(expected_features) + ' features, test data has ' + str(actual_features) + ' features')
            
            print('Features match:', actual_features)
            print('Test samples:', X_test.shape[0])
        
        
        def predict_native(model, X_test, algorithm_name):
            print('')
            print('[Method: Native Predict]')
            print('Using model.predict()')
            
            try:
                predictions = model.predict(X_test)
                print('Predictions generated:', len(predictions), 'samples')
                return predictions
                
            except Exception as e:
                print('Native predict failed:', str(e))
                raise
        
        
        def predict_nearest_neighbor(model, X_test, X_train, train_labels, algorithm_name, metadata):
            print('')
            print('[Method: Nearest Neighbor]')
            print('Finding nearest training sample for each test sample')
            
            from sklearn.neighbors import NearestNeighbors
            
            nn = NearestNeighbors(n_neighbors=1, algorithm='auto')
            nn.fit(X_train)
            
            distances, indices = nn.kneighbors(X_test)
            
            predictions = train_labels[indices.flatten()]
            
            print('Assigned labels based on nearest neighbors')
            
            capabilities = metadata.get('capabilities', {})
            if capabilities.get('allows_noise', False):
                threshold = None
                
                if hasattr(model, 'eps'):
                    threshold = model.eps
                    threshold_name = 'eps'
                elif hasattr(model, 'max_eps'):
                    if model.max_eps != float('inf'):
                        threshold = model.max_eps
                        threshold_name = 'max_eps'
                elif algorithm_name == 'HDBSCAN':
                    threshold = np.percentile(distances, 95)
                    threshold_name = '95th percentile'
                
                if threshold is not None:
                    noise_mask = distances.flatten() > threshold
                    n_noise = noise_mask.sum()
                    if n_noise > 0:
                        predictions[noise_mask] = -1
                        print('Marked', n_noise, 'points as noise (distance >', threshold_name, '=', round(threshold, 4), ')')
            
            return predictions
        
        
        def predict_refit(model, X_test, X_train, train_labels, algorithm_name, metadata):
            print('')
            print('[Method: Refit]')
            print('Warning: This will refit the model on combined data')
            print('Combining train and test data')
            
            X_combined = np.vstack([X_train, X_test])
            print('Combined shape:', X_combined.shape)
            
            try:
                if hasattr(model, 'fit_predict'):
                    print('Using fit_predict()')
                    combined_labels = model.fit_predict(X_combined)
                elif hasattr(model, 'fit'):
                    print('Using fit()')
                    model.fit(X_combined)
                    if hasattr(model, 'labels_'):
                        combined_labels = model.labels_
                    else:
                        combined_labels = model.predict(X_combined)
                else:
                    raise ValueError('Cannot refit ' + str(algorithm_name))
                
                test_predictions = combined_labels[len(X_train):]
                
                print('Refit complete')
                print('Note: Training cluster assignments may have changed')
                
                return test_predictions
                
            except Exception as e:
                print('Refit failed:', str(e))
                raise
        
        
        def predict_approximate_hdbscan(model, X_test):
            print('')
            print('[Method: HDBSCAN Approximate Predict]')
            
            try:
                import hdbscan
                
                print('Using hdbscan.approximate_predict()')
                test_labels, strengths = hdbscan.approximate_predict(model, X_test)
                
                print('Predictions generated')
                print('Prediction strengths: min=', round(strengths.min(), 3), ', max=', round(strengths.max(), 3))
                
                return test_labels
                
            except ImportError:
                raise ImportError("HDBSCAN package not installed. Install with: pip install hdbscan")
            except Exception as e:
                print('HDBSCAN approximate_predict failed:', str(e))
                raise
        
        
        def predict_fuzzy_cmeans(model, X_test):
            print('')
            print('[Method: Fuzzy C-Means Function Call]')
            
            try:
                import skfuzzy as fuzz
                
                print('Using skfuzzy.cmeans_predict()')
                
                centers = model['centers']
                params = model['params']
                m = params.get('m', 2)
                error = params.get('error', 0.005)
                maxiter = params.get('maxiter', 1000)
                
                u, u0, d, jm, p, fpc = fuzz.cluster.cmeans_predict(
                    X_test.T,
                    centers,
                    m,
                    error=error,
                    maxiter=maxiter
                )
                
                test_labels = np.argmax(u, axis=0)
                
                print('Predictions generated')
                
                return test_labels
                
            except ImportError:
                raise ImportError("scikit-fuzzy package not installed. Install with: pip install scikit-fuzzy")
            except Exception as e:
                print('Fuzzy C-Means predict failed:', str(e))
                raise
        
        
        def run_inference(model, X_test, metadata, X_train=None, train_labels=None, inference_mode='auto'):
            algorithm_name = metadata['algorithm']
            capabilities = metadata.get('capabilities', {})
            
            print('')
            print('Starting inference for', algorithm_name)
            print('Inference mode:', inference_mode)
            
            if inference_mode == 'auto':
                inference_mode = capabilities.get('preferred_inference', 'native')
                print('Auto-selected mode:', inference_mode)
            
            if inference_mode == 'native':
                if not capabilities.get('has_predict', False):
                    raise ValueError(str(algorithm_name) + ' does not support native predict(). Try: nearest_neighbor or refit mode')
                predictions = predict_native(model, X_test, algorithm_name)
            
            elif inference_mode == 'nearest_neighbor':
                if X_train is None or train_labels is None:
                    raise ValueError("Nearest neighbor inference requires training data. Provide train_data_ref and train_labels")
                predictions = predict_nearest_neighbor(
                    model, X_test, X_train, train_labels, algorithm_name, metadata
                )
            
            elif inference_mode == 'refit':
                if X_train is None or train_labels is None:
                    raise ValueError("Refit inference requires training data. Provide train_data_ref and train_labels")
                predictions = predict_refit(
                    model, X_test, X_train, train_labels, algorithm_name, metadata
                )
            
            elif inference_mode == 'approximate_predict':
                if algorithm_name != 'HDBSCAN':
                    raise ValueError('approximate_predict only works with HDBSCAN, not ' + str(algorithm_name))
                predictions = predict_approximate_hdbscan(model, X_test)
            
            elif inference_mode == 'function_call':
                if algorithm_name != 'FuzzyCMeans':
                    raise ValueError('function_call only works with FuzzyCMeans, not ' + str(algorithm_name))
                predictions = predict_fuzzy_cmeans(model, X_test)
            
            else:
                raise ValueError('Unknown inference mode: ' + str(inference_mode))
            
            return predictions
        
        
        def analyze_predictions(predictions):
            print('')
            print('Analyzing predictions')
            
            unique_labels = np.unique(predictions)
            n_clusters = len(unique_labels[unique_labels != -1])
            n_noise = np.sum(predictions == -1)
            
            stats = {
                'n_samples': int(len(predictions)),
                'n_clusters': int(n_clusters),
                'n_noise': int(n_noise),
                'noise_percentage': float(n_noise / len(predictions) * 100),
                'cluster_sizes': {}
            }
            
            print('Predictions:', len(predictions), 'samples')
            print('Clusters:', n_clusters)
            
            if n_noise > 0:
                print('Noise points:', n_noise, '(' + str(round(stats['noise_percentage'], 1)) + '%)')
            
            print('')
            print('Cluster distribution')
            for label in sorted(unique_labels):
                count = np.sum(predictions == label)
                pct = count / len(predictions) * 100
                label_name = "Noise" if label == -1 else 'Cluster ' + str(label)
                stats['cluster_sizes'][int(label)] = int(count)
                print(' ', label_name + ':', count, 'samples (' + str(round(pct, 1)) + '%)')
            
            return stats
        
        
        def save_predictions(predictions, stats, metadata, pred_path, meta_path):
            print('')
            print('Saving predictions')
            
            os.makedirs(os.path.dirname(pred_path), exist_ok=True)
            os.makedirs(os.path.dirname(meta_path), exist_ok=True)
            
            np.save(pred_path, predictions)
            print('Predictions saved:', pred_path)
            
            pred_metadata = {
                'algorithm': metadata['algorithm'],
                'category': metadata.get('category', 'unknown'),
                'prediction_statistics': stats,
                'inference_complete': True
            }
            
            with open(meta_path, 'w') as f:
                json.dump(pred_metadata, f, indent=2)
            
            file_size = os.path.getsize(meta_path) / 1024
            print('Metadata saved:', meta_path, '(' + str(round(file_size, 2)) + ' KB)')
        
        
        def main():
            parser = argparse.ArgumentParser(
                description='Universal Clustering Model Inference Component'
            )
            
            parser.add_argument('--model', required=True,
                               help='Trained model path')
            parser.add_argument('--metadata', required=True,
                               help='Training metadata path')
            parser.add_argument('--test_data', required=True,
                               help='Test data path')
            parser.add_argument('--train_data_ref', default='',
                               help='Training data reference path (optional)')
            parser.add_argument('--train_labels', default='',
                               help='Training labels path (optional)')
            parser.add_argument('--inference_mode', default='auto',
                               help='Inference method')
            
            parser.add_argument('--output_test_predictions', required=True,
                               help='Output path for predictions')
            parser.add_argument('--output_prediction_metadata', required=True,
                               help='Output path for metadata')
            
            args = parser.parse_args()
            
            try:
                print("="*80)
                print("UNIVERSAL CLUSTERING MODEL INFERENCE")
                print("="*80)
                
                print('')
                print('Configuration')
                print('  Model:', args.model)
                print('  Metadata:', args.metadata)
                print('  Test data:', args.test_data)
                print('  Train data ref:', args.train_data_ref if args.train_data_ref else 'Not provided')
                print('  Train labels:', args.train_labels if args.train_labels else 'Not provided')
                print('  Inference mode:', args.inference_mode)
                
                model = load_model(args.model)
                
                metadata = load_metadata(args.metadata)
                
                X_test = load_data(args.test_data)
                
                validate_test_data(X_test, metadata)
                
                X_train = None
                train_labels = None
                
                if args.train_data_ref and args.train_labels:
                    X_train, train_labels = load_training_data(
                        args.train_data_ref,
                        args.train_labels
                    )
                elif args.inference_mode in ['nearest_neighbor', 'refit']:
                    raise ValueError('Inference mode ' + args.inference_mode + ' requires training data. Provide train_data_ref and train_labels')
                
                predictions = run_inference(
                    model=model,
                    X_test=X_test,
                    metadata=metadata,
                    X_train=X_train,
                    train_labels=train_labels,
                    inference_mode=args.inference_mode
                )
                
                stats = analyze_predictions(predictions)
                
                save_predictions(
                    predictions, stats, metadata,
                    args.output_test_predictions,
                    args.output_prediction_metadata
                )
                
                print('')
                print("="*80)
                print("INFERENCE COMPLETED SUCCESSFULLY")
                print("="*80)
                print('')
                print('Summary')
                print('  Algorithm:', metadata['algorithm'])
                print('  Test samples:', X_test.shape[0])
                print('  Clusters assigned:', stats['n_clusters'])
                if stats['n_noise'] > 0:
                    print('  Noise points:', stats['n_noise'], '(' + str(round(stats['noise_percentage'], 1)) + '%)')
                
                print('')
                print('Output files created')
                print('  Predictions:', args.output_test_predictions)
                print('  Metadata:', args.output_prediction_metadata)
                print("="*80)
                
            except Exception as e:
                print('')
                print('ERROR:', str(e))
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        
        if __name__ == '__main__':
            main()

    args:
      - --model
      - {inputPath: model}
      - --metadata
      - {inputPath: metadata}
      - --test_data
      - {inputPath: test_data}
      - --train_data_ref
      - {inputPath: train_data_ref}
      - --train_labels
      - {inputPath: train_labels}
      - --inference_mode
      - {inputValue: inference_mode}
      - --output_test_predictions
      - {outputPath: test_predictions}
      - --output_prediction_metadata
      - {outputPath: prediction_metadata}
