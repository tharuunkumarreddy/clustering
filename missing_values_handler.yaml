name: Generic Missing Values Handler Component
description: Comprehensive missing value handling for clustering with 10 imputation strategies. Supports method-specific parameters via JSON. Methods include drop, mean, median, mode, constant, KNN, iterative, forward/backward fill.

inputs:
  - name: input_data
    type: Data
    description: 'Input dataset with potential missing values (CSV or Parquet)'
  - name: method
    type: String
    description: 'Imputation method: drop_rows, drop_cols, mean, median, mode, constant, zero, knn, iterative, forward_fill, backward_fill'
    default: 'mean'
  - name: method_params
    type: String
    description: 'Method-specific parameters as JSON string. Examples: {"n_neighbors":5}, {"drop_threshold":0.5}, {"fill_value":0}'
    default: '{}'

outputs:
  - name: data
    type: Data
    description: 'Dataset with missing values handled (CSV format)'
  - name: imputer
    type: Model
    description: 'Fitted imputer object for consistency with test data (PKL format)'
  - name: report
    type: Data
    description: 'Comprehensive imputation report and statistics (JSON format)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pickle
        import pandas as pd
        import numpy as np
        from sklearn.impute import SimpleImputer, KNNImputer
        from sklearn.experimental import enable_iterative_imputer
        from sklearn.impute import IterativeImputer
        from pathlib import Path
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        logger = logging.getLogger('missing_values_handler')
        
        
        def ensure_directory_exists(file_path):
            # Create directory if it doesn't exist
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
                logger.info(f"Created directory: {directory}")
        
        
        def load_data(input_path):
            # Load data from CSV or Parquet file
            logger.info(f"Loading dataset from: {input_path}")
            
            ext = Path(input_path).suffix.lower()
            
            try:
                if ext in ['.parquet', '.pq']:
                    df = pd.read_parquet(input_path)
                    logger.info("Loaded Parquet file")
                else:
                    df = pd.read_csv(input_path)
                    logger.info("Loaded CSV file")
                
                logger.info(f"Shape: {df.shape[0]} rows x {df.shape[1]} columns")
                return df
                
            except Exception as e:
                logger.error(f"Error loading data: {str(e)}")
                raise
        
        
        def analyze_missing_patterns(df):
            # Analyze missing value patterns in dataset
            # Returns: dict with missing value statistics
            logger.info("="*80)
            logger.info("ANALYZING MISSING VALUE PATTERNS")
            logger.info("="*80)
            
            total_missing = df.isnull().sum().sum()
            total_values = df.shape[0] * df.shape[1]
            missing_pct = (total_missing / total_values) * 100 if total_values > 0 else 0
            
            missing_info = {
                'total_missing': int(total_missing),
                'total_values': int(total_values),
                'missing_percentage': float(missing_pct),
                'columns_with_missing': {},
                'rows_with_missing': int(df.isnull().any(axis=1).sum()),
                'rows_with_missing_pct': float(df.isnull().any(axis=1).sum() / len(df) * 100) if len(df) > 0 else 0
            }
            
            # Per-column analysis
            cols_with_missing = []
            for col in df.columns:
                miss_count = df[col].isnull().sum()
                if miss_count > 0:
                    missing_info['columns_with_missing'][col] = {
                        'count': int(miss_count),
                        'percentage': float(miss_count / len(df) * 100),
                        'dtype': str(df[col].dtype)
                    }
                    cols_with_missing.append(col)
                    logger.info(f"  {col}: {miss_count} missing ({miss_count/len(df)*100:.1f}%)")
            
            if total_missing == 0:
                logger.info("✓ No missing values found")
            else:
                logger.info("")
                logger.info(f"Total missing values: {total_missing} ({missing_pct:.2f}%)")
                logger.info(f"Columns with missing: {len(cols_with_missing)}")
                logger.info(f"Rows with missing: {missing_info['rows_with_missing']} ({missing_info['rows_with_missing_pct']:.1f}%)")
            
            logger.info("")
            return missing_info
        
        
        def handle_missing_values(df, method, method_params):
            # Handle missing values using specified strategy
            # Parameters: df, method, method_params
            # Returns: df_imputed, imputer object, stats dict
            logger.info("="*80)
            logger.info("HANDLING MISSING VALUES")
            logger.info("="*80)
            logger.info(f"Method: {method}")
            
            if method_params:
                logger.info(f"Parameters: {method_params}")
            
            logger.info("")
            
            initial_shape = df.shape
            initial_missing = df.isnull().sum().sum()
            
            df_imputed = df.copy()
            imputer = None
            dropped_columns = []
            dropped_rows = 0
            
            stats = {
                'method': method,
                'method_params': method_params,
                'initial_missing': int(initial_missing),
                'initial_shape': list(initial_shape),
                'dropped_columns': [],
                'dropped_rows': 0,
                'final_missing': 0,
                'final_shape': [],
                'imputation_success': False
            }
            
            # METHOD 1: Drop rows with missing values
            if method == 'drop_rows':
                before = len(df_imputed)
                df_imputed = df_imputed.dropna()
                dropped_rows = before - len(df_imputed)
                logger.info(f"Dropped {dropped_rows} rows with missing values")
                logger.info(f"Remaining rows: {len(df_imputed)}")
                stats['dropped_rows'] = dropped_rows
            
            # METHOD 2: Drop columns exceeding threshold
            elif method == 'drop_cols':
                drop_threshold = method_params.get('drop_threshold', 0.5)
                logger.info(f"Drop threshold: {drop_threshold*100}%")
                
                missing_pct = df_imputed.isnull().sum() / len(df_imputed)
                cols_to_drop = missing_pct[missing_pct > drop_threshold].index.tolist()
                
                if cols_to_drop:
                    df_imputed = df_imputed.drop(columns=cols_to_drop)
                    dropped_columns = cols_to_drop
                    logger.info(f"Dropped {len(cols_to_drop)} columns:")
                    for col in cols_to_drop:
                        logger.info(f"  - {col} ({missing_pct[col]*100:.1f}% missing)")
                    stats['dropped_columns'] = cols_to_drop
                else:
                    logger.info(f"No columns exceed {drop_threshold*100}% missing threshold")
            
            # METHOD 3: Mean imputation (numeric only)
            elif method == 'mean':
                numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns.tolist()
                cat_cols = df_imputed.select_dtypes(exclude=[np.number]).columns.tolist()
                
                if len(numeric_cols) > 0:
                    imputer = SimpleImputer(strategy='mean')
                    df_imputed[numeric_cols] = imputer.fit_transform(df_imputed[numeric_cols])
                    logger.info(f"Mean imputation applied to {len(numeric_cols)} numeric columns")
                
                if len(cat_cols) > 0:
                    cat_imputer = SimpleImputer(strategy='most_frequent')
                    df_imputed[cat_cols] = cat_imputer.fit_transform(df_imputed[cat_cols])
                    logger.info(f"Mode imputation applied to {len(cat_cols)} categorical columns")
            
            # METHOD 4: Median imputation (numeric only)
            elif method == 'median':
                numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns.tolist()
                cat_cols = df_imputed.select_dtypes(exclude=[np.number]).columns.tolist()
                
                if len(numeric_cols) > 0:
                    imputer = SimpleImputer(strategy='median')
                    df_imputed[numeric_cols] = imputer.fit_transform(df_imputed[numeric_cols])
                    logger.info(f"Median imputation applied to {len(numeric_cols)} numeric columns")
                
                if len(cat_cols) > 0:
                    cat_imputer = SimpleImputer(strategy='most_frequent')
                    df_imputed[cat_cols] = cat_imputer.fit_transform(df_imputed[cat_cols])
                    logger.info(f"Mode imputation applied to {len(cat_cols)} categorical columns")
            
            # METHOD 5: Mode imputation (all columns)
            elif method == 'mode':
                imputer = SimpleImputer(strategy='most_frequent')
                df_imputed_values = imputer.fit_transform(df_imputed)
                df_imputed = pd.DataFrame(
                    df_imputed_values,
                    columns=df_imputed.columns,
                    index=df_imputed.index
                )
                logger.info(f"Mode imputation applied to all {len(df_imputed.columns)} columns")
            
            # METHOD 6: Constant value (with customizable fill_value)
            elif method in ['constant', 'zero']:
                fill_value = method_params.get('fill_value', 0)
                df_imputed = df_imputed.fillna(fill_value)
                logger.info(f"Filled missing values with constant: {fill_value}")
                stats['method_params']['fill_value'] = fill_value
            
            # METHOD 7: KNN imputation (numeric only, with parameters)
            elif method == 'knn':
                n_neighbors = method_params.get('n_neighbors', 5)
                weights = method_params.get('weights', 'uniform')
                
                numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns.tolist()
                cat_cols = df_imputed.select_dtypes(exclude=[np.number]).columns.tolist()
                
                if len(numeric_cols) > 0:
                    imputer = KNNImputer(n_neighbors=n_neighbors, weights=weights)
                    df_imputed[numeric_cols] = imputer.fit_transform(df_imputed[numeric_cols])
                    logger.info(f"KNN imputation (k={n_neighbors}, weights={weights})")
                    logger.info(f"  Applied to {len(numeric_cols)} numeric columns")
                    stats['method_params'] = {'n_neighbors': n_neighbors, 'weights': weights}
                
                if len(cat_cols) > 0:
                    cat_imputer = SimpleImputer(strategy='most_frequent')
                    df_imputed[cat_cols] = cat_imputer.fit_transform(df_imputed[cat_cols])
                    logger.info(f"  Mode imputation for {len(cat_cols)} categorical columns")
            
            # METHOD 8: Iterative imputation (numeric only, with parameters)
            elif method == 'iterative':
                max_iter = method_params.get('max_iter', 10)
                random_state = method_params.get('random_state', 42)
                
                numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns.tolist()
                cat_cols = df_imputed.select_dtypes(exclude=[np.number]).columns.tolist()
                
                if len(numeric_cols) > 0:
                    imputer = IterativeImputer(
                        max_iter=max_iter,
                        random_state=random_state,
                        verbose=0
                    )
                    df_imputed[numeric_cols] = imputer.fit_transform(df_imputed[numeric_cols])
                    logger.info(f"Iterative imputation (max_iter={max_iter})")
                    logger.info(f"  Applied to {len(numeric_cols)} numeric columns")
                    stats['method_params'] = {'max_iter': max_iter, 'random_state': random_state}
                
                if len(cat_cols) > 0:
                    cat_imputer = SimpleImputer(strategy='most_frequent')
                    df_imputed[cat_cols] = cat_imputer.fit_transform(df_imputed[cat_cols])
                    logger.info(f"  Mode imputation for {len(cat_cols)} categorical columns")
            
            # METHOD 9: Forward fill
            elif method in ['forward_fill', 'ffill']:
                df_imputed = df_imputed.fillna(method='ffill')
                # Backward fill any remaining NaNs at the start
                remaining = df_imputed.isnull().sum().sum()
                if remaining > 0:
                    df_imputed = df_imputed.fillna(method='bfill')
                    logger.info("Forward fill applied (with backward fill for leading NaNs)")
                else:
                    logger.info("Forward fill applied")
            
            # METHOD 10: Backward fill
            elif method in ['backward_fill', 'bfill']:
                df_imputed = df_imputed.fillna(method='bfill')
                # Forward fill any remaining NaNs at the end
                remaining = df_imputed.isnull().sum().sum()
                if remaining > 0:
                    df_imputed = df_imputed.fillna(method='ffill')
                    logger.info("Backward fill applied (with forward fill for trailing NaNs)")
                else:
                    logger.info("Backward fill applied")
            
            else:
                raise ValueError(
                    f"Unknown missing value method: {method}. "
                    f"Available: drop_rows, drop_cols, mean, median, mode, constant, "
                    f"knn, iterative, forward_fill, backward_fill"
                )
            
            # Update statistics
            final_missing = df_imputed.isnull().sum().sum()
            stats['final_missing'] = int(final_missing)
            stats['final_shape'] = list(df_imputed.shape)
            stats['imputation_success'] = bool(final_missing == 0)
            
            # Calculate reduction
            if initial_missing > 0:
                reduction_pct = ((initial_missing - final_missing) / initial_missing) * 100
                stats['missing_reduction_pct'] = float(reduction_pct)
            else:
                stats['missing_reduction_pct'] = 0.0
            
            logger.info("")
            logger.info(f"Missing values before: {initial_missing}")
            logger.info(f"Missing values after: {final_missing}")
            
            if final_missing == 0:
                logger.info("✓ All missing values handled successfully")
            elif final_missing < initial_missing:
                logger.info(f"⚠ {final_missing} missing values remain")
            
            logger.info(f"Final shape: {df_imputed.shape}")
            logger.info("")
            
            return df_imputed, imputer, stats
        
        
        def main():
            parser = argparse.ArgumentParser(
                description="Generic Missing Values Handler for Clustering"
            )
            parser.add_argument("--input_data", required=True,
                               help="Path to input dataset")
            parser.add_argument("--method", default='mean',
                               help="Missing value handling method")
            parser.add_argument("--method_params", type=str, default='{}',
                               help="Method-specific parameters as JSON string")
            parser.add_argument("--output_data", required=True,
                               help="Output path for imputed dataset")
            parser.add_argument("--output_imputer", required=True,
                               help="Output path for fitted imputer")
            parser.add_argument("--output_report", required=True,
                               help="Output path for imputation report")
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("MISSING VALUES HANDLER COMPONENT")
            logger.info("="*80)
            logger.info(f"Input: {args.input_data}")
            logger.info(f"Method: {args.method}")
            logger.info("")
            
            try:
                # Ensure output directories
                ensure_directory_exists(args.output_data)
                ensure_directory_exists(args.output_imputer)
                ensure_directory_exists(args.output_report)
                
                # Parse method parameters
                try:
                    method_params = json.loads(args.method_params)
                    if method_params:
                        logger.info(f"Method parameters: {method_params}")
                        logger.info("")
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON in method_params: {e}")
                    logger.error(f"Received: {args.method_params}")
                    logger.error("Expected format: '{\"param\": value}' (use double quotes)")
                    sys.exit(1)
                
                # Load data
                df = load_data(args.input_data)
                
                if df.empty:
                    logger.error("ERROR: Dataset is empty")
                    sys.exit(1)
                
                # Analyze missing patterns
                missing_analysis = analyze_missing_patterns(df)
                
                # Check if any missing values exist
                if missing_analysis['total_missing'] == 0:
                    logger.info("="*80)
                    logger.info("No missing values found - no imputation needed")
                    logger.info("="*80)
                    
                    # Save original data
                    df.to_csv(args.output_data, index=False)
                    
                    # Save empty imputer
                    with open(args.output_imputer, 'wb') as f:
                        pickle.dump(None, f)
                    
                    # Create report
                    report = {
                        'missing_analysis': missing_analysis,
                        'imputation_stats': {
                            'method': args.method,
                            'method_params': method_params,
                            'initial_missing': 0,
                            'final_missing': 0,
                            'imputation_success': True,
                            'imputation_needed': False
                        },
                        'method': args.method,
                        'method_params': method_params
                    }
                    
                    with open(args.output_report, 'w') as f:
                        json.dump(report, f, indent=2)
                    
                    logger.info("Outputs saved successfully")
                    logger.info("="*80)
                    sys.exit(0)
                
                # Handle missing values
                df_imputed, imputer, stats = handle_missing_values(
                    df=df,
                    method=args.method,
                    method_params=method_params
                )
                
                # Validate result
                if not stats['imputation_success']:
                    logger.warning("="*80)
                    logger.warning("WARNING: Not all missing values were handled")
                    logger.warning(f"Remaining missing values: {stats['final_missing']}")
                    logger.warning("="*80)
                
                # Save imputed data
                df_imputed.to_csv(args.output_data, index=False)
                logger.info(f"Imputed data saved: {args.output_data}")
                
                # Save imputer
                with open(args.output_imputer, 'wb') as f:
                    pickle.dump(imputer, f)
                logger.info(f"Imputer saved: {args.output_imputer}")
                
                # Create comprehensive report
                report = {
                    'missing_analysis': missing_analysis,
                    'imputation_stats': stats,
                    'method': args.method,
                    'method_params': method_params
                }
                
                with open(args.output_report, 'w') as f:
                    json.dump(report, f, indent=2)
                logger.info(f"Report saved: {args.output_report}")
                
                logger.info("")
                logger.info("="*80)
                logger.info("MISSING VALUES HANDLING COMPLETED")
                logger.info("="*80)
                logger.info(f"Method: {args.method}")
                logger.info(f"Initial missing: {missing_analysis['total_missing']} ({missing_analysis['missing_percentage']:.2f}%)")
                logger.info(f"Final missing: {stats['final_missing']}")
                
                if stats['dropped_columns']:
                    logger.info(f"Columns dropped: {len(stats['dropped_columns'])}")
                if stats['dropped_rows'] > 0:
                    logger.info(f"Rows dropped: {stats['dropped_rows']}")
                
                logger.info(f"Success: {stats['imputation_success']}")
                logger.info("="*80)
                
            except ValueError as e:
                logger.error(f"VALIDATION ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
            except Exception as e:
                logger.error(f"ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        
        if __name__ == "__main__":
            main()
    args:
      - --input_data
      - {inputPath: input_data}
      - --method
      - {inputValue: method}
      - --method_params
      - {inputValue: method_params}
      - --output_data
      - {outputPath: data}
      - --output_imputer
      - {outputPath: imputer}
      - --output_report
      - {outputPath: report}
