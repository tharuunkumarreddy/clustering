name: Generic Dimensionality Reduction Component
description: Comprehensive dimensionality reduction for clustering with 9 methods including PCA, Kernel PCA, ICA, t-SNE, Isomap, LLE, MDS, Factor Analysis, and Sparse PCA. Supports auto-selection of components and method-specific parameters via JSON.

inputs:
  - name: input_data
    type: Data
    description: 'Input dataset (CSV or Parquet)'
  - name: method
    type: String
    description: 'Reduction method: pca, kernel_pca, ica, factor_analysis, sparse_pca, tsne, isomap, lle, mds'
    default: 'pca'
  - name: method_params
    type: String
    description: 'Method-specific parameters as JSON string. Examples: {"n_components":"auto"}, {"kernel":"rbf"}, {"perplexity":30}'
    default: '{}'

outputs:
  - name: data
    type: Data
    description: 'Reduced dataset with component features (CSV format)'
  - name: reducer
    type: Model
    description: 'Fitted dimensionality reducer (PKL format)'
  - name: metadata
    type: Data
    description: 'Reduction metadata including variance explained (JSON format)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pickle
        import pandas as pd
        import numpy as np
        from sklearn.decomposition import PCA, KernelPCA, FastICA, FactorAnalysis, SparsePCA
        from sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding, MDS
        from pathlib import Path
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        logger = logging.getLogger('dimensionality_reduction')
        
        
        def ensure_directory_exists(file_path):
            # Create directory if it doesn't exist
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
                logger.info(f"Created directory: {directory}")
        
        
        def load_data(input_path):
            # Load data from CSV or Parquet file
            logger.info(f"Loading dataset from: {input_path}")
            
            ext = Path(input_path).suffix.lower()
            
            try:
                if ext in ['.parquet', '.pq']:
                    df = pd.read_parquet(input_path)
                    logger.info("Loaded Parquet file")
                else:
                    df = pd.read_csv(input_path)
                    logger.info("Loaded CSV file")
                
                logger.info(f"Shape: {df.shape[0]} rows x {df.shape[1]} columns")
                return df
                
            except Exception as e:
                logger.error(f"Error loading data: {str(e)}")
                raise
        
        
        def determine_n_components(X, method='variance', variance_threshold=0.95, max_components=None):
            # Automatically determine optimal number of components
            # Parameters: X (data matrix), method, variance_threshold, max_components
            # Returns: n_components (int)
            n_features = X.shape[1]
            n_samples = X.shape[0]
            
            if max_components is None:
                max_components = min(n_samples, n_features)
            
            if method == 'variance':
                # Fit temporary PCA to determine components
                pca_temp = PCA(n_components=min(n_samples, n_features))
                pca_temp.fit(X)
                
                cumsum = np.cumsum(pca_temp.explained_variance_ratio_)
                n_components = int(np.searchsorted(cumsum, variance_threshold) + 1)
                n_components = min(n_components, max_components)
                
                logger.info(
                    f"Auto-determined {n_components} components "
                    f"for {variance_threshold*100}% variance"
                )
                
            elif method == 'kaiser':
                # Kaiser criterion: eigenvalues > 1
                pca_temp = PCA(n_components=min(n_samples, n_features))
                pca_temp.fit(X)
                
                n_components = int(np.sum(pca_temp.explained_variance_ > 1))
                n_components = max(1, min(n_components, max_components))
                
                logger.info(f"Kaiser criterion: {n_components} components (eigenvalues > 1)")
            
            else:
                # Default fallback
                n_components = min(max_components, max(1, n_features // 2))
                logger.info(f"Default heuristic: {n_components} components")
            
            return n_components
        
        
        def apply_dimensionality_reduction(X, method, method_params):
            # Apply dimensionality reduction using specified method
            # Parameters: X (data matrix), method, method_params
            # Returns: X_reduced, reducer object, metadata dict
            logger.info("="*80)
            logger.info("APPLYING DIMENSIONALITY REDUCTION")
            logger.info("="*80)
            logger.info(f"Method: {method}")
            logger.info(f"Input shape: {X.shape}")
            
            if method_params:
                logger.info(f"Parameters: {method_params}")
            
            logger.info("")
            
            # Get common parameters
            n_components = method_params.get('n_components', 'auto')
            random_state = method_params.get('random_state', 42)
            
            metadata = {
                'method': method,
                'params': method_params,
                'input_shape': list(X.shape)
            }
            
            try:
                # METHOD 1: PCA (Principal Component Analysis)
                if method == 'pca':
                    variance_threshold = method_params.get('variance_threshold', 0.95)
                    
                    if n_components == 'auto':
                        n_components = determine_n_components(
                            X,
                            method='variance',
                            variance_threshold=variance_threshold
                        )
                    else:
                        n_components = min(int(n_components), X.shape[0], X.shape[1])
                    
                    reducer = PCA(n_components=n_components, random_state=random_state)
                    X_reduced = reducer.fit_transform(X)
                    
                    metadata['n_components'] = int(n_components)
                    metadata['explained_variance'] = reducer.explained_variance_.tolist()
                    metadata['explained_variance_ratio'] = reducer.explained_variance_ratio_.tolist()
                    metadata['cumulative_variance'] = np.cumsum(
                        reducer.explained_variance_ratio_
                    ).tolist()
                    
                    logger.info(f"Components: {n_components}")
                    logger.info(
                        f"Explained variance: {metadata['cumulative_variance'][-1]*100:.2f}%"
                    )
                
                # METHOD 2: Kernel PCA (Non-linear PCA)
                elif method == 'kernel_pca':
                    if n_components == 'auto':
                        n_components = min(X.shape[1] // 2, 50)
                    else:
                        n_components = min(int(n_components), X.shape[0], X.shape[1])
                    
                    kernel = method_params.get('kernel', 'rbf')
                    gamma = method_params.get('gamma', None)
                    
                    reducer = KernelPCA(
                        n_components=n_components,
                        kernel=kernel,
                        gamma=gamma,
                        random_state=random_state,
                        n_jobs=-1
                    )
                    X_reduced = reducer.fit_transform(X)
                    
                    metadata['n_components'] = int(n_components)
                    metadata['kernel'] = kernel
                    metadata['gamma'] = gamma if gamma is not None else 'auto'
                    
                    logger.info(f"Components: {n_components}")
                    logger.info(f"Kernel: {kernel}")
                
                # METHOD 3: ICA (Independent Component Analysis)
                elif method == 'ica':
                    if n_components == 'auto':
                        n_components = min(X.shape[1], X.shape[0])
                    else:
                        n_components = min(int(n_components), X.shape[0], X.shape[1])
                    
                    max_iter = method_params.get('max_iter', 1000)
                    
                    reducer = FastICA(
                        n_components=n_components,
                        random_state=random_state,
                        max_iter=max_iter
                    )
                    X_reduced = reducer.fit_transform(X)
                    
                    metadata['n_components'] = int(n_components)
                    metadata['max_iter'] = max_iter
                    
                    logger.info(f"Components: {n_components}")
                    logger.info(f"Max iterations: {max_iter}")
                
                # METHOD 4: Factor Analysis
                elif method == 'factor_analysis':
                    if n_components == 'auto':
                        n_components = min(X.shape[1] // 2, X.shape[0])
                    else:
                        n_components = min(int(n_components), X.shape[0], X.shape[1])
                    
                    max_iter = method_params.get('max_iter', 1000)
                    
                    reducer = FactorAnalysis(
                        n_components=n_components,
                        random_state=random_state,
                        max_iter=max_iter
                    )
                    X_reduced = reducer.fit_transform(X)
                    
                    metadata['n_components'] = int(n_components)
                    
                    logger.info(f"Components: {n_components}")
                
                # METHOD 5: Sparse PCA
                elif method == 'sparse_pca':
                    if n_components == 'auto':
                        n_components = min(X.shape[1] // 2, X.shape[0])
                    else:
                        n_components = min(int(n_components), X.shape[0], X.shape[1])
                    
                    alpha = method_params.get('alpha', 1.0)
                    
                    reducer = SparsePCA(
                        n_components=n_components,
                        alpha=alpha,
                        random_state=random_state,
                        n_jobs=-1
                    )
                    X_reduced = reducer.fit_transform(X)
                    
                    metadata['n_components'] = int(n_components)
                    metadata['alpha'] = float(alpha)
                    
                    logger.info(f"Components: {n_components}")
                    logger.info(f"Sparsity parameter (alpha): {alpha}")
                
                # METHOD 6: t-SNE (for visualization, typically 2-3 components)
                elif method == 'tsne':
                    n_components = min(int(method_params.get('n_components', 2)), 3)
                    perplexity = method_params.get('perplexity', 30.0)
                    n_iter = method_params.get('n_iter', 1000)
                    
                    # Validate perplexity
                    max_perplexity = (X.shape[0] - 1) / 3.0
                    if perplexity > max_perplexity:
                        logger.warning(
                            f"Perplexity {perplexity} too large for {X.shape[0]} samples. "
                            f"Reducing to {max_perplexity:.1f}"
                        )
                        perplexity = max_perplexity
                    
                    reducer = TSNE(
                        n_components=n_components,
                        perplexity=perplexity,
                        n_iter=n_iter,
                        random_state=random_state,
                        n_jobs=-1
                    )
                    X_reduced = reducer.fit_transform(X)
                    
                    metadata['n_components'] = int(n_components)
                    metadata['perplexity'] = float(perplexity)
                    metadata['n_iter'] = int(n_iter)
                    
                    logger.info(f"Components: {n_components}")
                    logger.info(f"Perplexity: {perplexity}")
                    logger.info(f"Iterations: {n_iter}")
                
                # METHOD 7: Isomap (non-linear, manifold learning)
                elif method == 'isomap':
                    n_components = int(method_params.get('n_components', 2))
                    n_neighbors = method_params.get('n_neighbors', 5)
                    
                    # Validate n_neighbors
                    if n_neighbors >= X.shape[0]:
                        n_neighbors = max(2, X.shape[0] - 1)
                        logger.warning(f"Reduced n_neighbors to {n_neighbors}")
                    
                    reducer = Isomap(
                        n_components=n_components,
                        n_neighbors=n_neighbors,
                        n_jobs=-1
                    )
                    X_reduced = reducer.fit_transform(X)
                    
                    metadata['n_components'] = int(n_components)
                    metadata['n_neighbors'] = int(n_neighbors)
                    
                    logger.info(f"Components: {n_components}")
                    logger.info(f"Neighbors: {n_neighbors}")
                
                # METHOD 8: LLE (Locally Linear Embedding)
                elif method == 'lle':
                    n_components = int(method_params.get('n_components', 2))
                    n_neighbors = method_params.get('n_neighbors', 5)
                    
                    # Validate n_neighbors
                    if n_neighbors >= X.shape[0]:
                        n_neighbors = max(2, X.shape[0] - 1)
                        logger.warning(f"Reduced n_neighbors to {n_neighbors}")
                    
                    reducer = LocallyLinearEmbedding(
                        n_components=n_components,
                        n_neighbors=n_neighbors,
                        random_state=random_state,
                        n_jobs=-1
                    )
                    X_reduced = reducer.fit_transform(X)
                    
                    metadata['n_components'] = int(n_components)
                    metadata['n_neighbors'] = int(n_neighbors)
                    
                    logger.info(f"Components: {n_components}")
                    logger.info(f"Neighbors: {n_neighbors}")
                
                # METHOD 9: MDS (Multidimensional Scaling)
                elif method == 'mds':
                    n_components = int(method_params.get('n_components', 2))
                    
                    reducer = MDS(
                        n_components=n_components,
                        random_state=random_state,
                        n_jobs=-1
                    )
                    X_reduced = reducer.fit_transform(X)
                    
                    metadata['n_components'] = int(n_components)
                    
                    logger.info(f"Components: {n_components}")
                
                else:
                    raise ValueError(
                        f"Unknown dimensionality reduction method: {method}. "
                        f"Available: pca, kernel_pca, ica, factor_analysis, sparse_pca, "
                        f"tsne, isomap, lle, mds"
                    )
                
                # Calculate reduction statistics
                metadata['output_shape'] = list(X_reduced.shape)
                metadata['reduction_ratio'] = float(X_reduced.shape[1] / X.shape[1])
                metadata['features_removed'] = int(X.shape[1] - X_reduced.shape[1])
                
                logger.info("")
                logger.info(f"Output shape: {X_reduced.shape}")
                logger.info(f"Reduction: {X.shape[1]} â†’ {X_reduced.shape[1]} features")
                logger.info(f"Features removed: {metadata['features_removed']}")
                logger.info("")
                
                return X_reduced, reducer, metadata
                
            except Exception as e:
                logger.error(f"Error in dimensionality reduction: {str(e)}")
                raise
        
        
        def main():
            parser = argparse.ArgumentParser(
                description="Generic Dimensionality Reduction Component"
            )
            parser.add_argument("--input_data", required=True,
                               help="Path to input dataset")
            parser.add_argument("--method", default='pca',
                               help="Dimensionality reduction method")
            parser.add_argument("--method_params", type=str, default='{}',
                               help="Method-specific parameters as JSON")
            parser.add_argument("--output_data", required=True,
                               help="Output path for reduced dataset")
            parser.add_argument("--output_reducer", required=True,
                               help="Output path for fitted reducer")
            parser.add_argument("--output_metadata", required=True,
                               help="Output path for reduction metadata")
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("DIMENSIONALITY REDUCTION COMPONENT")
            logger.info("="*80)
            logger.info(f"Input: {args.input_data}")
            logger.info(f"Method: {args.method}")
            logger.info("")
            
            try:
                # Ensure output directories
                ensure_directory_exists(args.output_data)
                ensure_directory_exists(args.output_reducer)
                ensure_directory_exists(args.output_metadata)
                
                # Parse method parameters
                try:
                    method_params = json.loads(args.method_params)
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON in method_params: {e}")
                    logger.error(f"Received: {args.method_params}")
                    logger.error('Expected format: {"param": value} with double quotes')
                    sys.exit(1)
                
                # Load data
                df = load_data(args.input_data)
                
                if df.empty:
                    logger.error("ERROR: Dataset is empty")
                    sys.exit(1)
                
                # Extract numeric columns only
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
                
                logger.info(f"Numeric columns: {len(numeric_cols)}")
                logger.info(f"Non-numeric columns: {len(non_numeric_cols)}")
                
                if len(numeric_cols) == 0:
                    logger.error("ERROR: No numeric columns found for dimensionality reduction")
                    sys.exit(1)
                
                logger.info(f"Using {len(numeric_cols)} numeric columns for reduction")
                logger.info("")
                
                X = df[numeric_cols].values
                
                # Apply dimensionality reduction
                X_reduced, reducer, metadata = apply_dimensionality_reduction(
                    X=X,
                    method=args.method,
                    method_params=method_params
                )
                
                # Create DataFrame with reduced features
                n_comp = X_reduced.shape[1]
                component_names = [f"Component_{i+1}" for i in range(n_comp)]
                df_reduced = pd.DataFrame(X_reduced, columns=component_names)
                
                # Add back non-numeric columns if any
                if len(non_numeric_cols) > 0:
                    for col in non_numeric_cols:
                        df_reduced[col] = df[col].values
                    logger.info(f"Preserved {len(non_numeric_cols)} non-numeric columns")
                
                # Save reduced data
                df_reduced.to_csv(args.output_data, index=False)
                logger.info(f"Reduced data saved: {args.output_data}")
                
                # Save reducer
                with open(args.output_reducer, 'wb') as f:
                    pickle.dump(reducer, f)
                logger.info(f"Reducer saved: {args.output_reducer}")
                
                # Add column information to metadata
                metadata['original_columns'] = numeric_cols
                metadata['component_names'] = component_names
                metadata['non_numeric_columns'] = non_numeric_cols
                metadata['total_output_columns'] = len(component_names) + len(non_numeric_cols)
                
                # Save metadata
                with open(args.output_metadata, 'w') as f:
                    json.dump(metadata, f, indent=2)
                logger.info(f"Metadata saved: {args.output_metadata}")
                
                logger.info("")
                logger.info("="*80)
                logger.info("DIMENSIONALITY REDUCTION COMPLETED")
                logger.info("="*80)
                logger.info(f"Method: {args.method}")
                logger.info(f"Original dimensions: {X.shape[1]}")
                logger.info(f"Reduced dimensions: {X_reduced.shape[1]}")
                logger.info(f"Reduction ratio: {metadata['reduction_ratio']:.2%}")
                
                if 'cumulative_variance' in metadata:
                    logger.info(
                        f"Variance retained: {metadata['cumulative_variance'][-1]*100:.2f}%"
                    )
                
                logger.info(f"Final dataset: {df_reduced.shape[0]} rows x {df_reduced.shape[1]} columns")
                logger.info("="*80)
                
            except ValueError as e:
                logger.error(f"VALIDATION ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
            except Exception as e:
                logger.error(f"ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        
        if __name__ == "__main__":
            main()
    args:
      - --input_data
      - {inputPath: input_data}
      - --method
      - {inputValue: method}
      - --method_params
      - {inputValue: method_params}
      - --output_data
      - {outputPath: data}
      - --output_reducer
      - {outputPath: reducer}
      - --output_metadata
      - {outputPath: metadata}
