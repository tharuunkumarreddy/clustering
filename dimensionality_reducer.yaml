name: Dimensionality Reducer v1.0
description: |
  Dimensionality reduction for clustering preprocessing.
  Implements methods from section 3.3 of the framework:
  - Linear: PCA, Factor Analysis
  - Non-linear: Kernel PCA, t-SNE, UMAP (if available)
  - Auto-selection based on data characteristics

inputs:
  - {name: input_data, type: Dataset, description: 'Input dataset after outlier handling'}
  - {name: outlier_metadata, type: Data, description: 'Metadata from previous step', optional: true}
  - {name: method, type: String, default: "auto", description: 'Method: auto, none, pca, kernel_pca, factor_analysis, truncated_svd'}
  - {name: n_components, type: String, default: "auto", description: 'Number of components or "auto" for variance-based'}
  - {name: variance_threshold, type: Float, default: "0.95", description: 'Variance to retain for auto PCA (0-1)'}
  - {name: max_components, type: Integer, default: "50", description: 'Maximum components to keep'}
  - {name: kernel, type: String, default: "rbf", description: 'Kernel for kernel PCA: rbf, poly, sigmoid, linear'}
  - {name: standardize_before, type: String, default: "true", description: 'Standardize data before reduction'}

outputs:
  - {name: reduced_data, type: Dataset, description: 'Dimensionally reduced dataset'}
  - {name: reduction_metadata, type: Data, description: 'Reduction metadata with explained variance'}

implementation:
  container:
    image: python:3.9-slim
    command:
      - python3
      - -u
      - -c
      - |
        import os, sys, json, argparse
        import subprocess
        
        subprocess.run([sys.executable, "-m", "pip", "install", "-q", 
                       "pandas", "numpy", "pyarrow", "scikit-learn"], check=True)
        
        import pandas as pd
        import numpy as np
        from datetime import datetime
        from sklearn.decomposition import PCA, KernelPCA, FactorAnalysis, TruncatedSVD
        from sklearn.preprocessing import StandardScaler
        
        def ensure_dir(path):
            d = os.path.dirname(path)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)
        
        def select_auto_method(df):
            """Auto-select dimensionality reduction method"""
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            n_features = len(numeric_cols)
            n_samples = len(df)
            
            if n_features <= 10:
                return 'none'  # Low dimensional, no reduction needed
            elif n_features <= 50:
                return 'pca'  # Medium dimensional, use PCA
            elif n_features <= 100:
                return 'pca'  # Use PCA with auto components
            else:
                return 'truncated_svd'  # High dimensional, use Truncated SVD (faster)
        
        def determine_n_components(X, n_comp_str, variance_threshold, max_comp, method):
            """Determine optimal number of components"""
            n_samples, n_features = X.shape
            
            if n_comp_str == 'auto':
                if method == 'pca':
                    # Use variance threshold
                    temp_pca = PCA(n_components=min(n_samples, n_features))
                    temp_pca.fit(X)
                    cumsum = np.cumsum(temp_pca.explained_variance_ratio_)
                    n_comp = int(np.searchsorted(cumsum, variance_threshold) + 1)
                    n_comp = max(2, min(n_comp, max_comp, n_features))
                    selection_method = f'variance_threshold_{variance_threshold}'
                else:
                    # Use heuristic: sqrt or half of features
                    n_comp = max(2, min(int(np.sqrt(n_features)), max_comp, n_features))
                    selection_method = 'sqrt_heuristic'
            else:
                try:
                    n_comp = int(n_comp_str)
                    n_comp = max(1, min(n_comp, n_features))
                    selection_method = 'manual'
                except:
                    n_comp = max(2, min(int(n_features / 2), max_comp))
                    selection_method = 'fallback_half'
            
            return n_comp, selection_method
        
        def apply_pca(X, n_comp):
            """Apply standard PCA"""
            pca = PCA(n_components=n_comp, random_state=42)
            X_reduced = pca.fit_transform(X)
            
            explained_var = pca.explained_variance_ratio_
            cumulative_var = np.cumsum(explained_var)
            
            info = {
                'explained_variance_ratio': explained_var.tolist(),
                'cumulative_variance_ratio': cumulative_var.tolist(),
                'total_variance_explained': float(cumulative_var[-1]),
                'singular_values': pca.singular_values_.tolist() if hasattr(pca, 'singular_values_') else None
            }
            
            return X_reduced, info, pca
        
        def apply_kernel_pca(X, n_comp, kernel):
            """Apply Kernel PCA"""
            kpca = KernelPCA(n_components=n_comp, kernel=kernel, random_state=42, n_jobs=-1)
            X_reduced = kpca.fit_transform(X)
            
            info = {
                'kernel': kernel,
                'explained_variance_ratio': kpca.eigenvalues_.tolist() if hasattr(kpca, 'eigenvalues_') else None
            }
            
            return X_reduced, info, kpca
        
        def apply_factor_analysis(X, n_comp):
            """Apply Factor Analysis"""
            fa = FactorAnalysis(n_components=n_comp, random_state=42)
            X_reduced = fa.fit_transform(X)
            
            info = {
                'noise_variance': float(fa.noise_variance_.mean()) if hasattr(fa, 'noise_variance_') else None
            }
            
            return X_reduced, info, fa
        
        def apply_truncated_svd(X, n_comp):
            """Apply Truncated SVD (like PCA but works with sparse data)"""
            svd = TruncatedSVD(n_components=n_comp, random_state=42)
            X_reduced = svd.fit_transform(X)
            
            explained_var = svd.explained_variance_ratio_
            cumulative_var = np.cumsum(explained_var)
            
            info = {
                'explained_variance_ratio': explained_var.tolist(),
                'cumulative_variance_ratio': cumulative_var.tolist(),
                'total_variance_explained': float(cumulative_var[-1])
            }
            
            return X_reduced, info, svd
        
        # Main execution
        parser = argparse.ArgumentParser()
        parser.add_argument('--input_data', type=str, required=True)
        parser.add_argument('--outlier_metadata', type=str)
        parser.add_argument('--method', type=str, default='auto')
        parser.add_argument('--n_components', type=str, default='auto')
        parser.add_argument('--variance_threshold', type=float, default=0.95)
        parser.add_argument('--max_components', type=int, default=50)
        parser.add_argument('--kernel', type=str, default='rbf')
        parser.add_argument('--standardize_before', type=str, default='true')
        parser.add_argument('--reduced_data', type=str, required=True)
        parser.add_argument('--reduction_metadata', type=str, required=True)
        args = parser.parse_args()
        
        try:
            print("=" * 80)
            print("CLUSTERING PREPROCESSING - COMPONENT 4: DIMENSIONALITY REDUCER")
            print("=" * 80)
            
            # Load data
            df = pd.read_parquet(args.input_data)
            print(f"[LOAD] Input shape: {df.shape}")
            
            # Extract numeric columns
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
            
            print(f"[ANALYZE] Numeric columns: {len(numeric_cols)}")
            print(f"[ANALYZE] Categorical columns: {len(categorical_cols)}")
            
            if len(numeric_cols) == 0:
                print(f"\n[SKIP] No numeric columns, passing through data unchanged")
                df_out = df.copy()
                method_used = 'none'
                reduction_info = {'reason': 'no_numeric_columns'}
                n_comp = 0
                
            else:
                # Determine method
                method = args.method.lower()
                if method == 'auto':
                    method = select_auto_method(df)
                    print(f"\n[AUTO] Selected method: {method}")
                else:
                    print(f"\n[METHOD] Using: {method}")
                
                if method == 'none':
                    print(f"\n[SKIP] Dimensionality reduction disabled")
                    df_out = df.copy()
                    method_used = 'none'
                    reduction_info = {'reason': 'disabled'}
                    n_comp = len(numeric_cols)
                    
                else:
                    # Prepare numeric data
                    X = df[numeric_cols].values
                    
                    # Standardize if requested
                    standardize = args.standardize_before.lower() in ('true', '1', 'yes', 't')
                    if standardize:
                        print(f"\n[STANDARDIZE] Standardizing features before reduction...")
                        scaler = StandardScaler()
                        X = scaler.fit_transform(X)
                    
                    # Determine n_components
                    n_comp, selection_method = determine_n_components(
                        X, args.n_components, args.variance_threshold, 
                        args.max_components, method
                    )
                    
                    print(f"\n[REDUCE] Reducing from {X.shape[1]} to {n_comp} components")
                    print(f"  Component selection: {selection_method}")
                    
                    # Apply dimensionality reduction
                    if method == 'pca':
                        X_reduced, reduction_info, model = apply_pca(X, n_comp)
                    elif method == 'kernel_pca':
                        X_reduced, reduction_info, model = apply_kernel_pca(X, n_comp, args.kernel)
                    elif method == 'factor_analysis':
                        X_reduced, reduction_info, model = apply_factor_analysis(X, n_comp)
                    elif method == 'truncated_svd':
                        X_reduced, reduction_info, model = apply_truncated_svd(X, n_comp)
                    else:
                        print(f"  ⚠ Unknown method '{method}', using PCA")
                        X_reduced, reduction_info, model = apply_pca(X, n_comp)
                        method = 'pca'
                    
                    method_used = method
                    reduction_info['selection_method'] = selection_method
                    reduction_info['standardized'] = standardize
                    
                    # Create output dataframe
                    component_names = [f'component_{i+1}' for i in range(X_reduced.shape[1])]
                    df_reduced = pd.DataFrame(X_reduced, columns=component_names, index=df.index)
                    
                    # Add back categorical columns if any
                    if categorical_cols:
                        df_out = pd.concat([df_reduced, df[categorical_cols].reset_index(drop=True)], axis=1)
                        print(f"  Kept {len(categorical_cols)} categorical columns")
                    else:
                        df_out = df_reduced
                    
                    # Print variance info if available
                    if 'total_variance_explained' in reduction_info:
                        print(f"  Total variance explained: {reduction_info['total_variance_explained']*100:.2f}%")
            
            # Save outputs
            print(f"\n[SAVE] Saving outputs...")
            ensure_dir(args.reduced_data)
            ensure_dir(args.reduction_metadata)
            
            df_out.to_parquet(args.reduced_data, index=False)
            print(f"  ✓ Data: {args.reduced_data}")
            
            metadata = {
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'component': 'dimensionality_reducer',
                'method_used': method_used,
                'n_components': int(n_comp),
                'reduction_info': reduction_info,
                'shape_change': {
                    'before': {'rows': int(df.shape[0]), 'cols': int(df.shape[1]), 'numeric_cols': len(numeric_cols)},
                    'after': {'rows': int(df_out.shape[0]), 'cols': int(df_out.shape[1])}
                }
            }
            
            with open(args.reduction_metadata, 'w') as f:
                json.dump(metadata, f, indent=2)
            print(f"  ✓ Metadata: {args.reduction_metadata}")
            
            print("\n" + "=" * 80)
            print("COMPONENT 4 COMPLETE")
            print("=" * 80)
            print(f"Method: {method_used}")
            print(f"Dimensions: {df.shape[1]} → {df_out.shape[1]} columns")
            print(f"Final shape: {df_out.shape[0]} rows × {df_out.shape[1]} columns")
            print("=" * 80 + "\n")
            
        except Exception as e:
            print(f"\n[ERROR] {str(e)}", file=sys.stderr)
            import traceback
            traceback.print_exc()
            sys.exit(1)
    
    args:
      - --input_data
      - {inputPath: input_data}
      - --outlier_metadata
      - {inputPath: outlier_metadata}
      - --method
      - {inputValue: method}
      - --n_components
      - {inputValue: n_components}
      - --variance_threshold
      - {inputValue: variance_threshold}
      - --max_components
      - {inputValue: max_components}
      - --kernel
      - {inputValue: kernel}
      - --standardize_before
      - {inputValue: standardize_before}
      - --reduced_data
      - {outputPath: reduced_data}
      - --reduction_metadata
      - {outputPath: reduction_metadata}