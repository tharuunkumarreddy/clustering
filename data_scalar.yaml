name: Generic Scaling & Normalization Component
description: Comprehensive feature scaling and normalization for clustering with 11 methods. Includes Standard, Min-Max, Robust, MaxAbs, L1/L2 normalization, Quantile transformers, and Power transformers. Supports method-specific parameters via JSON with automatic distribution analysis.

inputs:
  - name: input_data
    type: Data
    description: 'Input dataset (CSV or Parquet)'
  - name: method
    type: String
    description: 'Scaling method: standard, zscore, minmax, robust, maxabs, l1, l2, quantile_uniform, quantile_normal, power_yeo-johnson, power_box-cox, none'
    default: 'standard'
  - name: method_params
    type: String
    description: 'Method-specific parameters as JSON string. Examples: {"feature_range":[0,1]}, {"quantile_range":[25,75]}, {"n_quantiles":1000}'
    default: '{}'

outputs:
  - name: output_data
    type: Data
    description: 'Scaled dataset (CSV format)'
  - name: scaler
    type: Model
    description: 'Fitted scaler for test data transformation (PKL format)'
  - name: metadata
    type: Data
    description: 'Scaling metadata and distribution analysis (JSON format)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pickle
        import pandas as pd
        import numpy as np
        from sklearn.preprocessing import (
            StandardScaler,
            MinMaxScaler,
            RobustScaler,
            MaxAbsScaler,
            Normalizer,
            QuantileTransformer,
            PowerTransformer
        )
        from pathlib import Path
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        logger = logging.getLogger('scaler')
        
        
        def ensure_directory_exists(file_path):
            #Create directory if it doesn't exist#
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
                logger.info(f"Created directory: {directory}")
        
        
        def load_data(input_path):
            #Load data from CSV or Parquet file#
            logger.info(f"Loading dataset from: {input_path}")
            
            ext = Path(input_path).suffix.lower()
            
            try:
                if ext in ['.parquet', '.pq']:
                    df = pd.read_parquet(input_path)
                    logger.info("Loaded Parquet file")
                else:
                    df = pd.read_csv(input_path)
                    logger.info("Loaded CSV file")
                
                logger.info(f"Shape: {df.shape[0]} rows x {df.shape[1]} columns")
                return df
                
            except Exception as e:
                logger.error(f"Error loading data: {str(e)}")
                raise
        
        
        def analyze_data_distribution(df):
            #
            Analyze data distribution to suggest appropriate scaler
            
            Parameters:
            -----------
            df : pd.DataFrame
                Input dataset
            
            Returns:
            --------
            analysis : dict
                Distribution analysis with statistics per column
            #
            logger.info("="*80)
            logger.info("ANALYZING DATA DISTRIBUTION")
            logger.info("="*80)
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) == 0:
                logger.warning("No numeric columns found")
                return {
                    'has_numeric': False,
                    'n_numeric_cols': 0
                }
            
            logger.info(f"Analyzing {len(numeric_cols)} numeric columns")
            logger.info("")
            
            analysis = {
                'has_numeric': True,
                'n_numeric_cols': len(numeric_cols),
                'column_stats': {}
            }
            
            for col in numeric_cols:
                col_data = df[col].dropna()
                
                if len(col_data) == 0:
                    logger.warning(f"  {col}: No valid data (all NaN)")
                    continue
                
                stats = {
                    'mean': float(col_data.mean()),
                    'std': float(col_data.std()),
                    'min': float(col_data.min()),
                    'max': float(col_data.max()),
                    'range': float(col_data.max() - col_data.min()),
                    'skewness': float(col_data.skew()),
                    'kurtosis': float(col_data.kurtosis())
                }
                
                # Check for outliers using IQR method
                Q1 = col_data.quantile(0.25)
                Q3 = col_data.quantile(0.75)
                IQR = Q3 - Q1
                outliers = ((col_data < Q1 - 1.5*IQR) | (col_data > Q3 + 1.5*IQR)).sum()
                stats['outlier_percentage'] = float(outliers / len(col_data) * 100)
                
                # Suggest appropriate scaler based on distribution
                if abs(stats['skewness']) > 1.0:
                    stats['suggested_scaler'] = 'power_transformer'
                    suggestion_reason = f"High skewness ({stats['skewness']:.2f})"
                elif stats['outlier_percentage'] > 5:
                    stats['suggested_scaler'] = 'robust'
                    suggestion_reason = f"Many outliers ({stats['outlier_percentage']:.1f}%)"
                elif stats['min'] >= 0 and stats['max'] <= 1:
                    stats['suggested_scaler'] = 'none (already normalized)'
                    suggestion_reason = "Already in [0,1] range"
                else:
                    stats['suggested_scaler'] = 'standard'
                    suggestion_reason = "Normal distribution"
                
                stats['suggestion_reason'] = suggestion_reason
                analysis['column_stats'][col] = stats
                
                logger.info(
                    f"  {col}:"
                )
                logger.info(
                    f"    Range: [{stats['min']:.3f}, {stats['max']:.3f}]"
                )
                logger.info(
                    f"    Mean: {stats['mean']:.3f}, Std: {stats['std']:.3f}"
                )
                logger.info(
                    f"    Skewness: {stats['skewness']:.3f}, Kurtosis: {stats['kurtosis']:.3f}"
                )
                logger.info(
                    f"    Outliers: {stats['outlier_percentage']:.1f}%"
                )
                logger.info(
                    f"    Suggested: {stats['suggested_scaler']} ({suggestion_reason})"
                )
                logger.info("")
            
            return analysis
        
        
        def apply_scaling(df, method, method_params):
            #
            Apply scaling to numeric columns
            
            Parameters:
            -----------
            df : pd.DataFrame
                Input dataset
            method : str
                Scaling method name
            method_params : dict
                Method-specific parameters
            
            Returns:
            --------
            df_scaled : pd.DataFrame
                Scaled dataset
            scaler : object
                Fitted scaler object
            metadata : dict
                Scaling metadata and statistics
            #
            logger.info("="*80)
            logger.info("APPLYING SCALING")
            logger.info("="*80)
            logger.info(f"Method: {method}")
            
            if method_params:
                logger.info(f"Parameters: {method_params}")
            
            logger.info("")
            
            df_scaled = df.copy()
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) == 0:
                logger.warning("No numeric columns to scale")
                return df_scaled, None, {
                    'method': 'none',
                    'reason': 'no_numeric_columns',
                    'n_columns_scaled': 0
                }
            
            logger.info(f"Scaling {len(numeric_cols)} numeric columns")
            
            metadata = {
                'method': method,
                'params': method_params,
                'n_columns_scaled': len(numeric_cols),
                'scaled_columns': numeric_cols
            }
            
            # METHOD 1: Standard Scaling (Z-score normalization)
            if method in ['standard', 'zscore']:
                with_mean = method_params.get('with_mean', True)
                with_std = method_params.get('with_std', True)
                
                scaler = StandardScaler(with_mean=with_mean, with_std=with_std)
                df_scaled[numeric_cols] = scaler.fit_transform(df[numeric_cols])
                
                metadata['with_mean'] = with_mean
                metadata['with_std'] = with_std
                
                logger.info("Standard scaling (z-score normalization) applied")
                logger.info(f"  with_mean: {with_mean}, with_std: {with_std}")
            
            # METHOD 2: Min-Max Scaling
            elif method == 'minmax':
                feature_range = method_params.get('feature_range', [0, 1])
                if isinstance(feature_range, list) and len(feature_range) == 2:
                    feature_range = tuple(feature_range)
                
                scaler = MinMaxScaler(feature_range=feature_range)
                df_scaled[numeric_cols] = scaler.fit_transform(df[numeric_cols])
                
                metadata['feature_range'] = list(feature_range)
                
                logger.info(f"Min-Max scaling to range {feature_range} applied")
            
            # METHOD 3: Robust Scaling (uses median and IQR)
            elif method == 'robust':
                quantile_range = method_params.get('quantile_range', [25.0, 75.0])
                if isinstance(quantile_range, list) and len(quantile_range) == 2:
                    quantile_range = tuple(quantile_range)
                
                with_centering = method_params.get('with_centering', True)
                with_scaling = method_params.get('with_scaling', True)
                
                scaler = RobustScaler(
                    quantile_range=quantile_range,
                    with_centering=with_centering,
                    with_scaling=with_scaling
                )
                df_scaled[numeric_cols] = scaler.fit_transform(df[numeric_cols])
                
                metadata['quantile_range'] = list(quantile_range)
                metadata['with_centering'] = with_centering
                metadata['with_scaling'] = with_scaling
                
                logger.info(f"Robust scaling (quantile range {quantile_range}) applied")
                logger.info(f"  with_centering: {with_centering}, with_scaling: {with_scaling}")
            
            # METHOD 4: MaxAbs Scaling
            elif method == 'maxabs':
                scaler = MaxAbsScaler()
                df_scaled[numeric_cols] = scaler.fit_transform(df[numeric_cols])
                
                logger.info("MaxAbs scaling applied (scales to [-1, 1])")
            
            # METHOD 5: L1/L2 Normalization (row-wise)
            elif method in ['l1', 'l2']:
                norm = method_params.get('norm', method)  # 'l1' or 'l2'
                
                scaler = Normalizer(norm=norm)
                # Normalizer works row-wise, not column-wise
                df_scaled[numeric_cols] = scaler.fit_transform(df[numeric_cols])
                
                metadata['norm'] = norm
                metadata['note'] = 'Row-wise normalization (not column-wise)'
                
                logger.info(f"{norm.upper()} normalization applied (row-wise)")
            
            # METHOD 6: Quantile Transformation (Uniform)
            elif method == 'quantile_uniform':
                n_quantiles = method_params.get('n_quantiles', 1000)
                
                # Validate n_quantiles
                n_samples = len(df)
                if n_quantiles > n_samples:
                    n_quantiles = n_samples
                    logger.warning(f"Reduced n_quantiles to {n_quantiles} (n_samples)")
                
                scaler = QuantileTransformer(
                    output_distribution='uniform',
                    n_quantiles=n_quantiles,
                    random_state=42
                )
                df_scaled[numeric_cols] = scaler.fit_transform(df[numeric_cols])
                
                metadata['n_quantiles'] = int(n_quantiles)
                metadata['output_distribution'] = 'uniform'
                
                logger.info(
                    f"Quantile transformation (uniform distribution, "
                    f"n_quantiles={n_quantiles}) applied"
                )
            
            # METHOD 7: Quantile Transformation (Normal)
            elif method == 'quantile_normal':
                n_quantiles = method_params.get('n_quantiles', 1000)
                
                # Validate n_quantiles
                n_samples = len(df)
                if n_quantiles > n_samples:
                    n_quantiles = n_samples
                    logger.warning(f"Reduced n_quantiles to {n_quantiles} (n_samples)")
                
                scaler = QuantileTransformer(
                    output_distribution='normal',
                    n_quantiles=n_quantiles,
                    random_state=42
                )
                df_scaled[numeric_cols] = scaler.fit_transform(df[numeric_cols])
                
                metadata['n_quantiles'] = int(n_quantiles)
                metadata['output_distribution'] = 'normal'
                
                logger.info(
                    f"Quantile transformation (normal distribution, "
                    f"n_quantiles={n_quantiles}) applied"
                )
            
            # METHOD 8: Power Transformation (Yeo-Johnson)
            elif method == 'power_yeo-johnson':
                standardize = method_params.get('standardize', True)
                
                scaler = PowerTransformer(
                    method='yeo-johnson',
                    standardize=standardize
                )
                df_scaled[numeric_cols] = scaler.fit_transform(df[numeric_cols])
                
                metadata['standardize'] = standardize
                metadata['power_method'] = 'yeo-johnson'
                
                logger.info("Power transformation (Yeo-Johnson) applied")
                logger.info(f"  Standardize: {standardize}")
            
            # METHOD 9: Power Transformation (Box-Cox)
            elif method == 'power_box-cox':
                standardize = method_params.get('standardize', True)
                
                # Box-Cox requires all positive values
                if (df[numeric_cols] > 0).all().all():
                    scaler = PowerTransformer(
                        method='box-cox',
                        standardize=standardize
                    )
                    df_scaled[numeric_cols] = scaler.fit_transform(df[numeric_cols])
                    
                    metadata['standardize'] = standardize
                    metadata['power_method'] = 'box-cox'
                    
                    logger.info("Power transformation (Box-Cox) applied")
                    logger.info(f"  Standardize: {standardize}")
                else:
                    logger.warning(
                        "Box-Cox requires positive values, falling back to Yeo-Johnson"
                    )
                    scaler = PowerTransformer(
                        method='yeo-johnson',
                        standardize=standardize
                    )
                    df_scaled[numeric_cols] = scaler.fit_transform(df[numeric_cols])
                    
                    metadata['standardize'] = standardize
                    metadata['power_method'] = 'yeo-johnson (fallback from box-cox)'
                    metadata['fallback_reason'] = 'Box-Cox requires all positive values'
                    
                    logger.info("Fallback: Power transformation (Yeo-Johnson) applied")
            
            # METHOD 10: No Scaling
            elif method == 'none':
                scaler = None
                logger.info("No scaling applied")
                return df_scaled, scaler, {
                    'method': 'none',
                    'n_columns_scaled': 0
                }
            
            else:
                raise ValueError(
                    f"Unknown scaling method: {method}. "
                    f"Available: standard, zscore, minmax, robust, maxabs, l1, l2, "
                    f"quantile_uniform, quantile_normal, power_yeo-johnson, "
                    f"power_box-cox, none"
                )
            
            # Calculate before/after statistics for first few columns (limit output size)
            sample_cols = numeric_cols[:5]  # Limit to 5 columns for metadata
            
            metadata['scaling_stats'] = {}
            for col in sample_cols:
                metadata['scaling_stats'][col] = {
                    'original_mean': float(df[col].mean()),
                    'original_std': float(df[col].std()),
                    'original_min': float(df[col].min()),
                    'original_max': float(df[col].max()),
                    'scaled_mean': float(df_scaled[col].mean()),
                    'scaled_std': float(df_scaled[col].std()),
                    'scaled_min': float(df_scaled[col].min()),
                    'scaled_max': float(df_scaled[col].max())
                }
            
            if len(numeric_cols) > 5:
                metadata['note'] = f'Scaling stats shown for first 5 of {len(numeric_cols)} columns'
            
            logger.info("")
            logger.info("Scaling completed successfully")
            logger.info("")
            
            return df_scaled, scaler, metadata
        
        
        def main():
            parser = argparse.ArgumentParser(
                description="Generic Scaling & Normalization Component"
            )
            parser.add_argument("--input_data", required=True,
                               help="Path to input dataset")
            parser.add_argument("--method", default='standard',
                               help="Scaling method")
            parser.add_argument("--method_params", type=str, default='{}',
                               help="Method-specific parameters as JSON")
            parser.add_argument("--output_output_data", required=True,
                               help="Output path for scaled dataset")
            parser.add_argument("--output_scaler", required=True,
                               help="Output path for fitted scaler")
            parser.add_argument("--output_metadata", required=True,
                               help="Output path for scaling metadata")
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("SCALING & NORMALIZATION COMPONENT")
            logger.info("="*80)
            logger.info(f"Input: {args.input_data}")
            logger.info(f"Method: {args.method}")
            logger.info("")
            
            try:
                # Ensure output directories
                ensure_directory_exists(args.output_output_data)
                ensure_directory_exists(args.output_scaler)
                ensure_directory_exists(args.output_metadata)
                
                # Parse method parameters
                try:
                    method_params = json.loads(args.method_params)
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON in method_params: {e}")
                    logger.error(f"Received: {args.method_params}")
                    logger.error("Expected format: '{\"param\": value}' (use double quotes)")
                    sys.exit(1)
                
                # Load data
                df = load_data(args.input_data)
                
                if df.empty:
                    logger.error("ERROR: Dataset is empty")
                    sys.exit(1)
                
                # Analyze distribution (provides insights and suggestions)
                distribution_analysis = analyze_data_distribution(df)
                
                # Apply scaling
                df_scaled, scaler, scaling_metadata = apply_scaling(
                    df=df,
                    method=args.method,
                    method_params=method_params
                )
                
                # Save scaled data
                df_scaled.to_csv(args.output_output_data, index=False)
                logger.info(f"Scaled data saved: {args.output_output_data}")
                
                # Save scaler
                with open(args.output_scaler, 'wb') as f:
                    pickle.dump(scaler, f)
                logger.info(f"Scaler saved: {args.output_scaler}")
                
                # Combine all metadata
                full_metadata = {
                    'distribution_analysis': distribution_analysis,
                    'scaling_metadata': scaling_metadata,
                    'method': args.method,
                    'method_params': method_params
                }
                
                # Save metadata
                with open(args.output_metadata, 'w') as f:
                    json.dump(full_metadata, f, indent=2)
                logger.info(f"Metadata saved: {args.output_metadata}")
                
                logger.info("")
                logger.info("="*80)
                logger.info("SCALING COMPLETED")
                logger.info("="*80)
                logger.info(f"Method: {args.method}")
                logger.info(f"Columns scaled: {scaling_metadata.get('n_columns_scaled', 0)}")
                logger.info(f"Final shape: {df_scaled.shape}")
                logger.info("="*80)
                
            except ValueError as e:
                logger.error(f"VALIDATION ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
            except Exception as e:
                logger.error(f"ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        
        if __name__ == "__main__":
            main()
    args:
      - --input_data
      - {inputPath: input_data}
      - --method
      - {inputValue: method}
      - --method_params
      - {inputValue: method_params}
      - --output_output_data
      - {outputPath: output_data}
      - --output_scaler
      - {outputPath: scaler}
      - --output_metadata
      - {outputPath: metadata}

