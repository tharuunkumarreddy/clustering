name: Data Scaler v1.0
description: |
  Data scaling/normalization for clustering preprocessing.
  Implements methods from section 3.4 of the framework:
  - Min-Max normalization
  - Z-score (StandardScaler)
  - Robust scaling
  - L1/L2 normalization
  - Decimal scaling
  
  Critical for distance-based clustering algorithms.

inputs:
  - {name: input_data, type: Dataset, description: 'Input dataset after dimensionality reduction'}
  - {name: reduction_metadata, type: Data, description: 'Metadata from previous step', optional: true}
  - {name: method, type: String, default: "auto", description: 'Method: auto, none, minmax, standard, robust, l1, l2, maxabs'}
  - {name: feature_range_min, type: Float, default: "0.0", description: 'Min value for MinMax scaler'}
  - {name: feature_range_max, type: Float, default: "1.0", description: 'Max value for MinMax scaler'}
  - {name: scale_categorical, type: String, default: "false", description: 'Apply label encoding to categorical before scaling'}

outputs:
  - {name: scaled_data, type: Dataset, description: 'Scaled dataset'}
  - {name: scaling_metadata, type: Data, description: 'Scaling metadata with statistics'}

implementation:
  container:
    image: python:3.9-slim
    command:
      - python3
      - -u
      - -c
      - |
        import os, sys, json, argparse
        import subprocess
        
        subprocess.run([sys.executable, "-m", "pip", "install", "-q", 
                       "pandas", "numpy", "pyarrow", "scikit-learn"], check=True)
        
        import pandas as pd
        import numpy as np
        from datetime import datetime
        from sklearn.preprocessing import (
            MinMaxScaler, StandardScaler, RobustScaler, 
            Normalizer, MaxAbsScaler, LabelEncoder
        )
        
        def ensure_dir(path):
            d = os.path.dirname(path)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)
        
        def select_auto_method(df):
            """Auto-select scaling method based on data characteristics"""
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            
            if len(numeric_cols) == 0:
                return 'none'
            
            # Check for outliers using IQR
            outlier_fraction = 0
            for col in numeric_cols:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = ((df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)).sum()
                outlier_fraction += outliers / len(df)
            
            outlier_fraction /= len(numeric_cols)
            
            # Check if data is already normalized
            all_in_01 = True
            for col in numeric_cols:
                if df[col].min() < -0.1 or df[col].max() > 1.1:
                    all_in_01 = False
                    break
            
            if all_in_01:
                return 'none'  # Already normalized
            elif outlier_fraction > 0.1:
                return 'robust'  # Many outliers, use robust scaling
            else:
                return 'standard'  # Normal case, use standard scaling
        
        def apply_minmax(X, feature_range):
            """MinMax scaling to specified range"""
            scaler = MinMaxScaler(feature_range=feature_range)
            X_scaled = scaler.fit_transform(X)
            
            info = {
                'feature_range': list(feature_range),
                'data_min': scaler.data_min_.tolist(),
                'data_max': scaler.data_max_.tolist(),
                'data_range': scaler.data_range_.tolist()
            }
            
            return X_scaled, info
        
        def apply_standard(X):
            """Z-score standardization (mean=0, std=1)"""
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            info = {
                'mean': scaler.mean_.tolist(),
                'std': np.sqrt(scaler.var_).tolist(),
                'variance': scaler.var_.tolist()
            }
            
            return X_scaled, info
        
        def apply_robust(X):
            """Robust scaling using median and IQR (resistant to outliers)"""
            scaler = RobustScaler()
            X_scaled = scaler.fit_transform(X)
            
            info = {
                'center': scaler.center_.tolist(),
                'scale': scaler.scale_.tolist()
            }
            
            return X_scaled, info
        
        def apply_l1_norm(X):
            """L1 normalization (Manhattan norm)"""
            normalizer = Normalizer(norm='l1')
            X_scaled = normalizer.fit_transform(X)
            
            info = {'norm_type': 'l1'}
            
            return X_scaled, info
        
        def apply_l2_norm(X):
            """L2 normalization (Euclidean norm)"""
            normalizer = Normalizer(norm='l2')
            X_scaled = normalizer.fit_transform(X)
            
            info = {'norm_type': 'l2'}
            
            return X_scaled, info
        
        def apply_maxabs(X):
            """MaxAbs scaling (scales to [-1, 1] by dividing by max absolute value)"""
            scaler = MaxAbsScaler()
            X_scaled = scaler.fit_transform(X)
            
            info = {
                'max_abs': scaler.max_abs_.tolist(),
                'scale': scaler.scale_.tolist()
            }
            
            return X_scaled, info
        
        def encode_categorical(df):
            """Simple label encoding for categorical columns"""
            df = df.copy()
            categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
            
            encoding_map = {}
            for col in categorical_cols:
                le = LabelEncoder()
                df[col] = le.fit_transform(df[col].astype(str))
                encoding_map[col] = {
                    'classes': le.classes_.tolist()[:100]  # Limit to first 100
                }
            
            return df, encoding_map, categorical_cols
        
        # Main execution
        parser = argparse.ArgumentParser()
        parser.add_argument('--input_data', type=str, required=True)
        parser.add_argument('--reduction_metadata', type=str)
        parser.add_argument('--method', type=str, default='auto')
        parser.add_argument('--feature_range_min', type=float, default=0.0)
        parser.add_argument('--feature_range_max', type=float, default=1.0)
        parser.add_argument('--scale_categorical', type=str, default='false')
        parser.add_argument('--scaled_data', type=str, required=True)
        parser.add_argument('--scaling_metadata', type=str, required=True)
        args = parser.parse_args()
        
        try:
            print("=" * 80)
            print("CLUSTERING PREPROCESSING - COMPONENT 5: DATA SCALER")
            print("=" * 80)
            
            # Load data
            df = pd.read_parquet(args.input_data)
            print(f"[LOAD] Input shape: {df.shape}")
            
            # Handle categorical encoding if requested
            scale_cat = args.scale_categorical.lower() in ('true', '1', 'yes', 't')
            encoding_info = None
            encoded_cols = []
            
            if scale_cat:
                print(f"\n[ENCODE] Label encoding categorical columns...")
                df, encoding_info, encoded_cols = encode_categorical(df)
                if encoded_cols:
                    print(f"  Encoded {len(encoded_cols)} categorical columns")
            
            # Extract numeric columns
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
            
            print(f"[ANALYZE] Numeric columns: {len(numeric_cols)}")
            print(f"[ANALYZE] Categorical columns: {len(categorical_cols)}")
            
            if len(numeric_cols) == 0:
                print(f"\n[SKIP] No numeric columns to scale, passing through data unchanged")
                df_out = df.copy()
                method_used = 'none'
                scaling_info = {'reason': 'no_numeric_columns'}
                
            else:
                # Determine method
                method = args.method.lower()
                if method == 'auto':
                    method = select_auto_method(df)
                    print(f"\n[AUTO] Selected method: {method}")
                else:
                    print(f"\n[METHOD] Using: {method}")
                
                if method == 'none':
                    print(f"\n[SKIP] Scaling disabled")
                    df_out = df.copy()
                    method_used = 'none'
                    scaling_info = {'reason': 'disabled'}
                    
                else:
                    # Prepare numeric data
                    X = df[numeric_cols].values
                    
                    print(f"\n[SCALE] Applying {method} scaling...")
                    
                    # Apply scaling
                    if method == 'minmax':
                        feature_range = (args.feature_range_min, args.feature_range_max)
                        X_scaled, scaling_info = apply_minmax(X, feature_range)
                    elif method == 'standard':
                        X_scaled, scaling_info = apply_standard(X)
                    elif method == 'robust':
                        X_scaled, scaling_info = apply_robust(X)
                    elif method == 'l1':
                        X_scaled, scaling_info = apply_l1_norm(X)
                    elif method == 'l2':
                        X_scaled, scaling_info = apply_l2_norm(X)
                    elif method == 'maxabs':
                        X_scaled, scaling_info = apply_maxabs(X)
                    else:
                        print(f"  ⚠ Unknown method '{method}', using standard")
                        X_scaled, scaling_info = apply_standard(X)
                        method = 'standard'
                    
                    method_used = method
                    
                    # Create output dataframe
                    df_scaled = pd.DataFrame(X_scaled, columns=numeric_cols, index=df.index)
                    
                    # Add back categorical columns if any
                    if categorical_cols:
                        df_out = pd.concat([df_scaled, df[categorical_cols].reset_index(drop=True)], axis=1)
                    else:
                        df_out = df_scaled
                    
                    print(f"  ✓ Scaled {len(numeric_cols)} numeric columns")
                    
                    # Print scaling statistics
                    if method == 'minmax':
                        print(f"  Feature range: [{args.feature_range_min}, {args.feature_range_max}]")
                    elif method == 'standard':
                        print(f"  Mean: 0, Std: 1")
            
            # Save outputs
            print(f"\n[SAVE] Saving outputs...")
            ensure_dir(args.scaled_data)
            ensure_dir(args.scaling_metadata)
            
            df_out.to_parquet(args.scaled_data, index=False)
            print(f"  ✓ Data: {args.scaled_data}")
            
            metadata = {
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'component': 'data_scaler',
                'method_used': method_used,
                'scaling_info': scaling_info,
                'encoding_info': encoding_info,
                'encoded_columns': encoded_cols,
                'shape': {
                    'rows': int(df_out.shape[0]),
                    'cols': int(df_out.shape[1]),
                    'numeric_cols': len([c for c in df_out.columns if c in numeric_cols or c in encoded_cols]),
                    'categorical_cols': len([c for c in df_out.columns if c not in numeric_cols and c not in encoded_cols])
                }
            }
            
            with open(args.scaling_metadata, 'w') as f:
                json.dump(metadata, f, indent=2)
            print(f"  ✓ Metadata: {args.scaling_metadata}")
            
            print("\n" + "=" * 80)
            print("COMPONENT 5 COMPLETE")
            print("=" * 80)
            print(f"Method: {method_used}")
            if encoded_cols:
                print(f"Encoded {len(encoded_cols)} categorical columns")
            print(f"Final shape: {df_out.shape[0]} rows × {df_out.shape[1]} columns")
            print("=" * 80 + "\n")
            
        except Exception as e:
            print(f"\n[ERROR] {str(e)}", file=sys.stderr)
            import traceback
            traceback.print_exc()
            sys.exit(1)
    
    args:
      - --input_data
      - {inputPath: input_data}
      - --reduction_metadata
      - {inputPath: reduction_metadata}
      - --method
      - {inputValue: method}
      - --feature_range_min
      - {inputValue: feature_range_min}
      - --feature_range_max
      - {inputValue: feature_range_max}
      - --scale_categorical
      - {inputValue: scale_categorical}
      - --scaled_data
      - {outputPath: scaled_data}
      - --scaling_metadata
      - {outputPath: scaling_metadata}