name: Clustering Model Versioning Component
description: |
  Model versioning system with lineage tracking for clustering models.
  Only versions models that passed validation gate. Supports semantic versioning,
  auto-increment, and timestamp strategies. Maintains central model registry with
  complete provenance and deployment tracking.

inputs:
  - name: model
    type: Model
    description: 'Trained clustering model (PKL format from training component)'
  - name: training_metadata
    type: Data
    description: 'Training metadata JSON from training component'
  - name: evaluation_results
    type: Data
    description: 'Evaluation results JSON from evaluation component'
  - name: validation_results
    type: Data
    description: 'Validation results JSON from validation gate (REQUIRED!)'
  - name: version
    type: String
    description: 'Manual version string (e.g., v1.0.0). If empty, uses version_strategy'
    default: ''
  - name: version_strategy
    type: String
    description: 'Version generation strategy: auto, semantic, or timestamp'
    default: 'semantic'
  - name: registry_path
    type: String
    description: 'Path to model registry JSON file'
    default: 'model_registry.json'

outputs:
  - name: versioned_model
    type: Model
    description: 'Versioned model copy with complete metadata'
  - name: version_metadata
    type: Data
    description: 'Complete version metadata with lineage (JSON format)'
  - name: versioning_result
    type: Data
    description: 'Versioning operation result summary (JSON format)'
  - name: model_registry
    type: Data
    description: 'Updated model registry with all versions (JSON format)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import sys
        import shutil
        import hashlib
        from datetime import datetime
        
        
        class ModelVersioning:
            
            def __init__(self, registry_path='model_registry.json'):
                self.registry_path = registry_path
                self.registry = self._load_registry()
            
            def _load_registry(self):
                if os.path.exists(self.registry_path):
                    with open(self.registry_path, 'r') as f:
                        return json.load(f)
                else:
                    return {
                        'versions': [],
                        'production': None,
                        'latest': None,
                        'created_at': datetime.now().isoformat(),
                        'total_models_trained': 0,
                        'total_models_approved': 0,
                        'total_models_deployed': 0
                    }
            
            def _save_registry(self):
                os.makedirs(os.path.dirname(self.registry_path) or '.', exist_ok=True)
                with open(self.registry_path, 'w') as f:
                    json.dump(self.registry, f, indent=2)
            
            def check_validation_approval(self, validation_results):
                approved = validation_results.get('deploy_approved', False)
                
                print('')
                print("="*80)
                print("VALIDATION STATUS CHECK")
                print("="*80)
                
                if approved:
                    print('')
                    print('Validation Status: APPROVED')
                    checks_passed = validation_results.get('checks_passed', 0)
                    checks_total = validation_results.get('checks_total', 0)
                    print('  Checks Passed:', str(checks_passed) + '/' + str(checks_total))
                    print('  Decision:', validation_results.get('decision_reason', 'Unknown'))
                else:
                    print('')
                    print('Validation Status: REJECTED')
                    checks_passed = validation_results.get('checks_passed', 0)
                    checks_total = validation_results.get('checks_total', 0)
                    checks_failed = validation_results.get('checks_failed', 0)
                    print('  Checks Passed:', str(checks_passed) + '/' + str(checks_total))
                    print('  Checks Failed:', checks_failed)
                    print('  Decision:', validation_results.get('decision_reason', 'Unknown'))
                    
                    if validation_results.get('failed_checks'):
                        print('')
                        print('  Failed Checks:')
                        for check in validation_results['failed_checks']:
                            print('    -', check)
                
                print("="*80)
                
                return approved
            
            def generate_version(self, strategy, metadata, metrics):
                if strategy == 'auto':
                    return self._generate_auto_version()
                elif strategy == 'semantic':
                    return self._generate_semantic_version(metadata, metrics)
                elif strategy == 'timestamp':
                    return self._generate_timestamp_version()
                else:
                    raise ValueError('Unknown version strategy: ' + str(strategy))
            
            def _generate_auto_version(self):
                if not self.registry['versions']:
                    return 'v1.0.0'
                
                latest = self.registry['versions'][-1]['version']
                major, minor, patch = self._parse_version(latest)
                return 'v' + str(major) + '.' + str(minor) + '.' + str(patch + 1)
            
            def _generate_semantic_version(self, metadata, metrics):
                if not self.registry['versions']:
                    return 'v1.0.0'
                
                latest_entry = self.registry['versions'][-1]
                latest_version = latest_entry['version']
                major, minor, patch = self._parse_version(latest_version)
                
                current_algo = metadata.get('algorithm')
                previous_algo = latest_entry.get('algorithm')
                
                if current_algo != previous_algo:
                    print('')
                    print('  Algorithm changed:', previous_algo, '->', current_algo)
                    print('  Versioning: Major bump (v' + str(major) + '.x.x -> v' + str(major+1) + '.0.0)')
                    return 'v' + str(major + 1) + '.0.0'
                
                current_sil = metrics.get('internal_metrics', {}).get('silhouette_score', 0)
                previous_sil = latest_entry.get('silhouette_score', 0)
                
                improvement = (current_sil - previous_sil) / (previous_sil + 1e-10)
                if improvement > 0.1:
                    print('')
                    print('  Significant improvement:', round(previous_sil, 3), '->', round(current_sil, 3), '(+' + str(round(improvement*100, 1)) + '%)')
                    print('  Versioning: Minor bump (v' + str(major) + '.' + str(minor) + '.x -> v' + str(major) + '.' + str(minor+1) + '.0)')
                    return 'v' + str(major) + '.' + str(minor + 1) + '.0'
                
                print('')
                print('  Retrain with similar config')
                print('  Versioning: Patch bump (v' + str(major) + '.' + str(minor) + '.' + str(patch) + ' -> v' + str(major) + '.' + str(minor) + '.' + str(patch+1) + ')')
                return 'v' + str(major) + '.' + str(minor) + '.' + str(patch + 1)
            
            def _generate_timestamp_version(self):
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                return 'v' + timestamp
            
            def _parse_version(self, version):
                version = version.lstrip('v')
                parts = version.split('.')
                return int(parts[0]), int(parts[1]), int(parts[2])
            
            def _calculate_file_hash(self, filepath):
                sha256_hash = hashlib.sha256()
                with open(filepath, 'rb') as f:
                    for byte_block in iter(lambda: f.read(4096), b""):
                        sha256_hash.update(byte_block)
                return sha256_hash.hexdigest()
            
            def version_model(
                self,
                model_path,
                training_metadata,
                evaluation_results,
                validation_results,
                version,
                output_model_path,
                output_metadata_path
            ):
                print('')
                print("="*80)
                print("MODEL VERSIONING")
                print("="*80)
                
                print('')
                print('Version:', version)
                
                model_hash = self._calculate_file_hash(model_path)
                print('Model hash:', model_hash[:16] + '...')
                
                os.makedirs(os.path.dirname(output_model_path), exist_ok=True)
                shutil.copy2(model_path, output_model_path)
                model_size = os.path.getsize(output_model_path) / 1024
                print('Model copied:', output_model_path, '(' + str(round(model_size, 2)) + ' KB)')
                
                algorithm = training_metadata.get('algorithm', 'unknown')
                training_results = training_metadata.get('training_results', {})
                internal_metrics = evaluation_results.get('internal_metrics', {})
                basic_stats = evaluation_results.get('basic_statistics', {})
                
                version_metadata = {
                    'version': version,
                    'created_at': datetime.now().isoformat(),
                    'model_hash': model_hash,
                    
                    'training': {
                        'algorithm': algorithm,
                        'category': training_metadata.get('category', 'unknown'),
                        'parameters': training_metadata.get('parameters', {}),
                        'train_samples': training_results.get('n_train_samples'),
                        'n_features': training_results.get('n_features'),
                        'n_clusters': training_results.get('n_clusters'),
                        'training_complete': training_metadata.get('training_complete', False)
                    },
                    
                    'evaluation': {
                        'dataset': evaluation_results.get('dataset', 'test'),
                        'test_samples': basic_stats.get('n_samples'),
                        'silhouette_score': internal_metrics.get('silhouette_score'),
                        'davies_bouldin_score': internal_metrics.get('davies_bouldin_score'),
                        'calinski_harabasz_score': internal_metrics.get('calinski_harabasz_score'),
                        'separation_ratio': evaluation_results.get('cluster_quality', {}).get('separation_ratio'),
                        'noise_percentage': basic_stats.get('noise_percentage', 0.0)
                    },
                    
                    'validation': {
                        'deploy_approved': validation_results.get('deploy_approved', False),
                        'checks_passed': validation_results.get('checks_passed', 0),
                        'checks_total': validation_results.get('checks_total', 0),
                        'checks_failed': validation_results.get('checks_failed', 0),
                        'failed_checks': validation_results.get('failed_checks', []),
                        'decision_reason': validation_results.get('decision_reason', 'Unknown'),
                        'validation_timestamp': validation_results.get('timestamp', 'Unknown'),
                        'thresholds_used': validation_results.get('thresholds_used', {})
                    },
                    
                    'status': 'validated',
                    'deployment_approved': validation_results.get('deploy_approved', False),
                    'deployed_at': None,
                    'production_serving': False
                }
                
                os.makedirs(os.path.dirname(output_metadata_path), exist_ok=True)
                with open(output_metadata_path, 'w') as f:
                    json.dump(version_metadata, f, indent=2)
                
                metadata_size = os.path.getsize(output_metadata_path) / 1024
                print('Metadata saved:', output_metadata_path, '(' + str(round(metadata_size, 2)) + ' KB)')
                
                registry_entry = {
                    'version': version,
                    'created_at': version_metadata['created_at'],
                    'algorithm': algorithm,
                    'n_clusters': version_metadata['training']['n_clusters'],
                    'silhouette_score': version_metadata['evaluation']['silhouette_score'],
                    'validation_approved': validation_results.get('deploy_approved', False),
                    'status': 'validated',
                    'model_path': output_model_path
                }
                
                self.registry['versions'].append(registry_entry)
                self.registry['latest'] = version
                self.registry['total_models_trained'] += 1
                self.registry['total_models_approved'] += 1
                self._save_registry()
                
                print('Registry updated:', self.registry_path)
                
                self._print_version_summary(version_metadata)
                
                return {
                    'version': version,
                    'model_path': output_model_path,
                    'metadata_path': output_metadata_path,
                    'model_hash': model_hash,
                    'created_at': version_metadata['created_at'],
                    'validation_approved': validation_results.get('deploy_approved', False),
                    'registry_path': self.registry_path
                }
            
            def _print_version_summary(self, metadata):
                print('')
                print("="*80)
                print('MODEL VERSION:', metadata['version'])
                print("="*80)
                
                print('')
                print('Model Information:')
                print('  Algorithm:', metadata['training']['algorithm'])
                print('  Clusters:', metadata['training']['n_clusters'])
                print('  Training samples:', metadata['training']['train_samples'])
                print('  Features:', metadata['training']['n_features'])
                
                print('')
                print('Quality Metrics:')
                print('  Silhouette Score:', round(metadata['evaluation']['silhouette_score'], 4))
                print('  Davies-Bouldin:', round(metadata['evaluation']['davies_bouldin_score'], 4))
                print('  Separation Ratio:', round(metadata['evaluation']['separation_ratio'], 4))
                
                print('')
                print('Validation Status:')
                print('  Approved:', 'YES' if metadata['validation']['deploy_approved'] else 'NO')
                checks_passed = metadata['validation']['checks_passed']
                checks_total = metadata['validation']['checks_total']
                print('  Checks Passed:', str(checks_passed) + '/' + str(checks_total))
                
                print('')
                print('Provenance:')
                print('  Created:', metadata['created_at'])
                print('  Hash:', metadata['model_hash'][:16] + '...')
                print('  Status:', metadata['status'])
                
                print("="*80)
            
            def get_registry_info(self):
                return {
                    'total_versions': len(self.registry['versions']),
                    'total_trained': self.registry.get('total_models_trained', 0),
                    'total_approved': self.registry.get('total_models_approved', 0),
                    'total_deployed': self.registry.get('total_models_deployed', 0),
                    'latest_version': self.registry.get('latest'),
                    'production_version': self.registry.get('production'),
                    'all_versions': [v['version'] for v in self.registry['versions']]
                }
        
        
        def load_json(filepath, description):
            print('')
            print('Loading', description, 'from:', filepath)
            
            if not os.path.exists(filepath):
                raise FileNotFoundError(description + ' not found: ' + str(filepath))
            
            with open(filepath, 'r') as f:
                data = json.load(f)
            
            print('Loaded successfully')
            return data
        
        
        def main():
            parser = argparse.ArgumentParser(
                description='Model Versioning for Clustering Models'
            )
            
            parser.add_argument('--model', required=True,
                               help='Path to trained model PKL')
            parser.add_argument('--training_metadata', required=True,
                               help='Path to training metadata JSON')
            parser.add_argument('--evaluation_results', required=True,
                               help='Path to evaluation results JSON')
            parser.add_argument('--validation_results', required=True,
                               help='Path to validation results JSON')
            parser.add_argument('--version', default='',
                               help='Manual version string')
            parser.add_argument('--version_strategy', default='semantic',
                               help='Version generation strategy')
            parser.add_argument('--registry_path', default='model_registry.json',
                               help='Path to model registry')
            
            parser.add_argument('--output_versioned_model', required=True,
                               help='Output path for versioned model')
            parser.add_argument('--output_version_metadata', required=True,
                               help='Output path for version metadata')
            parser.add_argument('--output_versioning_result', required=True,
                               help='Output path for versioning result')
            parser.add_argument('--output_model_registry', required=True,
                               help='Output path for model registry')
            
            args = parser.parse_args()
            
            try:
                print("="*80)
                print("MODEL VERSIONING FOR CLUSTERING")
                print("="*80)
                print('')
                print("IMPORTANT: This component requires validation results!")
                print("Only models approved by validation gate will be versioned.")
                print("="*80)
                
                training_metadata = load_json(args.training_metadata, "training metadata")
                evaluation_results = load_json(args.evaluation_results, "evaluation results")
                validation_results = load_json(args.validation_results, "validation results")
                
                versioning = ModelVersioning(registry_path=args.registry_path)
                
                if not versioning.check_validation_approval(validation_results):
                    print('')
                    print("="*80)
                    print("MODEL VERSIONING ABORTED")
                    print("="*80)
                    print('')
                    print("Reason: Model was REJECTED by validation gate")
                    print('')
                    print("Recommendations:")
                    print("  1. Review failed validation checks")
                    print("  2. Retrain model with different parameters")
                    print("  3. Try different algorithm")
                    print("  4. Review data quality")
                    print("="*80)
                    
                    os.makedirs(os.path.dirname(args.output_versioned_model), exist_ok=True)
                    os.makedirs(os.path.dirname(args.output_version_metadata), exist_ok=True)
                    os.makedirs(os.path.dirname(args.output_versioning_result), exist_ok=True)
                    os.makedirs(os.path.dirname(args.output_model_registry), exist_ok=True)
                    
                    rejection_result = {
                        'success': False,
                        'reason': 'Model rejected by validation gate',
                        'validation_results': validation_results
                    }
                    with open(args.output_versioning_result, 'w') as f:
                        json.dump(rejection_result, f, indent=2)
                    
                    versioning._save_registry()
                    shutil.copy2(args.registry_path, args.output_model_registry)
                    
                    return
                
                if args.version:
                    version = args.version
                    print('')
                    print('Using manual version:', version)
                else:
                    version = versioning.generate_version(
                        strategy=args.version_strategy,
                        metadata=training_metadata,
                        metrics=evaluation_results
                    )
                    print('')
                    print('Generated version (' + args.version_strategy + '):', version)
                
                result = versioning.version_model(
                    model_path=args.model,
                    training_metadata=training_metadata,
                    evaluation_results=evaluation_results,
                    validation_results=validation_results,
                    version=version,
                    output_model_path=args.output_versioned_model,
                    output_metadata_path=args.output_version_metadata
                )
                
                result['success'] = True
                os.makedirs(os.path.dirname(args.output_versioning_result), exist_ok=True)
                with open(args.output_versioning_result, 'w') as f:
                    json.dump(result, f, indent=2)
                
                print('')
                print('Versioning result saved:', args.output_versioning_result)
                
                shutil.copy2(args.registry_path, args.output_model_registry)
                print('Model registry saved:', args.output_model_registry)
                
                registry_info = versioning.get_registry_info()
                
                print('')
                print("="*80)
                print("VERSIONING SUMMARY")
                print("="*80)
                
                print('')
                print('Model Successfully Versioned:')
                print('  Version:', result['version'])
                print('  Model:', result['model_path'])
                print('  Metadata:', result['metadata_path'])
                print('  Validation Approved:', 'YES' if result['validation_approved'] else 'NO')
                
                print('')
                print('Model Registry:')
                print('  Total versions:', registry_info['total_versions'])
                print('  Total trained:', registry_info['total_trained'])
                print('  Total approved:', registry_info['total_approved'])
                print('  Latest:', registry_info['latest_version'])
                
                print('')
                print('Next Steps:')
                print('  1. Review version metadata')
                print('  2. Ready for deployment to production')
                print('  3. Set up monitoring and alerting')
                
                print('')
                print("="*80)
                print("MODEL VERSIONING COMPLETED SUCCESSFULLY")
                
            except Exception as e:
                print('')
                print('ERROR:', str(e))
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        
        if __name__ == '__main__':
            main()

    args:
      - --model
      - {inputPath: model}
      - --training_metadata
      - {inputPath: training_metadata}
      - --evaluation_results
      - {inputPath: evaluation_results}
      - --validation_results
      - {inputPath: validation_results}
      - --version
      - {inputValue: version}
      - --version_strategy
      - {inputValue: version_strategy}
      - --registry_path
      - {inputValue: registry_path}
      - --output_versioned_model
      - {outputPath: versioned_model}
      - --output_version_metadata
      - {outputPath: version_metadata}
      - --output_versioning_result
      - {outputPath: versioning_result}
      - --output_model_registry
      - {outputPath: model_registry}
