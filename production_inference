name: Production Clustering Inference Component
description: |
  Production-ready inference with JSON input and automatic evaluation.
  Supports both single samples and batches with quality assessment.
  
  Features:
  - JSON/dict input (no CSV required)
  - Single sample or batch prediction
  - Automatic quality evaluation (internal metrics)
  - REST API ready
  - Complete prediction report with quality assessment

inputs:
  - name: model
    type: Model
    description: 'Trained clustering model (PKL format)'
  - name: metadata
    type: Data
    description: 'Training or version metadata (JSON format)'
  - name: input_data
    type: String
    description: 'JSON string with input data (single dict or list of dicts)'
  - name: evaluate
    type: String
    description: 'Whether to evaluate predictions with quality metrics (true/false)'
    default: 'true'

outputs:
  - name: predictions
    type: Data
    description: 'Cluster predictions with quality evaluation (JSON format)'
  - name: prediction_summary
    type: Data
    description: 'Human-readable prediction summary (TXT format)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import pickle
        import argparse
        import warnings
        import shutil
        import numpy as np
        import pandas as pd
        from datetime import datetime
        
        warnings.filterwarnings('ignore')
        
        from sklearn.metrics import (
            silhouette_score,
            davies_bouldin_score,
            calinski_harabasz_score
        )
        
        
        def load_model(model_path):
            print('')
            print('Loading model from:', model_path)
            
            try:
                with open(model_path, 'rb') as f:
                    model = pickle.load(f)
                print('Model loaded successfully')
                return model
            except Exception as e:
                print('Error loading model:', str(e))
                raise
        
        
        def load_metadata(metadata_path):
            print('')
            print('Loading metadata from:', metadata_path)
            
            try:
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)
                print('Metadata loaded successfully')
                return metadata
            except Exception as e:
                print('Error loading metadata:', str(e))
                raise
        
        
        def extract_model_info(metadata):
            print('')
            print('Extracting model information')
            
            # Support both version_metadata.json and training_metadata.json formats
            if 'training' in metadata:
                # version_metadata.json format (from versioned models)
                algorithm = metadata['training']['algorithm']
                n_clusters = metadata['training']['n_clusters']
                n_features = metadata['training']['n_features']
                feature_names = metadata.get('feature_names', None)
                version = metadata.get('version', 'unknown')
            else:
                # training_metadata.json format (from training component)
                algorithm = metadata['algorithm']
                n_clusters = metadata['training_results']['n_clusters']
                n_features = metadata['training_results']['n_features']
                feature_names = metadata.get('feature_names', None)
                version = 'unknown'
            
            print('Algorithm:', algorithm)
            print('Clusters:', n_clusters)
            print('Features:', n_features)
            print('Version:', version)
            
            return {
                'algorithm': algorithm,
                'n_clusters': n_clusters,
                'n_features': n_features,
                'feature_names': feature_names,
                'version': version
            }
        
        
        def parse_input_data(input_data_str, model_info):
            print('')
            print('Parsing input data')
            
            try:
                input_data = json.loads(input_data_str)
            except json.JSONDecodeError as e:
                raise ValueError('Invalid JSON in input_data: ' + str(e))
            
            feature_names = model_info['feature_names']
            n_features = model_info['n_features']
            
            # Case 1: Single sample (dict)
            if isinstance(input_data, dict):
                print('Input type: single sample (dict)')
                
                # Extract features in correct order
                if feature_names:
                    features = [input_data[name] for name in feature_names]
                else:
                    # Auto-detect from input keys
                    input_keys = list(input_data.keys())
                    if len(input_keys) == n_features:
                        features = [input_data[key] for key in input_keys]
                        feature_names = input_keys
                        model_info['feature_names'] = feature_names
                    else:
                        # Use values in order as fallback
                        if len(input_data) == n_features:
                            features = list(input_data.values())
                        else:
                            raise ValueError('Feature count mismatch: expected ' + str(n_features) + ', got ' + str(len(input_data)))
                
                X = np.array(features).reshape(1, -1)
                n_samples = 1
                input_type = 'single_sample'
            
            # Case 2: Batch (list of dicts)
            elif isinstance(input_data, list):
                print('Input type: batch (list of dicts)')
                
                if not input_data:
                    raise ValueError('Empty input list')
                
                # Convert to DataFrame then numpy
                df = pd.DataFrame(input_data)
                
                if feature_names:
                    X = df[feature_names].values
                else:
                    # Auto-detect feature names from first sample
                    input_keys = list(input_data[0].keys())
                    if len(input_keys) == n_features:
                        feature_names = input_keys
                        model_info['feature_names'] = feature_names
                        X = df[feature_names].values
                    else:
                        X = df.values
                
                n_samples = len(input_data)
                input_type = 'batch'
            
            else:
                raise ValueError('Input must be dict (single sample) or list of dicts (batch)')
            
            print('Parsed successfully')
            print('Samples:', n_samples)
            print('Features:', X.shape[1])
            
            return X, n_samples, input_type
        
        
        def validate_input(X, model_info):
            print('')
            print('Validating input')
            
            n_features = model_info['n_features']
            
            # Check feature count
            if X.shape[1] != n_features:
                raise ValueError('Feature mismatch: model expects ' + str(n_features) + ' features, got ' + str(X.shape[1]))
            
            # Check for NaN/Inf
            if not np.isfinite(X).all():
                raise ValueError('Input contains NaN or Inf values')
            
            print('Validation passed')
        
        
        def predict_clusters(model, X, model_info):
            print('')
            print('Predicting clusters')
            
            # Check if model has predict method
            if not hasattr(model, 'predict'):
                raise ValueError(str(model_info['algorithm']) + ' does not support native predict(). This component requires models with predict() method.')
            
            predictions = model.predict(X)
            
            print('Predictions generated:', len(predictions), 'samples')
            
            # Show preview
            unique_labels = np.unique(predictions)
            print('Clusters found:', len(unique_labels))
            for label in unique_labels:
                count = np.sum(predictions == label)
                print('  Cluster', label, ':', count, 'samples')
            
            return predictions
        
        
        def evaluate_predictions(X, predictions):
            print('')
            print('Evaluating prediction quality')
            
            # Check if enough samples
            if len(predictions) < 2:
                print('Skipping evaluation: requires at least 2 samples')
                return {
                    'evaluated': False,
                    'note': 'Evaluation requires multiple samples (n > 1)'
                }
            
            unique_labels = np.unique(predictions)
            n_clusters = len(unique_labels)
            
            if n_clusters < 2:
                print('Skipping evaluation: requires at least 2 clusters')
                return {
                    'evaluated': False,
                    'note': 'Evaluation requires at least 2 clusters'
                }
            
            # Calculate internal metrics
            try:
                silhouette = float(silhouette_score(X, predictions))
                print('Silhouette Score:', round(silhouette, 4))
            except:
                silhouette = None
            
            try:
                davies_bouldin = float(davies_bouldin_score(X, predictions))
                print('Davies-Bouldin Score:', round(davies_bouldin, 4))
            except:
                davies_bouldin = None
            
            try:
                calinski_harabasz = float(calinski_harabasz_score(X, predictions))
                print('Calinski-Harabasz Score:', round(calinski_harabasz, 4))
            except:
                calinski_harabasz = None
            
            # Calculate separation ratio
            separation_ratio = calculate_separation_ratio(X, predictions)
            if separation_ratio:
                print('Separation Ratio:', round(separation_ratio, 4))
            
            # Cluster distribution
            cluster_distribution = {}
            for label in unique_labels:
                count = int(np.sum(predictions == label))
                cluster_distribution[int(label)] = count
            
            # Assess quality
            quality = assess_quality(silhouette, separation_ratio, davies_bouldin)
            
            print('Overall Quality:', quality['overall_quality'])
            
            evaluation = {
                'internal_metrics': {
                    'silhouette_score': silhouette,
                    'davies_bouldin_score': davies_bouldin,
                    'calinski_harabasz_score': calinski_harabasz,
                    'separation_ratio': separation_ratio
                },
                'cluster_distribution': cluster_distribution,
                'quality_assessment': quality,
                'evaluated': True,
                'note': 'Internal metrics calculated without ground truth labels'
            }
            
            return evaluation
        
        
        def calculate_separation_ratio(X, predictions):
            try:
                unique_labels = np.unique(predictions)
                
                # Calculate intra-cluster distances
                intra_distances = []
                for label in unique_labels:
                    if label == -1:
                        continue
                    cluster_points = X[predictions == label]
                    if len(cluster_points) > 1:
                        center = cluster_points.mean(axis=0)
                        distances = np.linalg.norm(cluster_points - center, axis=1)
                        intra_distances.extend(distances)
                
                if not intra_distances:
                    return None
                
                mean_intra = np.mean(intra_distances)
                
                # Calculate inter-cluster distances
                centers = []
                for label in unique_labels:
                    if label == -1:
                        continue
                    cluster_points = X[predictions == label]
                    center = cluster_points.mean(axis=0)
                    centers.append(center)
                
                if len(centers) < 2:
                    return None
                
                inter_distances = []
                for i in range(len(centers)):
                    for j in range(i + 1, len(centers)):
                        dist = np.linalg.norm(centers[i] - centers[j])
                        inter_distances.append(dist)
                
                mean_inter = np.mean(inter_distances)
                
                if mean_intra > 0:
                    separation_ratio = mean_inter / mean_intra
                    return float(separation_ratio)
                else:
                    return None
            except:
                return None
        
        
        def assess_quality(silhouette, separation, davies_bouldin):
            quality_levels = []
            
            sil_quality = 'N/A'
            sep_quality = 'N/A'
            db_quality = 'N/A'
            
            # Silhouette assessment
            if silhouette is not None:
                if silhouette >= 0.7:
                    sil_quality = 'Excellent'
                elif silhouette >= 0.5:
                    sil_quality = 'Good'
                elif silhouette >= 0.25:
                    sil_quality = 'Fair'
                else:
                    sil_quality = 'Poor'
                quality_levels.append(sil_quality)
            
            # Separation assessment
            if separation is not None:
                if separation >= 2.0:
                    sep_quality = 'Excellent'
                elif separation >= 1.5:
                    sep_quality = 'Good'
                elif separation >= 1.2:
                    sep_quality = 'Fair'
                else:
                    sep_quality = 'Poor'
                quality_levels.append(sep_quality)
            
            # Davies-Bouldin assessment (lower is better)
            if davies_bouldin is not None:
                if davies_bouldin <= 0.5:
                    db_quality = 'Excellent'
                elif davies_bouldin <= 1.0:
                    db_quality = 'Good'
                elif davies_bouldin <= 2.0:
                    db_quality = 'Fair'
                else:
                    db_quality = 'Poor'
                quality_levels.append(db_quality)
            
            # Overall quality
            if not quality_levels:
                overall = 'Unknown'
            else:
                quality_counts = {
                    'Excellent': quality_levels.count('Excellent'),
                    'Good': quality_levels.count('Good'),
                    'Fair': quality_levels.count('Fair'),
                    'Poor': quality_levels.count('Poor')
                }
                
                if quality_counts['Poor'] > len(quality_levels) / 2:
                    overall = 'Poor'
                elif quality_counts['Excellent'] > len(quality_levels) / 2:
                    overall = 'Excellent'
                elif quality_counts['Good'] >= quality_counts['Fair']:
                    overall = 'Good'
                else:
                    overall = 'Fair'
            
            return {
                'overall_quality': overall,
                'silhouette_quality': sil_quality,
                'separation_quality': sep_quality,
                'davies_bouldin_quality': db_quality
            }
        
        
        def generate_summary_report(result):
            report = []
            report.append('')
            report.append('='*80)
            report.append('PRODUCTION INFERENCE RESULTS')
            report.append('='*80)
            
            # Model info
            model_info = result['model_info']
            report.append('')
            report.append('Model Information:')
            report.append('  Algorithm: ' + str(model_info['algorithm']))
            report.append('  Version: ' + str(model_info['model_version']))
            report.append('  Clusters: ' + str(model_info['n_clusters']))
            
            # Predictions
            pred_info = result['predictions']
            report.append('')
            report.append('Predictions:')
            report.append('  Samples: ' + str(pred_info['n_samples']))
            report.append('  Input type: ' + str(pred_info['input_type']))
            
            predictions = pred_info['cluster_labels']
            if len(predictions) <= 10:
                report.append('  Cluster labels: ' + str(predictions))
            else:
                preview = str(predictions[:5]) + ' ... ' + str(predictions[-5:])
                report.append('  Cluster labels: ' + preview)
            
            # Evaluation
            if 'quality_evaluation' in result and result['quality_evaluation'].get('evaluated'):
                eval_data = result['quality_evaluation']
                
                report.append('')
                report.append('Quality Evaluation (Internal Metrics):')
                
                metrics = eval_data['internal_metrics']
                if metrics.get('silhouette_score') is not None:
                    report.append('  Silhouette Score: ' + str(round(metrics['silhouette_score'], 4)))
                if metrics.get('davies_bouldin_score') is not None:
                    report.append('  Davies-Bouldin: ' + str(round(metrics['davies_bouldin_score'], 4)))
                if metrics.get('separation_ratio') is not None:
                    report.append('  Separation Ratio: ' + str(round(metrics['separation_ratio'], 4)))
                
                report.append('')
                report.append('Quality Assessment:')
                quality = eval_data['quality_assessment']
                report.append('  Overall: ' + quality['overall_quality'])
                report.append('  Silhouette: ' + quality['silhouette_quality'])
                report.append('  Separation: ' + quality['separation_quality'])
                report.append('  Davies-Bouldin: ' + quality['davies_bouldin_quality'])
                
                report.append('')
                report.append('Cluster Distribution:')
                for cluster, count in eval_data['cluster_distribution'].items():
                    pct = count / pred_info['n_samples'] * 100
                    report.append('  Cluster ' + str(cluster) + ': ' + str(count) + ' samples (' + str(round(pct, 1)) + '%)')
            
            report.append('')
            report.append('='*80)
            
            sep = chr(10)
            result_text = sep.join(report)
            return result_text
        
        
        def main():
            parser = argparse.ArgumentParser(
                description='Production Clustering Inference Component'
            )
            
            parser.add_argument('--model', required=True,
                               help='Trained model path')
            parser.add_argument('--metadata', required=True,
                               help='Training or version metadata path')
            parser.add_argument('--input_data', required=True,
                               help='JSON string with input data')
            parser.add_argument('--evaluate', default='true',
                               help='Whether to evaluate predictions')
            
            parser.add_argument('--output_predictions', required=True,
                               help='Output path for predictions JSON')
            parser.add_argument('--output_prediction_summary', required=True,
                               help='Output path for summary report')
            
            args = parser.parse_args()
            
            try:
                print('='*80)
                print('PRODUCTION CLUSTERING INFERENCE')
                print('='*80)
                
                # Load model and metadata
                model = load_model(args.model)
                metadata = load_metadata(args.metadata)
                model_info = extract_model_info(metadata)
                
                # Parse input
                X, n_samples, input_type = parse_input_data(args.input_data, model_info)
                
                # Validate
                validate_input(X, model_info)
                
                # Predict
                predictions = predict_clusters(model, X, model_info)
                
                # Build result
                result = {
                    'model_info': {
                        'algorithm': model_info['algorithm'],
                        'n_clusters': model_info['n_clusters'],
                        'model_version': model_info['version']
                    },
                    'predictions': {
                        'cluster_labels': predictions.tolist(),
                        'n_samples': int(n_samples),
                        'input_type': input_type
                    },
                    'timestamp': datetime.now().isoformat()
                }
                
                # Evaluate (if requested)
                evaluate = args.evaluate.lower() == 'true'
                if evaluate:
                    evaluation = evaluate_predictions(X, predictions)
                    result['quality_evaluation'] = evaluation
                
                # Save outputs
                os.makedirs(os.path.dirname(args.output_predictions), exist_ok=True)
                os.makedirs(os.path.dirname(args.output_prediction_summary), exist_ok=True)
                
                with open(args.output_predictions, 'w') as f:
                    json.dump(result, f, indent=2)
                
                file_size = os.path.getsize(args.output_predictions) / 1024
                print('')
                print('Predictions saved:', args.output_predictions, '(' + str(round(file_size, 2)) + ' KB)')
                
                summary = generate_summary_report(result)
                with open(args.output_prediction_summary, 'w') as f:
                    f.write(summary)
                
                print('Summary report saved:', args.output_prediction_summary)
                
                print(summary)
                
                print('')
                print('='*80)
                print('PRODUCTION INFERENCE COMPLETED SUCCESSFULLY')
                print('='*80)
                print('')
                print('Summary')
                print('  Algorithm:', model_info['algorithm'])
                print('  Samples predicted:', n_samples)
                print('  Input type:', input_type)
                
                if evaluate and result['quality_evaluation'].get('evaluated'):
                    quality = result['quality_evaluation']['quality_assessment']
                    print('  Overall quality:', quality['overall_quality'])
                
                print('='*80)
                
            except Exception as e:
                print('')
                print('ERROR:', str(e))
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        
        if __name__ == '__main__':
            main()

    args:
      - --model
      - {inputPath: model}
      - --metadata
      - {inputPath: metadata}
      - --input_data
      - {inputValue: input_data}
      - --evaluate
      - {inputValue: evaluate}
      - --output_predictions
      - {outputPath: predictions}
      - --output_prediction_summary
      - {outputPath: prediction_summary}
