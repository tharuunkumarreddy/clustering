name: Data Loader for Clustering v1.0
description: |
  Generic data loading component for clustering preprocessing pipeline.
  Supports multiple formats and provides comprehensive validation.
  Based on the generic framework for clustering preprocessing.

inputs:
  - {name: input_path, type: String, description: 'Path to input data file or URL'}
  - {name: file_format, type: String, default: "auto", description: 'Format: auto, csv, parquet, excel, json, tsv'}
  - {name: encoding, type: String, default: "utf-8", description: 'File encoding (utf-8, latin1, cp1252)'}
  - {name: csv_separator, type: String, default: ",", description: 'CSV separator character'}
  - {name: handle_duplicates, type: String, default: "remove", description: 'Duplicate handling: remove, keep_first, keep_last, keep_all'}
  - {name: min_samples_threshold, type: Integer, default: "2", description: 'Minimum samples required for clustering'}
  - {name: validate_clustering_readiness, type: String, default: "true", description: 'Validate data is ready for clustering'}

outputs:
  - {name: loaded_data, type: Dataset, description: 'Loaded dataset (parquet)'}
  - {name: load_metadata, type: Data, description: 'Loading metadata and validation results (JSON)'}

implementation:
  container:
    image: python:3.9-slim
    command:
      - python3
      - -u
      - -c
      - |
        import os, sys, json, argparse
        from pathlib import Path
        import subprocess
        
        # Install dependencies
        subprocess.run([sys.executable, "-m", "pip", "install", "-q", 
                       "pandas", "numpy", "pyarrow", "openpyxl", "xlrd"], check=True)
        
        import pandas as pd
        import numpy as np
        from datetime import datetime
        
        def ensure_dir(path):
            d = os.path.dirname(path)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)
        
        def auto_detect_format(path):
            ext = Path(path).suffix.lower()
            format_map = {
                '.csv': 'csv', '.tsv': 'tsv', '.txt': 'csv',
                '.parquet': 'parquet', '.pq': 'parquet',
                '.xlsx': 'excel', '.xls': 'excel',
                '.json': 'json', '.jsonl': 'json', '.ndjson': 'json'
            }
            return format_map.get(ext, 'csv')
        
        def load_data(path, fmt, encoding, sep):
            print(f"[LOAD] Loading from: {path}")
            print(f"[LOAD] Format: {fmt}, Encoding: {encoding}, Separator: {repr(sep)}")
            
            try:
                if fmt == 'csv' or fmt == 'tsv':
                    df = pd.read_csv(path, encoding=encoding, sep=sep, low_memory=False)
                elif fmt == 'parquet':
                    df = pd.read_parquet(path)
                elif fmt == 'excel':
                    df = pd.read_excel(path)
                elif fmt == 'json':
                    # Try both orientations
                    try:
                        df = pd.read_json(path, lines=True)
                    except:
                        df = pd.read_json(path)
                else:
                    raise ValueError(f"Unsupported format: {fmt}")
                
                print(f"[LOAD] ✓ Loaded: {df.shape[0]} rows × {df.shape[1]} columns")
                return df
                
            except Exception as e:
                print(f"[LOAD] ✗ Error: {str(e)}", file=sys.stderr)
                raise
        
        def validate_for_clustering(df, min_samples):
            validation = {
                'is_valid': True,
                'errors': [],
                'warnings': [],
                'checks_passed': []
            }
            
            # Check 1: Empty dataset
            if df.empty:
                validation['is_valid'] = False
                validation['errors'].append("Dataset is empty")
                return validation
            
            validation['checks_passed'].append(f"Dataset has {len(df)} samples")
            
            # Check 2: Minimum samples for clustering
            if len(df) < min_samples:
                validation['is_valid'] = False
                validation['errors'].append(f"Need at least {min_samples} samples for clustering (got {len(df)})")
                return validation
            
            validation['checks_passed'].append(f"Minimum samples threshold met ({len(df)} >= {min_samples})")
            
            # Check 3: Duplicate columns
            if df.columns.duplicated().any():
                dup_cols = df.columns[df.columns.duplicated()].tolist()
                validation['warnings'].append(f"Duplicate column names: {dup_cols}")
            else:
                validation['checks_passed'].append("No duplicate column names")
            
            # Check 4: Identify column types
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
            
            validation['numeric_columns'] = numeric_cols
            validation['categorical_columns'] = categorical_cols
            validation['checks_passed'].append(f"Found {len(numeric_cols)} numeric columns")
            validation['checks_passed'].append(f"Found {len(categorical_cols)} categorical columns")
            
            if len(numeric_cols) == 0 and len(categorical_cols) == 0:
                validation['is_valid'] = False
                validation['errors'].append("No valid columns found")
                return validation
            
            # Check 5: Constant columns (problematic for clustering)
            constant_cols = [col for col in df.columns if df[col].nunique(dropna=True) <= 1]
            if constant_cols:
                validation['warnings'].append(f"Constant columns detected (will be removed): {constant_cols}")
            else:
                validation['checks_passed'].append("No constant columns")
            
            # Check 6: Missing values
            missing_total = df.isnull().sum().sum()
            missing_pct = (missing_total / (df.shape[0] * df.shape[1])) * 100 if df.size > 0 else 0
            
            if missing_total > 0:
                validation['warnings'].append(f"Missing values: {missing_total} ({missing_pct:.2f}%)")
                missing_by_col = {}
                for col in df.columns:
                    miss_count = df[col].isnull().sum()
                    if miss_count > 0:
                        missing_by_col[col] = {
                            'count': int(miss_count),
                            'percentage': float(miss_count / len(df) * 100)
                        }
                validation['missing_by_column'] = missing_by_col
            else:
                validation['checks_passed'].append("No missing values")
            
            # Check 7: Infinite values in numeric columns
            inf_count = 0
            for col in numeric_cols:
                col_inf = df[col].isin([np.inf, -np.inf]).sum()
                inf_count += col_inf
            
            if inf_count > 0:
                validation['warnings'].append(f"Infinite values: {inf_count}")
            else:
                validation['checks_passed'].append("No infinite values")
            
            # Check 8: All-NaN columns
            all_nan_cols = [col for col in df.columns if df[col].isna().all()]
            if all_nan_cols:
                validation['warnings'].append(f"All-NaN columns (will be removed): {all_nan_cols}")
            
            return validation
        
        def handle_duplicates(df, strategy):
            before = len(df)
            
            if strategy == 'remove':
                df = df.drop_duplicates(keep='first')
            elif strategy == 'keep_first':
                df = df.drop_duplicates(keep='first')
            elif strategy == 'keep_last':
                df = df.drop_duplicates(keep='last')
            elif strategy == 'keep_all':
                pass  # Do nothing
            else:
                raise ValueError(f"Invalid duplicate strategy: {strategy}")
            
            after = len(df)
            removed = before - after
            
            return df.reset_index(drop=True), removed
        
        def collect_statistics(df):
            stats = {
                'shape': {'rows': int(df.shape[0]), 'cols': int(df.shape[1])},
                'columns': list(df.columns),
                'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()},
                'memory_mb': float(df.memory_usage(deep=True).sum() / 1024**2)
            }
            
            # Numeric statistics
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            if numeric_cols:
                stats['numeric_stats'] = {}
                for col in numeric_cols:
                    try:
                        stats['numeric_stats'][col] = {
                            'count': int(df[col].count()),
                            'mean': float(df[col].mean()),
                            'std': float(df[col].std()),
                            'min': float(df[col].min()),
                            'q25': float(df[col].quantile(0.25)),
                            'median': float(df[col].median()),
                            'q75': float(df[col].quantile(0.75)),
                            'max': float(df[col].max()),
                            'unique': int(df[col].nunique())
                        }
                    except:
                        pass
            
            # Categorical statistics
            categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
            if categorical_cols:
                stats['categorical_stats'] = {}
                for col in categorical_cols[:10]:  # Limit to first 10
                    try:
                        stats['categorical_stats'][col] = {
                            'unique': int(df[col].nunique()),
                            'top_values': df[col].value_counts().head(5).to_dict()
                        }
                    except:
                        pass
            
            return stats
        
        # Main execution
        parser = argparse.ArgumentParser()
        parser.add_argument('--input_path', type=str, required=True)
        parser.add_argument('--file_format', type=str, default='auto')
        parser.add_argument('--encoding', type=str, default='utf-8')
        parser.add_argument('--csv_separator', type=str, default=',')
        parser.add_argument('--handle_duplicates', type=str, default='remove')
        parser.add_argument('--min_samples_threshold', type=int, default=2)
        parser.add_argument('--validate_clustering_readiness', type=str, default='true')
        parser.add_argument('--loaded_data', type=str, required=True)
        parser.add_argument('--load_metadata', type=str, required=True)
        args = parser.parse_args()
        
        try:
            print("=" * 80)
            print("CLUSTERING PREPROCESSING - COMPONENT 1: DATA LOADING")
            print("=" * 80)
            
            # Auto-detect format
            if args.file_format == 'auto':
                file_format = auto_detect_format(args.input_path)
            else:
                file_format = args.file_format
            
            # Load data
            df = load_data(args.input_path, file_format, args.encoding, args.csv_separator)
            
            # Handle duplicates
            dup_strategy = args.handle_duplicates.lower()
            df, removed_dups = handle_duplicates(df, dup_strategy)
            if removed_dups > 0:
                print(f"[CLEAN] Removed {removed_dups} duplicate rows")
            
            # Validate for clustering
            validate = args.validate_clustering_readiness.lower() in ('true', '1', 'yes', 't')
            if validate:
                validation = validate_for_clustering(df, args.min_samples_threshold)
                
                print(f"\n[VALIDATE] Clustering Readiness Check:")
                for check in validation['checks_passed']:
                    print(f"  ✓ {check}")
                for warning in validation['warnings']:
                    print(f"  ⚠ {warning}")
                for error in validation['errors']:
                    print(f"  ✗ {error}")
                
                if not validation['is_valid']:
                    print("\n[VALIDATE] ✗ Validation FAILED")
                    sys.exit(1)
                
                print("\n[VALIDATE] ✓ Data is ready for clustering preprocessing")
            else:
                validation = {'is_valid': True, 'errors': [], 'warnings': [], 'checks_passed': ['Validation skipped']}
            
            # Collect statistics
            statistics = collect_statistics(df)
            
            # Save outputs
            print(f"\n[SAVE] Saving outputs...")
            ensure_dir(args.loaded_data)
            ensure_dir(args.load_metadata)
            
            df.to_parquet(args.loaded_data, index=False)
            print(f"  ✓ Data: {args.loaded_data}")
            
            metadata = {
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'component': 'data_loader',
                'input_path': args.input_path,
                'file_format': file_format,
                'encoding': args.encoding,
                'duplicates_removed': removed_dups,
                'validation': validation,
                'statistics': statistics
            }
            
            with open(args.load_metadata, 'w') as f:
                json.dump(metadata, f, indent=2)
            print(f"  ✓ Metadata: {args.load_metadata}")
            
            print("\n" + "=" * 80)
            print("COMPONENT 1 COMPLETE")
            print("=" * 80)
            print(f"Final shape: {df.shape[0]} rows × {df.shape[1]} columns")
            print(f"Numeric columns: {len(validation.get('numeric_columns', []))}")
            print(f"Categorical columns: {len(validation.get('categorical_columns', []))}")
            print("=" * 80 + "\n")
            
        except Exception as e:
            print(f"\n[ERROR] {str(e)}", file=sys.stderr)
            import traceback
            traceback.print_exc()
            sys.exit(1)
    
    args:
      - --input_path
      - {inputValue: input_path}
      - --file_format
      - {inputValue: file_format}
      - --encoding
      - {inputValue: encoding}
      - --csv_separator
      - {inputValue: csv_separator}
      - --handle_duplicates
      - {inputValue: handle_duplicates}
      - --min_samples_threshold
      - {inputValue: min_samples_threshold}
      - --validate_clustering_readiness
      - {inputValue: validate_clustering_readiness}
      - --loaded_data
      - {outputPath: loaded_data}
      - --load_metadata
      - {outputPath: load_metadata}