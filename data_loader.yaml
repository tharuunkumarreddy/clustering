name: PI Schema Data Loader v2
description: Load data from PI schemas for clustering. Handles pagination and feature selection.

inputs:
- {name: api_base_url, type: String, description: 'PI API base URL'}
- {name: access_token, type: String, description: 'Bearer access token'}
- {name: schema_id, type: String, description: 'PI Schema ID'}
- {name: feature_list, type: String, default: 'none', description: 'Comma-separated features (none = all)'}
- {name: show_referenced_data, type: String, default: 'false', description: 'Include referenced data'}
- {name: show_dbaas_keywords, type: String, default: 'false', description: 'Show DBaaS keywords'}
- {name: show_pageable_metadata, type: String, default: 'true', description: 'Include pagination metadata'}
- {name: page_size, type: String, default: '20', description: 'Page size'}
- {name: max_pages, type: String, default: '-1', description: 'Max pages (-1 = all)'}
- {name: start_page, type: String, default: '0', description: 'Starting page (default 0)'}
- {name: drop_metadata_columns, type: String, default: 'true', description: 'Drop metadata columns'}

outputs:
- {name: loaded_data, type: Dataset, description: 'Loaded dataset'}
- {name: load_metadata, type: Dataset, description: 'Loading metadata JSON'}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
    - sh
    - -c
    - |
      set -e
      python3 -u -c '
      import argparse
      import os
      import sys
      import json
      import pandas as pd
      import numpy as np
      import requests
      from requests.adapters import HTTPAdapter
      from urllib3.util.retry import Retry
      import logging
      from datetime import datetime
      
      logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")
      logger = logging.getLogger("pi_loader")
      
      def ensure_dir(path):
          d = os.path.dirname(path)
          if d and not os.path.exists(d):
              os.makedirs(d, exist_ok=True)
      
      def extract_records(body):
          if isinstance(body, list):
              return body
          if isinstance(body, dict):
              for key in ["data", "content", "records", "results", "items", "instances"]:
                  if key in body and isinstance(body[key], list):
                      return body[key]
              if any(k for k in body.keys() if not k.startswith("page") and not k.startswith("total")):
                  return [body]
          return []
      
      def discover_page_metadata(body):
          page_size = None
          total_pages = None
          total_instances = None
          
          if isinstance(body, dict):
              page_size = body.get("pageSize") or body.get("size")
              total_pages = body.get("totalPages")
              total_instances = body.get("totalInstances") or body.get("totalElements")
              
              if "page" in body and isinstance(body["page"], dict):
                  p = body["page"]
                  page_size = page_size or p.get("pageSize") or p.get("size")
                  total_pages = total_pages or p.get("totalPages")
                  total_instances = total_instances or p.get("totalElements")
              
              if "pageable" in body and isinstance(body["pageable"], dict):
                  pg = body["pageable"]
                  page_size = page_size or pg.get("pageSize")
              
              if total_instances and page_size and not total_pages:
                  total_pages = (total_instances + page_size - 1) // page_size
          
          return page_size, total_pages, total_instances
      
      def build_api_url(base_url, schema_id, page, size, show_ref, show_dbaas, show_page_meta):
          base_url = base_url.rstrip("/")
          url = base_url + "/schemas/" + schema_id + "/instances/list"
          params = [
              "showReferencedData=" + show_ref,
              "showDBaaSReservedKeywords=" + show_dbaas,
              "showPageableMetaData=" + show_page_meta,
              "size=" + str(size),
              "page=" + str(page)
          ]
          return url + "?" + "&".join(params)
      
      def fetch_data(session, base_url, schema_id, headers, page_size, max_pages, show_ref, show_dbaas, show_page_meta, start_page):
          logger.info("Loading from PI schema: " + schema_id)
          
          meta_url = build_api_url(base_url, schema_id, start_page, page_size, show_ref, show_dbaas, show_page_meta)
          logger.info("API URL: " + meta_url)
          
          try:
              resp_meta = session.post(meta_url, headers=headers, json={}, timeout=30)
              resp_meta.raise_for_status()
              body_meta = resp_meta.json()
              logger.info("Status: " + str(resp_meta.status_code))
          except requests.exceptions.RequestException as e:
              logger.error("Request failed: " + str(e))
              if hasattr(e, "response") and e.response is not None:
                  logger.error("Status: " + str(e.response.status_code))
                  logger.error("Body: " + str(e.response.text[:500]))
              raise
          
          detected_page_size, total_pages, total_instances = discover_page_metadata(body_meta)
          logger.info("Detected: pageSize=" + str(detected_page_size) + ", totalPages=" + str(total_pages) + ", totalInstances=" + str(total_instances))
          
          actual_page_size = detected_page_size or page_size
          
          if total_pages is None and total_instances:
              total_pages = (total_instances + actual_page_size - 1) // actual_page_size
              logger.info("Calculated pages: " + str(total_pages))
          
          if max_pages > 0 and total_pages:
              total_pages = min(total_pages, max_pages)
              logger.info("Limited to: " + str(total_pages) + " pages")
          
          all_records = []
          
          if total_pages is None:
              logger.warning("No pagination - single page")
              all_records.extend(extract_records(body_meta))
          else:
              logger.info("Fetching " + str(total_pages) + " pages...")
              
              for page in range(start_page, start_page + total_pages):
                  page_url = build_api_url(base_url, schema_id, page, actual_page_size, show_ref, show_dbaas, show_page_meta)
                  
                  try:
                      resp_page = session.post(page_url, headers=headers, json={}, timeout=30)
                      resp_page.raise_for_status()
                      body_page = resp_page.json()
                      
                      records = extract_records(body_page)
                      all_records.extend(records)
                      
                      if (page - start_page + 1) % 10 == 0 or page == start_page + total_pages - 1:
                          logger.info("Progress: " + str(page - start_page + 1) + "/" + str(total_pages) + " pages, " + str(len(all_records)) + " records")
                  except requests.exceptions.RequestException as e:
                      logger.error("Failed page " + str(page) + ": " + str(e))
                      raise
          
          logger.info("Total: " + str(len(all_records)) + " records")
          return all_records
      
      def select_features(df, feature_list):
          if not feature_list or feature_list == "none":
              return df
          
          features = [f.strip() for f in feature_list.split(",") if f.strip()]
          if not features:
              return df
          
          available = [f for f in features if f in df.columns]
          missing = [f for f in features if f not in df.columns]
          
          if missing:
              logger.warning("Missing: " + str(missing))
          
          if not available:
              logger.error("No features found")
              return df
          
          logger.info("Selected " + str(len(available)) + "/" + str(len(features)))
          return df[available]
      
      def drop_metadata_cols(df):
          metadata_cols = ["piMetadata", "execution_timestamp", "pipelineid", "component_id", "projectid", "created_at", "updated_at", "_id", "userId", "orgId", "createdOn", "updatedOn"]
          cols_to_drop = [c for c in metadata_cols if c in df.columns]
          if cols_to_drop:
              logger.info("Dropping: " + str(cols_to_drop))
              df = df.drop(columns=cols_to_drop)
          return df
      
      parser = argparse.ArgumentParser()
      parser.add_argument("--api_base_url", type=str, required=True)
      parser.add_argument("--access_token", type=str, required=True)
      parser.add_argument("--schema_id", type=str, required=True)
      parser.add_argument("--feature_list", type=str, default="none")
      parser.add_argument("--show_referenced_data", type=str, default="false")
      parser.add_argument("--show_dbaas_keywords", type=str, default="false")
      parser.add_argument("--show_pageable_metadata", type=str, default="true")
      parser.add_argument("--page_size", type=str, default="20")
      parser.add_argument("--max_pages", type=str, default="-1")
      parser.add_argument("--start_page", type=str, default="0")
      parser.add_argument("--drop_metadata_columns", type=str, default="true")
      parser.add_argument("--loaded_data", type=str, required=True)
      parser.add_argument("--load_metadata", type=str, required=True)
      args = parser.parse_args()
      
      try:
          print("=" * 80)
          print("PI SCHEMA DATA LOADER v2")
          print("=" * 80)
          
          page_size = int(args.page_size)
          max_pages = int(args.max_pages)
          start_page = int(args.start_page)
          
          with open(args.access_token, "r") as f:
              access_token = f.read().strip()
          
          session = requests.Session()
          retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504], allowed_methods=["POST"])
          adapter = HTTPAdapter(max_retries=retries)
          session.mount("http://", adapter)
          session.mount("https://", adapter)
          
          headers = {"Content-Type": "application/json", "Authorization": "Bearer " + access_token}
          
          print("\n[CONFIG] Schema: " + args.schema_id)
          print("[CONFIG] Base URL: " + args.api_base_url)
          print("[CONFIG] Page size: " + str(page_size))
          print("[CONFIG] Max pages: " + str(max_pages))
          print("[CONFIG] Start page: " + str(start_page))
          
          print("\n[FETCH] Loading data...")
          
          records = fetch_data(
              session=session,
              base_url=args.api_base_url,
              schema_id=args.schema_id,
              headers=headers,
              page_size=page_size,
              max_pages=max_pages,
              show_ref=args.show_referenced_data,
              show_dbaas=args.show_dbaas_keywords,
              show_page_meta=args.show_pageable_metadata,
              start_page=start_page
          )
          
          if not records:
              raise ValueError("No records retrieved")
          
          df = pd.DataFrame(records)
          logger.info("DataFrame: " + str(df.shape))
          
          if args.feature_list and args.feature_list != "none":
              df = select_features(df, args.feature_list)
              logger.info("After selection: " + str(df.shape))
          
          if args.drop_metadata_columns.lower() in ("true", "1", "yes"):
              print("\n[CLEAN] Dropping metadata...")
              df = drop_metadata_cols(df)
              logger.info("Final: " + str(df.shape))
          
          print("\n[SAVE] Saving...")
          ensure_dir(args.loaded_data)
          ensure_dir(args.load_metadata)
          
          df.to_csv(args.loaded_data, index=False)
          logger.info("Data: " + args.loaded_data)
          
          metadata = {
              "timestamp": datetime.utcnow().isoformat() + "Z",
              "component": "pi_schema_data_loader_v2",
              "api_base_url": args.api_base_url,
              "schema_id": args.schema_id,
              "records_fetched": len(records),
              "final_shape": {"rows": int(df.shape[0]), "cols": int(df.shape[1])},
              "columns": list(df.columns),
              "dtypes": {col: str(dtype) for col, dtype in df.dtypes.items()},
              "statistics": {
                  "numeric_columns": df.select_dtypes(include=[np.number]).columns.tolist(),
                  "categorical_columns": df.select_dtypes(exclude=[np.number]).columns.tolist(),
                  "memory_mb": float(df.memory_usage(deep=True).sum() / 1024**2)
              }
          }
          
          with open(args.load_metadata, "w") as f:
              json.dump(metadata, f, indent=2)
          logger.info("Metadata: " + args.load_metadata)
          
          print("\n" + "=" * 80)
          print("COMPLETE")
          print("=" * 80)
          print("Schema: " + args.schema_id)
          print("Records: " + str(len(records)))
          print("Shape: " + str(df.shape[0]) + " x " + str(df.shape[1]))
          print("Numeric: " + str(len(metadata["statistics"]["numeric_columns"])))
          print("Categorical: " + str(len(metadata["statistics"]["categorical_columns"])))
          print("=" * 80 + "\n")
          
      except Exception as e:
          print("\n[ERROR] " + str(e), file=sys.stderr)
          import traceback
          traceback.print_exc()
          sys.exit(1)
      ' -- "$@"
    args:
    - --api_base_url
    - {inputValue: api_base_url}
    - --access_token
    - {inputPath: access_token}
    - --schema_id
    - {inputValue: schema_id}
    - --feature_list
    - {inputValue: feature_list}
    - --show_referenced_data
    - {inputValue: show_referenced_data}
    - --show_dbaas_keywords
    - {inputValue: show_dbaas_keywords}
    - --show_pageable_metadata
    - {inputValue: show_pageable_metadata}
    - --page_size
    - {inputValue: page_size}
    - --max_pages
    - {inputValue: max_pages}
    - --start_page
    - {inputValue: start_page}
    - --drop_metadata_columns
    - {inputValue: drop_metadata_columns}
    - --loaded_data
    - {outputPath: loaded_data}
    - --load_metadata
    - {outputPath: load_metadata}
