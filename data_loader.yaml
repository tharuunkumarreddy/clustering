name: PI Schema Data Loader v2
description: Load data from PI schemas for clustering. Handles pagination and feature selection.

inputs:
- {name: api_base_url, type: String, description: 'PI API base URL'}
- {name: access_token, type: String, description: 'Bearer access token'}
- {name: schema_id, type: String, description: 'PI Schema ID'}
- {name: feature_list, type: String, default: 'none', description: 'Comma-separated features or none for all'}
- {name: show_referenced_data, type: String, default: 'false', description: 'Include referenced data'}
- {name: show_dbaas_keywords, type: String, default: 'false', description: 'Show DBaaS keywords'}
- {name: show_pageable_metadata, type: String, default: 'true', description: 'Include pagination metadata'}
- {name: page_size, type: Integer, default: 50, description: 'Page size'}
- {name: max_pages, type: Integer, default: 100, description: 'Max pages (100=all)'}
- {name: start_page, type: Integer, default: 1, description: 'Starting page (1=first)'}
- {name: drop_metadata_columns, type: String, default: 'true', description: 'Drop metadata columns'}

outputs:
- {name: loaded_data, type: Dataset, description: 'Loaded dataset'}
- {name: load_metadata, type: Dataset, description: 'Loading metadata JSON'}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
    - sh
    - -c
    - |
      python3 -u -c 'import argparse; import os; import sys; import json; import pandas as pd; import numpy as np; import requests; from requests.adapters import HTTPAdapter; from urllib3.util.retry import Retry; import logging; from datetime import datetime; logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s"); logger = logging.getLogger("pi_loader")
      def ensure_dir(path):
          d = os.path.dirname(path)
          if d and not os.path.exists(d): os.makedirs(d, exist_ok=True)
      def extract_records(body):
          if isinstance(body, list): return body
          if isinstance(body, dict):
              for key in ["data", "content", "records", "results", "items", "instances"]:
                  if key in body and isinstance(body[key], list): return body[key]
              if any(k for k in body.keys() if not k.startswith("page") and not k.startswith("total")): return [body]
          return []
      def discover_page_metadata(body):
          page_size = None; total_pages = None; total_instances = None
          if isinstance(body, dict):
              page_size = body.get("pageSize") or body.get("size"); total_pages = body.get("totalPages"); total_instances = body.get("totalInstances") or body.get("totalElements")
              if "page" in body and isinstance(body["page"], dict):
                  p = body["page"]; page_size = page_size or p.get("pageSize") or p.get("size"); total_pages = total_pages or p.get("totalPages"); total_instances = total_instances or p.get("totalElements")
              if "pageable" in body and isinstance(body["pageable"], dict): pg = body["pageable"]; page_size = page_size or pg.get("pageSize")
              if total_instances and page_size and not total_pages: total_pages = (total_instances + page_size - 1) // page_size
          return page_size, total_pages, total_instances
      def build_api_url(base_url, schema_id, page, size, show_ref, show_dbaas, show_page_meta):
          base_url = base_url.rstrip("/"); url = base_url + "/schemas/" + schema_id + "/instances/list"
          params = ["showReferencedData=" + show_ref, "showDBaaSReservedKeywords=" + show_dbaas, "showPageableMetaData=" + show_page_meta, "size=" + str(size), "page=" + str(page)]
          return url + "?" + "&".join(params)
      def fetch_data(session, base_url, schema_id, headers, page_size, max_pages, show_ref, show_dbaas, show_page_meta, start_page):
          api_start_page = max(0, start_page - 1); logger.info("Loading schema: " + schema_id); meta_url = build_api_url(base_url, schema_id, api_start_page, page_size, show_ref, show_dbaas, show_page_meta)
          try:
              resp_meta = session.post(meta_url, headers=headers, json={}, timeout=30); resp_meta.raise_for_status(); body_meta = resp_meta.json()
          except Exception as e:
              logger.error("Request failed: " + str(e)); raise
          detected_page_size, total_pages, total_instances = discover_page_metadata(body_meta); actual_page_size = detected_page_size or page_size
          if total_pages is None and total_instances: total_pages = (total_instances + actual_page_size - 1) // actual_page_size
          if total_pages and max_pages >= total_pages: pages_to_fetch = total_pages
          elif max_pages > 0: pages_to_fetch = min(total_pages or max_pages, max_pages)
          else: pages_to_fetch = total_pages or 1
          all_records = []
          if not total_pages: all_records.extend(extract_records(body_meta))
          else:
              for i in range(pages_to_fetch):
                  page = api_start_page + i; page_url = build_api_url(base_url, schema_id, page, actual_page_size, show_ref, show_dbaas, show_page_meta)
                  try:
                      resp_page = session.post(page_url, headers=headers, json={}, timeout=30); resp_page.raise_for_status(); all_records.extend(extract_records(resp_page.json()))
                  except Exception as e:
                      logger.error("Page " + str(page) + " failed: " + str(e)); raise
          logger.info("Total records: " + str(len(all_records))); return all_records
      def select_features(df, feature_list):
          if not feature_list or feature_list == "none": return df
          feature_list = feature_list.strip(chr(34)).strip(chr(39)); features = [f.strip() for f in feature_list.split(",") if f.strip()]
          if not features: return df
          available = [f for f in features if f in df.columns]
          if not available: return df
          return df[available]
      def drop_metadata_cols(df):
          metadata_cols = ["piMetadata", "execution_timestamp", "pipelineid", "component_id", "projectid", "created_at", "updated_at", "_id", "userId", "orgId", "createdOn", "updatedOn"]
          cols_to_drop = [c for c in metadata_cols if c in df.columns]
          if cols_to_drop: df = df.drop(columns=cols_to_drop)
          return df
      parser = argparse.ArgumentParser()
      parser.add_argument("--api_base_url", type=str, required=True); parser.add_argument("--access_token", type=str, required=True); parser.add_argument("--schema_id", type=str, required=True); parser.add_argument("--feature_list", type=str, default="none"); parser.add_argument("--show_referenced_data", type=str, default="false"); parser.add_argument("--show_dbaas_keywords", type=str, default="false"); parser.add_argument("--show_pageable_metadata", type=str, default="true"); parser.add_argument("--page_size", type=int, default=50); parser.add_argument("--max_pages", type=int, default=100); parser.add_argument("--start_page", type=int, default=1); parser.add_argument("--drop_metadata_columns", type=str, default="true"); parser.add_argument("--loaded_data", type=str, required=True); parser.add_argument("--load_metadata", type=str, required=True); args = parser.parse_args()
      try:
          print("=" * 80); print("PI SCHEMA DATA LOADER v2"); print("=" * 80)
          with open(args.access_token, "r") as f: access_token = f.read().strip()
          session = requests.Session(); retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504], allowed_methods=["POST"]); adapter = HTTPAdapter(max_retries=retries); session.mount("http://", adapter); session.mount("https://", adapter)
          headers = {"Content-Type": "application/json", "Authorization": "Bearer " + access_token}
          print("[FETCH] Loading..."); records = fetch_data(session, args.api_base_url, args.schema_id, headers, args.page_size, args.max_pages, args.show_referenced_data, args.show_dbaas_keywords, args.show_pageable_metadata, args.start_page)
          if not records: raise ValueError("No records")
          df = pd.DataFrame(records)
          if args.feature_list and args.feature_list != "none": df = select_features(df, args.feature_list)
          if args.drop_metadata_columns.lower() in ("true", "1", "yes"): df = drop_metadata_cols(df)
          ensure_dir(args.loaded_data); ensure_dir(args.load_metadata); df.to_csv(args.loaded_data, index=False)
          metadata = {"timestamp": datetime.utcnow().isoformat() + "Z", "component": "pi_schema_data_loader_v2", "api_base_url": args.api_base_url, "schema_id": args.schema_id, "records_fetched": len(records), "final_shape": {"rows": int(df.shape[0]), "cols": int(df.shape[1])}, "columns": list(df.columns), "dtypes": {col: str(dtype) for col, dtype in df.dtypes.items()}, "statistics": {"numeric_columns": df.select_dtypes(include=[np.number]).columns.tolist(), "categorical_columns": df.select_dtypes(exclude=[np.number]).columns.tolist(), "memory_mb": float(df.memory_usage(deep=True).sum() / 1024**2)}}
          with open(args.load_metadata, "w") as f: json.dump(metadata, f, indent=2)
          print("COMPLETE - Records: " + str(len(records)) + " Shape: " + str(df.shape[0]) + "x" + str(df.shape[1]))
      except Exception as e:
          print("[ERROR] " + str(e), file=sys.stderr); import traceback; traceback.print_exc(); sys.exit(1)
      ' "$@"
    args:
    - --api_base_url
    - {inputValue: api_base_url}
    - --access_token
    - {inputPath: access_token}
    - --schema_id
    - {inputValue: schema_id}
    - --feature_list
    - {inputValue: feature_list}
    - --show_referenced_data
    - {inputValue: show_referenced_data}
    - --show_dbaas_keywords
    - {inputValue: show_dbaas_keywords}
    - --show_pageable_metadata
    - {inputValue: show_pageable_metadata}
    - --page_size
    - {inputValue: page_size}
    - --max_pages
    - {inputValue: max_pages}
    - --start_page
    - {inputValue: start_page}
    - --drop_metadata_columns
    - {inputValue: drop_metadata_columns}
    - --loaded_data
    - {outputPath: loaded_data}
    - --load_metadata
    - {outputPath: load_metadata}
