name: PI Schema Data Loader for Clustering v3.0
description: |
  Load data from PI schemas for clustering preprocessing.
  Updated to match reference component pattern with proper PI API handling.
  
  API Format: /pi-entity-instances-service/v3.0/schemas/{schemaId}/instances/list
  
  Supports:
  - PI schema instance queries with pagination
  - Feature selection from schema attributes
  - Automatic pagination handling
  - No train-test split (clustering is unsupervised)

inputs:
  - {name: api_base_url, type: String, description: 'Base API URL (e.g., https://igs.gov-cloud.ai/pi-entity-instances-service/v3.0)'}
  - {name: access_token, type: String, description: 'Bearer access token for API authentication'}
  - {name: schema_id, type: String, description: 'PI Schema ID (e.g., 694ffbee7387204b7154ed26)'}
  - {name: feature_list, type: String, default: "", description: 'Comma-separated features (empty = all features)'}
  - {name: show_referenced_data, type: String, default: "false", description: 'Include referenced data in response'}
  - {name: show_dbaas_keywords, type: String, default: "false", description: 'Show DBaaS reserved keywords'}
  - {name: show_pageable_metadata, type: String, default: "true", description: 'Include pagination metadata'}
  - {name: page_size, type: Integer, default: 20, description: 'Page size for pagination'}
  - {name: max_pages, type: Integer, default: -1, description: 'Max pages to fetch (-1 = all pages)'}
  - {name: start_page, type: Integer, default: 0, description: 'Starting page number'}
  - {name: drop_metadata_columns, type: String, default: "true", description: 'Drop PI metadata columns'}
  - {name: validate_clustering_readiness, type: String, default: "true", description: 'Validate data for clustering'}

outputs:
  - {name: processed_dataset, type: Data, description: 'Loaded dataset from PI schema'}
  - {name: schema_json, type: Data, description: 'Schema metadata with statistics'}

implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import json
        import pandas as pd
        import numpy as np
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        import logging
        from datetime import datetime
        
        logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
        logger = logging.getLogger('pi_schema_loader')
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
        
        def get_token_value(token_arg):
            """Extract token from file or return direct value"""
            if not token_arg:
                logger.info("Token argument is None or empty")
                return None
            
            token_str = str(token_arg).strip()
            logger.info("Token argument received (length: {})".format(len(token_str)))
            
            if os.path.exists(token_str):
                logger.info("Reading token from file: {}".format(token_str))
                try:
                    with open(token_str, 'r') as f:
                        content = f.read().strip()
                        logger.info("Token file content length: {}".format(len(content)))
                        return content if content else None
                except Exception as e:
                    logger.error("Failed to read token file: {}".format(str(e)))
                    return None
            else:
                logger.info("Token is direct string value (not a file path)")
                return token_str if token_str and token_str != "None" else None
        
        def extract_records(body):
            """Extract records from API response"""
            if isinstance(body, list):
                return body
            if isinstance(body, dict):
                for key in ["data", "content", "records", "results", "items", "instances"]:
                    if key in body and isinstance(body[key], list):
                        return body[key]
                if any(k for k in body.keys() if not k.startswith('page') and not k.startswith('total')):
                    return [body]
            return []
        
        def discover_page_metadata(body):
            """Discover pagination metadata from response"""
            page_size = None
            total_pages = None
            total_instances = None
            
            if isinstance(body, dict):
                page_size = body.get("pageSize") or body.get("size")
                total_pages = body.get("totalPages")
                total_instances = body.get("totalInstances") or body.get("totalElements")
                
                if "page" in body and isinstance(body["page"], dict):
                    p = body["page"]
                    page_size = page_size or p.get("pageSize") or p.get("size")
                    total_pages = total_pages or p.get("totalPages")
                    total_instances = total_instances or p.get("totalElements")
                
                if "pageable" in body and isinstance(body["pageable"], dict):
                    pg = body["pageable"]
                    page_size = page_size or pg.get("pageSize")
                
                if total_instances and page_size and not total_pages:
                    total_pages = (total_instances + page_size - 1) // page_size
            
            return page_size, total_pages, total_instances
        
        def build_api_url(base_url, schema_id, page, size, show_ref, show_dbaas, show_meta):
            """Build complete API URL with query parameters"""
            base_url = base_url.rstrip('/')
            url = "{}/schemas/{}/instances/list".format(base_url, schema_id)
            
            params = [
                "showReferencedData={}".format(show_ref),
                "showDBaaSReservedKeywords={}".format(show_dbaas),
                "showPageableMetaData={}".format(show_meta),
                "size={}".format(size),
                "page={}".format(page)
            ]
            
            return "{}?{}".format(url, '&'.join(params))
        
        def fetch_pi_schema_data(session, base_url, schema_id, headers, page_size, max_pages, 
                                 show_ref, show_dbaas, show_meta, start_page):
            """Fetch all data from PI schema with pagination"""
            
            logger.info("Starting data fetch from schema: {}".format(schema_id))
            logger.info("Page size: {}, Max pages: {}".format(
                page_size, max_pages if max_pages > 0 else 'all'))
            
            # Build initial URL for metadata discovery
            meta_url = build_api_url(base_url, schema_id, start_page, page_size,
                                    show_ref, show_dbaas, show_meta)
            
            logger.info("API URL: {}".format(meta_url))
            
            try:
                resp_meta = session.post(meta_url, headers=headers, json={}, timeout=30)
                resp_meta.raise_for_status()
                body_meta = resp_meta.json()
                
                logger.info("Response status: {}".format(resp_meta.status_code))
                logger.info("Response type: {}".format(type(body_meta)))
                
                if isinstance(body_meta, dict):
                    logger.info("Response keys: {}".format(list(body_meta.keys())))
                
            except requests.exceptions.RequestException as e:
                logger.error("API request failed: {}".format(str(e)))
                if hasattr(e, 'response') and e.response is not None:
                    logger.error("Status: {}".format(e.response.status_code))
                    logger.error("Body: {}".format(e.response.text[:500]))
                raise
            
            # Extract pagination metadata
            detected_page_size, total_pages, total_instances = discover_page_metadata(body_meta)
            
            logger.info("Detected: pageSize={}, totalPages={}, totalInstances={}".format(
                detected_page_size, total_pages, total_instances))
            
            actual_page_size = detected_page_size or page_size
            
            if total_pages is None and total_instances:
                total_pages = (total_instances + actual_page_size - 1) // actual_page_size
                logger.info("Calculated totalPages: {}".format(total_pages))
            
            if max_pages > 0 and total_pages:
                total_pages = min(total_pages, max_pages)
                logger.info("Limited to {} pages (max_pages={})".format(total_pages, max_pages))
            
            all_records = []
            
            if total_pages is None:
                logger.warning("No pagination metadata - assuming single page")
                all_records.extend(extract_records(body_meta))
            else:
                logger.info("Fetching {} pages...".format(total_pages))
                
                for page in range(start_page, start_page + total_pages):
                    page_url = build_api_url(base_url, schema_id, page, actual_page_size,
                                           show_ref, show_dbaas, show_meta)
                    
                    try:
                        resp_page = session.post(page_url, headers=headers, json={}, timeout=30)
                        resp_page.raise_for_status()
                        body_page = resp_page.json()
                        
                        records = extract_records(body_page)
                        all_records.extend(records)
                        
                        if (page - start_page + 1) % 10 == 0 or page == start_page + total_pages - 1:
                            logger.info("Progress: {}/{} pages, {} records".format(
                                page - start_page + 1, total_pages, len(all_records)))
                        
                    except requests.exceptions.RequestException as e:
                        logger.error("Failed to fetch page {}: {}".format(page, str(e)))
                        raise
            
            logger.info("Total records collected: {}".format(len(all_records)))
            return all_records
        
        def select_features(df, feature_list):
            """Select specific features from dataframe"""
            if not feature_list:
                return df
            
            features = [f.strip() for f in feature_list.split(',') if f.strip()]
            if not features:
                return df
            
            available = [f for f in features if f in df.columns]
            missing = [f for f in features if f not in df.columns]
            
            if missing:
                logger.warning("Missing features: {}".format(missing))
            
            if not available:
                logger.error("No requested features found in schema")
                return df
            
            logger.info("Selected {}/{} features".format(len(available), len(features)))
            return df[available]
        
        def drop_metadata_cols(df):
            """Drop PI metadata columns"""
            metadata_cols = [
                "piMetadata", "execution_timestamp", "pipelineid",
                "component_id", "projectid", "created_at", "updated_at",
                "_id", "userId", "orgId", "createdOn", "updatedOn"
            ]
            
            cols_to_drop = [c for c in metadata_cols if c in df.columns]
            if cols_to_drop:
                logger.info("Dropping metadata columns: {}".format(cols_to_drop))
                df = df.drop(columns=cols_to_drop)
            
            return df
        
        def validate_for_clustering(df):
            """Validate data is ready for clustering"""
            validation = {
                'is_valid': True,
                'errors': [],
                'warnings': [],
                'checks_passed': []
            }
            
            if df.empty:
                validation['is_valid'] = False
                validation['errors'].append("Dataset is empty")
                return validation
            
            validation['checks_passed'].append("Dataset has {} samples".format(len(df)))
            
            if len(df) < 2:
                validation['is_valid'] = False
                validation['errors'].append("Need at least 2 samples for clustering")
                return validation
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
            
            validation['numeric_columns'] = numeric_cols
            validation['categorical_columns'] = categorical_cols
            validation['checks_passed'].append("Found {} numeric, {} categorical columns".format(
                len(numeric_cols), len(categorical_cols)))
            
            if len(numeric_cols) == 0 and len(categorical_cols) == 0:
                validation['is_valid'] = False
                validation['errors'].append("No valid columns found")
                return validation
            
            constant_cols = [col for col in df.columns if df[col].nunique(dropna=True) <= 1]
            if constant_cols:
                validation['warnings'].append("Constant columns (consider removing): {}".format(constant_cols))
            
            missing_total = df.isnull().sum().sum()
            if missing_total > 0:
                missing_pct = (missing_total / (df.shape[0] * df.shape[1])) * 100
                validation['warnings'].append("Missing values: {} ({:.2f}%)".format(
                    missing_total, missing_pct))
            
            return validation
        
        # Parse arguments
        parser = argparse.ArgumentParser(description="PI Schema Data Loader for Clustering")
        parser.add_argument('--api_base_url', type=str, required=True)
        parser.add_argument('--access_token', type=str, required=True)
        parser.add_argument('--schema_id', type=str, required=True)
        parser.add_argument('--feature_list', type=str, default='')
        parser.add_argument('--show_referenced_data', type=str, default='false')
        parser.add_argument('--show_dbaas_keywords', type=str, default='false')
        parser.add_argument('--show_pageable_metadata', type=str, default='true')
        parser.add_argument('--page_size', type=int, default=20)
        parser.add_argument('--max_pages', type=int, default=-1)
        parser.add_argument('--start_page', type=int, default=0)
        parser.add_argument('--drop_metadata_columns', type=str, default='true')
        parser.add_argument('--validate_clustering_readiness', type=str, default='true')
        parser.add_argument('--processed_dataset', type=str, required=True)
        parser.add_argument('--schema_json', type=str, required=True)
        args = parser.parse_args()
        
        try:
            print("=" * 80)
            print("CLUSTERING PREPROCESSING - PI SCHEMA DATA LOADER v3.0")
            print("=" * 80)
            
            # Get access token (handle both file and direct value)
            access_token = get_token_value(args.access_token)
            
            if not access_token:
                raise ValueError("PI Schema loader requires access_token. Please provide a valid token.")
            
            logger.info("Access Token present: {}".format(bool(access_token)))
            logger.info("Access Token length: {}".format(len(access_token) if access_token else 0))
            
            # Setup HTTP session with retry logic
            session = requests.Session()
            retries = Retry(
                total=5,
                backoff_factor=1,
                status_forcelist=[500, 502, 503, 504],
                allowed_methods=["POST"]
            )
            adapter = HTTPAdapter(max_retries=retries)
            session.mount("http://", adapter)
            session.mount("https://", adapter)
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": "Bearer {}".format(access_token)
            }
            
            print("\n[CONFIG] Schema ID: {}".format(args.schema_id))
            print("[CONFIG] API Base URL: {}".format(args.api_base_url))
            print("[CONFIG] Page size: {}".format(args.page_size))
            print("[CONFIG] Max pages: {}".format(args.max_pages if args.max_pages > 0 else 'all'))
            
            # Fetch data from PI schema
            print("\n[FETCH] Loading data from PI schema...")
            
            records = fetch_pi_schema_data(
                session=session,
                base_url=args.api_base_url,
                schema_id=args.schema_id,
                headers=headers,
                page_size=args.page_size,
                max_pages=args.max_pages,
                show_ref=args.show_referenced_data,
                show_dbaas=args.show_dbaas_keywords,
                show_meta=args.show_pageable_metadata,
                start_page=args.start_page
            )
            
            if not records:
                raise ValueError("No records retrieved from PI schema")
            
            # Convert to DataFrame
            df = pd.DataFrame(records)
            logger.info("Created DataFrame: {}".format(df.shape))
            logger.info("Columns: {}".format(list(df.columns)))
            
            # Select features if specified
            if args.feature_list:
                print("\n[SELECT] Selecting features...")
                df = select_features(df, args.feature_list)
                logger.info("After feature selection: {}".format(df.shape))
            
            # Drop metadata columns if requested
            if args.drop_metadata_columns.lower() in ('true', '1', 'yes'):
                print("\n[CLEAN] Dropping metadata columns...")
                df = drop_metadata_cols(df)
                logger.info("Final shape: {}".format(df.shape))
            
            # Validate for clustering
            if args.validate_clustering_readiness.lower() in ('true', '1', 'yes'):
                print("\n[VALIDATE] Checking clustering readiness...")
                validation = validate_for_clustering(df)
                
                for check in validation['checks_passed']:
                    logger.info("  {}".format(check))
                for warning in validation['warnings']:
                    logger.warning("  {}".format(warning))
                for error in validation['errors']:
                    logger.error("  {}".format(error))
                
                if not validation['is_valid']:
                    raise ValueError("Data validation failed for clustering")
                
                logger.info("  Data is ready for clustering")
            else:
                validation = {'is_valid': True, 'checks_passed': ['Validation skipped']}
            
            # Save outputs
            print("\n[SAVE] Saving outputs...")
            ensure_directory_exists(args.processed_dataset)
            ensure_directory_exists(args.schema_json)
            
            # Save dataset as CSV
            df.to_csv(args.processed_dataset, index=False)
            logger.info("  Dataset: {}".format(args.processed_dataset))
            
            # Prepare schema metadata
            schema = {
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'component': 'pi_schema_data_loader_v3',
                'dataset_source': 'pi_schema',
                'api_base_url': args.api_base_url,
                'schema_id': args.schema_id,
                'records_fetched': len(records),
                'final_shape': {
                    'rows': int(df.shape[0]),
                    'cols': int(df.shape[1])
                },
                'columns': list(df.columns),
                'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()},
                'validation': validation,
                'statistics': {
                    'numeric_columns': validation.get('numeric_columns', []),
                    'categorical_columns': validation.get('categorical_columns', []),
                    'memory_mb': float(df.memory_usage(deep=True).sum() / 1024**2)
                },
                'feature_list': args.feature_list if args.feature_list else 'all',
                'page_size': args.page_size,
                'pages_fetched': args.max_pages if args.max_pages > 0 else 'all'
            }
            
            with open(args.schema_json, 'w') as f:
                json.dump(schema, f, indent=2)
            logger.info("  Schema: {}".format(args.schema_json))
            
            # Final summary
            print("\n" + "=" * 80)
            print("PI SCHEMA DATA LOADER COMPLETE")
            print("=" * 80)
            print("Schema ID: {}".format(args.schema_id))
            print("Records loaded: {}".format(len(records)))
            print("Final dataset: {} rows x {} columns".format(df.shape[0], df.shape[1]))
            print("Numeric columns: {}".format(len(validation.get('numeric_columns', []))))
            print("Categorical columns: {}".format(len(validation.get('categorical_columns', []))))
            print("Memory usage: {:.2f} MB".format(schema['statistics']['memory_mb']))
            print("=" * 80 + "\n")
            
            logger.info("Dataset preparation complete")
            logger.info(json.dumps(schema, indent=2))
            
        except Exception as e:
            print("\n[ERROR] {}".format(str(e)), file=sys.stderr)
            import traceback
            traceback.print_exc()
            sys.exit(1)
    args:
      - --api_base_url
      - {inputValue: api_base_url}
      - --access_token
      - {inputValue: access_token}
      - --schema_id
      - {inputValue: schema_id}
      - --feature_list
      - {inputValue: feature_list}
      - --show_referenced_data
      - {inputValue: show_referenced_data}
      - --show_dbaas_keywords
      - {inputValue: show_dbaas_keywords}
      - --show_pageable_metadata
      - {inputValue: show_pageable_metadata}
      - --page_size
      - {inputValue: page_size}
      - --max_pages
      - {inputValue: max_pages}
      - --start_page
      - {inputValue: start_page}
      - --drop_metadata_columns
      - {inputValue: drop_metadata_columns}
      - --validate_clustering_readiness
      - {inputValue: validate_clustering_readiness}
      - --processed_dataset
      - {outputPath: processed_dataset}
      - --schema_json
      - {outputPath: schema_json}
