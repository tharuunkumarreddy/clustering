name: PI Schema Data Loader
description: |
  Load data from PI schemas for clustering preprocessing.
  Uses proven PI API loading logic with pagination support.
  API Format: /pi-entity-instances-service/v3.0/schemas/{schemaId}/instances/list
  Features:
  - Robust PI API data extraction (flat + nested schemas)
  - Feature selection from schema attributes
  - Automatic pagination handling
  - No train-test split (clustering is unsupervised)
inputs:
- name: api_base_url
  type: String
  description: 'Base API URL (e.g., https://igs.gov-cloud.ai/pi-entity-instances-service/v3.0)'
- name: access_token
  type: String
  description: 'Bearer access token for API authentication'
- name: schema_id
  type: String
  description: 'PI Schema ID (e.g., 694ffbee7387204b7154ed26)'
- name: feature_list
  type: String
  default: ""
  description: 'Comma-separated features to select (empty = all features)'
- name: show_referenced_data
  type: String
  default: "false"
  description: 'Include referenced data in response'
- name: show_dbaas_keywords
  type: String
  default: "false"
  description: 'Show DBaaS reserved keywords'
- name: show_pageable_metadata
  type: String
  default: "true"
  description: 'Include pagination metadata'
- name: page_size
  type: Integer
  default: 20
  description: 'Records per page'
- name: max_pages
  type: Integer
  default: -1
  description: 'Max pages to fetch (-1 = all pages)'
- name: start_page
  type: Integer
  default: 0
  description: 'Starting page number (0-indexed)'
- name: drop_metadata_columns
  type: String
  default: "true"
  description: 'Drop PI metadata columns'
- name: validate_clustering_readiness
  type: String
  default: "true"
  description: 'Validate data for clustering'
outputs:
- name: loaded_data
  type: Dataset
  description: 'Loaded dataset from PI schema'
- name: load_metadata
  type: String
  description: 'Loading metadata JSON'
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
    - sh
    - -c
    - |
      exec "$0" "$@"
    - python3
    - -u
    - -c
    - |
      import argparse
      import os
      import sys
      import json
      import pandas as pd
      import numpy as np
      import requests
      from requests.adapters import HTTPAdapter
      from urllib3.util.retry import Retry
      import logging
      from datetime import datetime
      logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
      logger = logging.getLogger("pi_schema_loader")
      def ensure_dir(path):
          d = os.path.dirname(path)
          if d and not os.path.exists(d):
              os.makedirs(d, exist_ok=True)
      def get_token_value(token_arg):
          if not token_arg:
              return None
          token_str = str(token_arg).strip()
          if os.path.exists(token_str):
              logger.info("Reading token from file: {}".format(token_str))
              try:
                  with open(token_str, 'r') as f:
                      content = f.read().strip()
                      return content if content else None
              except Exception as e:
                  logger.error("Failed to read token file: {}".format(e))
                  return None
          else:
              return token_str if token_str and token_str != "None" else None
      def extract_records(body):
          if isinstance(body, list):
              return body
          if isinstance(body, dict):
              for key in ["data", "content", "records", "results", "items", "instances"]:
                  if key in body and isinstance(body[key], list):
                      logger.info("Extracted records from '{}' key".format(key))
                      return body[key]
              non_meta_keys = [k for k in body.keys() if not k.startswith('page') and not k.startswith('total') and k not in ['pageable', 'sort', 'size', 'number']]
              if non_meta_keys:
                  return [body]
          return []
      def discover_page_metadata(body):
          page_size = None
          total_pages = None
          total_instances = None
          if isinstance(body, dict):
              page_size = body.get("pageSize") or body.get("size")
              total_pages = body.get("totalPages")
              total_instances = body.get("totalInstances") or body.get("totalElements")
              if "page" in body and isinstance(body["page"], dict):
                  p = body["page"]
                  page_size = page_size or p.get("pageSize") or p.get("size")
                  total_pages = total_pages or p.get("totalPages")
                  total_instances = total_instances or p.get("totalElements")
              if "pageable" in body and isinstance(body["pageable"], dict):
                  pg = body["pageable"]
                  page_size = page_size or pg.get("pageSize")
              if total_instances and page_size and not total_pages:
                  total_pages = (total_instances + page_size - 1) // page_size
          return page_size, total_pages, total_instances
      def build_api_url(base_url, schema_id, page, size, show_ref, show_dbaas, show_page_meta):
          base_url = base_url.rstrip('/')
          url = "{}/schemas/{}/instances/list".format(base_url, schema_id)
          params = ["showReferencedData={}".format(show_ref), "showDBaaSReservedKeywords={}".format(show_dbaas), "showPageableMetaData={}".format(show_page_meta), "size={}".format(size), "page={}".format(page)]
          return "{}?{}".format(url, '&'.join(params))
      def fetch_all_pages(session, base_url, schema_id, headers, page_size, max_pages, show_ref, show_dbaas, show_page_meta, start_page):
          logger.info("=" * 80)
          logger.info("FETCHING DATA FROM PI SCHEMA")
          logger.info("=" * 80)
          logger.info("Schema ID: {}".format(schema_id))
          logger.info("Page size: {}".format(page_size))
          logger.info("Max pages: {}".format(max_pages if max_pages > 0 else 'all'))
          meta_url = build_api_url(base_url, schema_id, start_page, page_size, show_ref, show_dbaas, show_page_meta)
          logger.info("API URL: {}".format(meta_url))
          try:
              resp_meta = session.post(meta_url, headers=headers, json={}, timeout=30)
              resp_meta.raise_for_status()
              body_meta = resp_meta.json()
              logger.info("Response status: {}".format(resp_meta.status_code))
              logger.info("Response type: {}".format(type(body_meta)))
              if isinstance(body_meta, dict):
                  logger.info("Response keys: {}".format(list(body_meta.keys())))
          except requests.exceptions.RequestException as e:
              logger.error("API request failed: {}".format(str(e)))
              if hasattr(e, 'response') and e.response is not None:
                  logger.error("Status: {}".format(e.response.status_code))
                  logger.error("Body: {}".format(e.response.text[:500]))
              raise
          detected_page_size, total_pages, total_instances = discover_page_metadata(body_meta)
          logger.info("Pagination detected:")
          logger.info("  pageSize: {}".format(detected_page_size))
          logger.info("  totalPages: {}".format(total_pages))
          logger.info("  totalInstances: {}".format(total_instances))
          actual_page_size = detected_page_size or page_size
          if total_pages is None and total_instances:
              total_pages = (total_instances + actual_page_size - 1) // actual_page_size
              logger.info("Calculated totalPages: {}".format(total_pages))
          if max_pages > 0 and total_pages:
              total_pages = min(total_pages, max_pages)
              logger.info("Limited to {} pages (max_pages={})".format(total_pages, max_pages))
          all_records = []
          if total_pages is None or total_pages == 0:
              logger.warning("No pagination metadata - fetching single page")
              all_records.extend(extract_records(body_meta))
          else:
              logger.info("Fetching {} pages...".format(total_pages))
              for page in range(start_page, start_page + total_pages):
                  page_url = build_api_url(base_url, schema_id, page, actual_page_size, show_ref, show_dbaas, show_page_meta)
                  try:
                      resp_page = session.post(page_url, headers=headers, json={}, timeout=30)
                      resp_page.raise_for_status()
                      body_page = resp_page.json()
                      records = extract_records(body_page)
                      all_records.extend(records)
                      if (page - start_page + 1) % 10 == 0 or page == start_page + total_pages - 1:
                          logger.info("Progress: {}/{} pages, {} records".format(page - start_page + 1, total_pages, len(all_records)))
                  except requests.exceptions.RequestException as e:
                      logger.error("Failed to fetch page {}: {}".format(page, str(e)))
                      raise
          logger.info("=" * 80)
          logger.info("Total records collected: {}".format(len(all_records)))
          logger.info("=" * 80)
          return all_records
      def select_features(df, feature_list):
          if not feature_list or not feature_list.strip():
              logger.info("No feature selection - using all columns")
              return df
          features = [f.strip() for f in feature_list.split(',') if f.strip()]
          if not features:
              return df
          available = [f for f in features if f in df.columns]
          missing = [f for f in features if f not in df.columns]
          if missing:
              logger.warning("Missing features: {}".format(missing))
              logger.warning("Available columns: {}".format(list(df.columns)))
          if not available:
              logger.error("No requested features found in schema")
              logger.error("Requested: {}".format(features))
              logger.error("Available: {}".format(list(df.columns)))
              return df
          logger.info("Selected {}/{} features: {}".format(len(available), len(features), available))
          return df[available]
      def drop_metadata_cols(df):
          metadata_cols = ["piMetadata", "execution_timestamp", "pipelineid", "component_id", "projectid", "created_at", "updated_at", "_id", "userId", "orgId", "createdOn", "updatedOn", "id"]
          cols_to_drop = [c for c in metadata_cols if c in df.columns]
          if cols_to_drop:
              logger.info("Dropping metadata columns: {}".format(cols_to_drop))
              df = df.drop(columns=cols_to_drop)
          return df
      def validate_for_clustering(df):
          validation = {'is_valid': True, 'errors': [], 'warnings': [], 'checks_passed': []}
          if df.empty:
              validation['is_valid'] = False
              validation['errors'].append("Dataset is empty")
              return validation
          validation['checks_passed'].append("Dataset has {} samples".format(len(df)))
          if len(df) < 2:
              validation['is_valid'] = False
              validation['errors'].append("Need at least 2 samples for clustering")
              return validation
          numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
          categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
          validation['numeric_columns'] = numeric_cols
          validation['categorical_columns'] = categorical_cols
          validation['checks_passed'].append("Found {} numeric, {} categorical columns".format(len(numeric_cols), len(categorical_cols)))
          if len(numeric_cols) == 0 and len(categorical_cols) == 0:
              validation['is_valid'] = False
              validation['errors'].append("No valid columns found")
              return validation
          constant_cols = [col for col in df.columns if df[col].nunique(dropna=True) <= 1]
          if constant_cols:
              validation['warnings'].append("Constant columns (consider removing): {}".format(constant_cols))
          missing_total = df.isnull().sum().sum()
          if missing_total > 0:
              missing_pct = (missing_total / (df.shape[0] * df.shape[1])) * 100
              validation['warnings'].append("Missing values: {} ({:.2f}%)".format(missing_total, missing_pct))
          return validation
      parser = argparse.ArgumentParser()
      parser.add_argument('--api_base_url', type=str, required=True)
      parser.add_argument('--access_token', type=str, required=True)
      parser.add_argument('--schema_id', type=str, required=True)
      parser.add_argument('--feature_list', type=str, default='')
      parser.add_argument('--show_referenced_data', type=str, default='false')
      parser.add_argument('--show_dbaas_keywords', type=str, default='false')
      parser.add_argument('--show_pageable_metadata', type=str, default='true')
      parser.add_argument('--page_size', type=int, default=20)
      parser.add_argument('--max_pages', type=int, default=-1)
      parser.add_argument('--start_page', type=int, default=0)
      parser.add_argument('--drop_metadata_columns', type=str, default='true')
      parser.add_argument('--validate_clustering_readiness', type=str, default='true')
      parser.add_argument('--loaded_data', type=str, required=True)
      parser.add_argument('--load_metadata', type=str, required=True)
      args = parser.parse_args()
      try:
          print("=" * 80)
          print("PI SCHEMA DATA LOADER FOR CLUSTERING v3.0")
          print("=" * 80)
          access_token = get_token_value(args.access_token)
          if not access_token:
              raise ValueError("Access token is required for PI API")
          logger.info("Access token present: {}".format(bool(access_token)))
          logger.info("Access token length: {}".format(len(access_token)))
          session = requests.Session()
          retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504], allowed_methods=["POST"])
          adapter = HTTPAdapter(max_retries=retries)
          session.mount("http://", adapter)
          session.mount("https://", adapter)
          headers = {"Content-Type": "application/json", "Authorization": "Bearer {}".format(access_token)}
          print("\n[CONFIG]")
          print("  Schema ID: {}".format(args.schema_id))
          print("  API Base URL: {}".format(args.api_base_url))
          print("  Page size: {}".format(args.page_size))
          print("  Max pages: {}".format(args.max_pages if args.max_pages > 0 else 'all'))
          print("  Feature list: {}".format(args.feature_list or 'all features'))
          print("\n[FETCH] Loading data from PI schema...")
          records = fetch_all_pages(session=session, base_url=args.api_base_url, schema_id=args.schema_id, headers=headers, page_size=args.page_size, max_pages=args.max_pages, show_ref=args.show_referenced_data, show_dbaas=args.show_dbaas_keywords, show_page_meta=args.show_pageable_metadata, start_page=args.start_page)
          if not records:
              raise ValueError("No records retrieved from PI schema")
          print("\n[PROCESS] Converting to DataFrame...")
          df = pd.DataFrame(records)
          logger.info("Initial DataFrame: {} rows x {} columns".format(df.shape[0], df.shape[1]))
          logger.info("Columns: {}".format(list(df.columns)))
          if args.feature_list:
              print("\n[PROCESS] Selecting features...")
              df = select_features(df, args.feature_list)
              logger.info("After feature selection: {} rows x {} columns".format(df.shape[0], df.shape[1]))
          if args.drop_metadata_columns.lower() in ('true', '1', 'yes'):
              print("\n[CLEAN] Dropping metadata columns...")
              df = drop_metadata_cols(df)
              logger.info("After cleanup: {} rows x {} columns".format(df.shape[0], df.shape[1]))
          validation = {'is_valid': True, 'checks_passed': ['Validation skipped']}
          if args.validate_clustering_readiness.lower() in ('true', '1', 'yes'):
              print("\n[VALIDATE] Checking clustering readiness...")
              validation = validate_for_clustering(df)
              for check in validation['checks_passed']:
                  logger.info("  PASS: {}".format(check))
              for warning in validation['warnings']:
                  logger.warning("  WARN: {}".format(warning))
              for error in validation['errors']:
                  logger.error("  FAIL: {}".format(error))
              if not validation['is_valid']:
                  raise ValueError("Data validation failed for clustering")
              logger.info("  Data is ready for clustering!")
          print("\n[SAVE] Saving outputs...")
          ensure_dir(args.loaded_data)
          ensure_dir(args.load_metadata)
          df.to_csv(args.loaded_data, index=False)
          logger.info("  Data saved: {}".format(args.loaded_data))
          metadata = {'timestamp': datetime.utcnow().isoformat() + 'Z', 'component': 'pi_schema_data_loader_v3', 'api_base_url': args.api_base_url, 'schema_id': args.schema_id, 'records_fetched': len(records), 'final_shape': {'rows': int(df.shape[0]), 'cols': int(df.shape[1])}, 'columns': list(df.columns), 'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()}, 'validation': validation, 'statistics': {'numeric_columns': validation.get('numeric_columns', []), 'categorical_columns': validation.get('categorical_columns', []), 'memory_mb': float(df.memory_usage(deep=True).sum() / 1024**2)}}
          with open(args.load_metadata, 'w') as f:
              json.dump(metadata, f, indent=2)
          logger.info("  Metadata saved: {}".format(args.load_metadata))
          print("\n" + "=" * 80)
          print("DATA LOADING COMPLETE")
          print("=" * 80)
          print("Schema ID: {}".format(args.schema_id))
          print("Records loaded: {}".format(len(records)))
          print("Final dataset: {} rows x {} columns".format(df.shape[0], df.shape[1]))
          print("Columns: {}".format(list(df.columns)))
          print("Numeric columns: {}".format(len(validation.get('numeric_columns', []))))
          print("Categorical columns: {}".format(len(validation.get('categorical_columns', []))))
          print("Memory usage: {:.2f} MB".format(metadata['statistics']['memory_mb']))
          print("=" * 80 + "\n")
      except Exception as e:
          print("\n[ERROR] {}".format(str(e)), file=sys.stderr)
          import traceback
          traceback.print_exc()
          sys.exit(1)
    args:
    - --api_base_url
    - {inputValue: api_base_url}
    - --access_token
    - {inputValue: access_token}
    - --schema_id
    - {inputValue: schema_id}
    - --feature_list
    - {inputValue: feature_list}
    - --show_referenced_data
    - {inputValue: show_referenced_data}
    - --show_dbaas_keywords
    - {inputValue: show_dbaas_keywords}
    - --show_pageable_metadata
    - {inputValue: show_pageable_metadata}
    - --page_size
    - {inputValue: page_size}
    - --max_pages
    - {inputValue: max_pages}
    - --start_page
    - {inputValue: start_page}
    - --drop_metadata_columns
    - {inputValue: drop_metadata_columns}
    - --validate_clustering_readiness
    - {inputValue: validate_clustering_readiness}
    - --loaded_data
    - {outputPath: loaded_data}
    - --load_metadata
    - {outputPath: load_metadata}
