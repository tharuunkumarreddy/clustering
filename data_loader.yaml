name: PI Schema Data Loader for Clustering v2.0
description: |
  Load data from PI schemas for clustering preprocessing.
  Updated to handle actual PI instance API structure.
  
  API Format: /pi-entity-instances-service/v3.0/schemas/{schemaId}/instances/list
  
  Supports:
  - PI schema instance queries
  - Feature selection from schema attributes  
  - Pagination with showPageableMetaData=true
  - Query parameters: showReferencedData, showDBaaSReservedKeywords
  - No train-test split (clustering is unsupervised)

inputs:
  # Connection parameters
  - {name: api_base_url, type: String, description: 'Base API URL (e.g., https://igs.gov-cloud.ai/pi-entity-instances-service/v3.0)'}
  - {name: access_token, type: String, description: 'Bearer access token for API authentication'}
  
  # Schema selection
  - {name: schema_id, type: String, description: 'PI Schema ID (e.g., 694ffbee7387204b7154ed26 for your IRIS schema)'}
  - {name: feature_list, type: String, default: "", optional: true, description: 'Comma-separated list of features to select (empty = all features)'}
  
  # API Query Parameters (matching your API)
  - {name: show_referenced_data, type: String, default: "false", description: 'Include referenced data in response'}
  - {name: show_dbaas_keywords, type: String, default: "false", description: 'Show DBaaS reserved keywords'}
  - {name: show_pageable_metadata, type: String, default: "true", description: 'Include pagination metadata in response'}
  
  # Pagination parameters
  - {name: page_size, type: Integer, default: "20", description: 'Page size for pagination (size parameter)'}
  - {name: max_pages, type: Integer, default: "0", description: 'Max pages to fetch (0 = all pages)'}
  - {name: start_page, type: Integer, default: "0", description: 'Starting page number'}
  
  # Data processing
  - {name: drop_metadata_columns, type: String, default: "true", description: 'Drop PI metadata columns (piMetadata, etc.)'}
  - {name: validate_clustering_readiness, type: String, default: "true", description: 'Validate data is ready for clustering'}

outputs:
  - {name: loaded_data, type: Dataset, description: 'Loaded dataset from PI schema'}
  - {name: load_metadata, type: Data, description: 'Loading metadata with schema info and statistics'}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import json
        import pandas as pd
        import numpy as np
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        import logging
        from datetime import datetime

        # Setup logging
        logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
        logger = logging.getLogger("pi_schema_loader")

        def ensure_dir(path):
            d = os.path.dirname(path)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def extract_records(body):
            #Extract records from API response#
            if isinstance(body, list):
                return body
            if isinstance(body, dict):
                for key in ["data", "content", "records", "results", "items", "instances"]:
                    if key in body and isinstance(body[key], list):
                        return body[key]
                if any(k for k in body.keys() if not k.startswith('page') and not k.startswith('total')):
                    return [body]
            return []

        def discover_page_metadata(body):
            #Discover pagination metadata from response#
            page_size = None
            total_pages = None
            total_instances = None

            if isinstance(body, dict):
                page_size = body.get("pageSize") or body.get("size")
                total_pages = body.get("totalPages")
                total_instances = body.get("totalInstances") or body.get("totalElements")

                if "page" in body and isinstance(body["page"], dict):
                    p = body["page"]
                    page_size = page_size or p.get("pageSize") or p.get("size")
                    total_pages = total_pages or p.get("totalPages")
                    total_instances = total_instances or p.get("totalElements")
                
                if "pageable" in body and isinstance(body["pageable"], dict):
                    pg = body["pageable"]
                    page_size = page_size or pg.get("pageSize")
                
                if total_instances and page_size and not total_pages:
                    total_pages = (total_instances + page_size - 1) // page_size

            return page_size, total_pages, total_instances

        def build_api_url(base_url, schema_id, page, size, show_ref, show_dbaas, show_page_meta):
            #Build complete API URL with query parameters
            
            Format: {base_url}/schemas/{schema_id}/instances/list?params
            #
            base_url = base_url.rstrip('/')
            
            url = f"{base_url}/schemas/{schema_id}/instances/list"
            
            params = [
                f"showReferencedData={show_ref}",
                f"showDBaaSReservedKeywords={show_dbaas}",
                f"showPageableMetaData={show_page_meta}",
                f"size={size}",
                f"page={page}"
            ]
            
            return f"{url}?{'&'.join(params)}"

        def fetch_schema_data(session, base_url, schema_id, headers, page_size, max_pages, 
                             show_ref, show_dbaas, show_page_meta, start_page):
            #Fetch all data from PI schema with pagination#
            
            logger.info(f"üìä Starting data fetch from schema: {schema_id}")
            logger.info(f"üìÑ Page size: {page_size}, Max pages: {max_pages or 'all'}")
            
            meta_url = build_api_url(base_url, schema_id, start_page, page_size, 
                                    show_ref, show_dbaas, show_page_meta)
            
            logger.info(f"üîó API URL: {meta_url}")
            
            try:
                resp_meta = session.post(meta_url, headers=headers, json={}, timeout=30)
                resp_meta.raise_for_status()
                body_meta = resp_meta.json()
                
                logger.info(f"‚úì Response status: {resp_meta.status_code}")
                
            except requests.exceptions.RequestException as e:
                logger.error(f"‚úó API request failed: {e}")
                if hasattr(e, 'response') and e.response is not None:
                    logger.error(f"‚úó Status: {e.response.status_code}")
                    logger.error(f"‚úó Body: {e.response.text[:500]}")
                raise

            detected_page_size, total_pages, total_instances = discover_page_metadata(body_meta)
            
            logger.info(f"üìã Detected: pageSize={detected_page_size}, totalPages={total_pages}, totalInstances={total_instances}")

            actual_page_size = detected_page_size or page_size
            
            if total_pages is None and total_instances:
                total_pages = (total_instances + actual_page_size - 1) // actual_page_size
                logger.info(f"üìä Calculated totalPages: {total_pages}")
            
            if max_pages > 0 and total_pages:
                total_pages = min(total_pages, max_pages)
                logger.info(f"‚ö†Ô∏è  Limited to {total_pages} pages (max_pages={max_pages})")

            all_records = []

            if total_pages is None:
                logger.warning("‚ö†Ô∏è  No pagination metadata - assuming single page")
                all_records.extend(extract_records(body_meta))
            else:
                logger.info(f"üîÑ Fetching {total_pages} pages...")
                
                for page in range(start_page, start_page + total_pages):
                    page_url = build_api_url(base_url, schema_id, page, actual_page_size,
                                           show_ref, show_dbaas, show_page_meta)
                    
                    try:
                        resp_page = session.post(page_url, headers=headers, json={}, timeout=30)
                        resp_page.raise_for_status()
                        body_page = resp_page.json()
                        
                        records = extract_records(body_page)
                        all_records.extend(records)
                        
                        if (page - start_page + 1) % 10 == 0 or page == start_page + total_pages - 1:
                            logger.info(f"üìÑ Progress: {page - start_page + 1}/{total_pages} pages, {len(all_records)} records")
                        
                    except requests.exceptions.RequestException as e:
                        logger.error(f"‚úó Failed to fetch page {page}: {e}")
                        raise

            logger.info(f"‚úì Total records collected: {len(all_records)}")
            return all_records

        def select_features(df, feature_list):
            #Select specific features from dataframe#
            if not feature_list:
                return df
            
            features = [f.strip() for f in feature_list.split(',') if f.strip()]
            if not features:
                return df
            
            available = [f for f in features if f in df.columns]
            missing = [f for f in features if f not in df.columns]
            
            if missing:
                logger.warning(f"‚ö†Ô∏è  Missing features: {missing}")
            
            if not available:
                logger.error(f"‚úó No requested features found in schema")
                return df
            
            logger.info(f"‚úì Selected {len(available)}/{len(features)} features")
            return df[available]

        def drop_metadata_cols(df):
            #Drop PI metadata columns#
            metadata_cols = [
                "piMetadata", "execution_timestamp", "pipelineid", 
                "component_id", "projectid", "created_at", "updated_at",
                "_id", "userId", "orgId", "createdOn", "updatedOn"
            ]
            
            cols_to_drop = [c for c in metadata_cols if c in df.columns]
            if cols_to_drop:
                logger.info(f"üóëÔ∏è  Dropping metadata columns: {cols_to_drop}")
                df = df.drop(columns=cols_to_drop)
            
            return df

        def validate_for_clustering(df):
            #Validate data is ready for clustering#
            validation = {
                'is_valid': True,
                'errors': [],
                'warnings': [],
                'checks_passed': []
            }
            
            if df.empty:
                validation['is_valid'] = False
                validation['errors'].append("Dataset is empty")
                return validation
            
            validation['checks_passed'].append(f"Dataset has {len(df)} samples")
            
            if len(df) < 2:
                validation['is_valid'] = False
                validation['errors'].append("Need at least 2 samples for clustering")
                return validation
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
            
            validation['numeric_columns'] = numeric_cols
            validation['categorical_columns'] = categorical_cols
            validation['checks_passed'].append(f"Found {len(numeric_cols)} numeric, {len(categorical_cols)} categorical columns")
            
            if len(numeric_cols) == 0 and len(categorical_cols) == 0:
                validation['is_valid'] = False
                validation['errors'].append("No valid columns found")
                return validation
            
            constant_cols = [col for col in df.columns if df[col].nunique(dropna=True) <= 1]
            if constant_cols:
                validation['warnings'].append(f"Constant columns (consider removing): {constant_cols}")
            
            missing_total = df.isnull().sum().sum()
            if missing_total > 0:
                missing_pct = (missing_total / (df.shape[0] * df.shape[1])) * 100
                validation['warnings'].append(f"Missing values: {missing_total} ({missing_pct:.2f}%)")
            
            return validation

        # Parse arguments
        parser = argparse.ArgumentParser()
        parser.add_argument('--api_base_url', type=str, required=True)
        parser.add_argument('--access_token', type=str, required=True)
        parser.add_argument('--schema_id', type=str, required=True)
        parser.add_argument('--feature_list', type=str, default='')
        parser.add_argument('--show_referenced_data', type=str, default='false')
        parser.add_argument('--show_dbaas_keywords', type=str, default='false')
        parser.add_argument('--show_pageable_metadata', type=str, default='true')
        parser.add_argument('--page_size', type=int, default=20)
        parser.add_argument('--max_pages', type=int, default=0)
        parser.add_argument('--start_page', type=int, default=0)
        parser.add_argument('--drop_metadata_columns', type=str, default='true')
        parser.add_argument('--validate_clustering_readiness', type=str, default='true')
        parser.add_argument('--loaded_data', type=str, required=True)
        parser.add_argument('--load_metadata', type=str, required=True)
        args = parser.parse_args()

        try:
            print("=" * 80)
            print("CLUSTERING PREPROCESSING - PI SCHEMA DATA LOADER v2.0")
            print("=" * 80)
            
            # Read access token
            with open(args.access_token, 'r') as f:
                access_token = f.read().strip()

            # Setup HTTP session
            session = requests.Session()
            retries = Retry(
                total=5,
                backoff_factor=1,
                status_forcelist=[500, 502, 503, 504],
                allowed_methods=["POST"]
            )
            adapter = HTTPAdapter(max_retries=retries)
            session.mount("http://", adapter)
            session.mount("https://", adapter)

            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {access_token}"
            }

            print(f"\n[CONFIG] Schema ID: {args.schema_id}")
            print(f"[CONFIG] API Base URL: {args.api_base_url}")
            print(f"[CONFIG] Page size: {args.page_size}")
            
            # Fetch data
            print(f"\n[FETCH] Loading data from PI schema...")
            
            records = fetch_schema_data(
                session=session,
                base_url=args.api_base_url,
                schema_id=args.schema_id,
                headers=headers,
                page_size=args.page_size,
                max_pages=args.max_pages,
                show_ref=args.show_referenced_data,
                show_dbaas=args.show_dbaas_keywords,
                show_page_meta=args.show_pageable_metadata,
                start_page=args.start_page
            )
            
            if not records:
                raise ValueError("No records retrieved from PI schema")

            df = pd.DataFrame(records)
            logger.info(f"‚úì Created DataFrame: {df.shape}")

            # Select features if specified
            if args.feature_list:
                df = select_features(df, args.feature_list)
                logger.info(f"‚úì After feature selection: {df.shape}")

            # Drop metadata columns if requested
            if args.drop_metadata_columns.lower() in ('true', '1', 'yes'):
                print(f"\n[CLEAN] Dropping metadata columns...")
                df = drop_metadata_cols(df)
                logger.info(f"‚úì Final shape: {df.shape}")

            # Validate for clustering
            if args.validate_clustering_readiness.lower() in ('true', '1', 'yes'):
                print(f"\n[VALIDATE] Checking clustering readiness...")
                validation = validate_for_clustering(df)
                
                for check in validation['checks_passed']:
                    logger.info(f"  ‚úì {check}")
                for warning in validation['warnings']:
                    logger.warning(f"  ‚ö†Ô∏è  {warning}")
                for error in validation['errors']:
                    logger.error(f"  ‚úó {error}")
                
                if not validation['is_valid']:
                    raise ValueError("Data validation failed for clustering")
                
                logger.info(f"  ‚úì Data is ready for clustering")
            else:
                validation = {'is_valid': True, 'checks_passed': ['Validation skipped']}

            # Save outputs
            print(f"\n[SAVE] Saving outputs...")
            ensure_dir(args.loaded_data)
            ensure_dir(args.load_metadata)
            
            df.to_csv(args.loaded_data, index=False)
            logger.info(f"  ‚úì Data: {args.loaded_data}")
            
            # Prepare metadata
            metadata = {
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'component': 'pi_schema_data_loader_v2',
                'api_base_url': args.api_base_url,
                'schema_id': args.schema_id,
                'records_fetched': len(records),
                'final_shape': {
                    'rows': int(df.shape[0]),
                    'cols': int(df.shape[1])
                },
                'columns': list(df.columns),
                'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()},
                'validation': validation,
                'statistics': {
                    'numeric_columns': validation.get('numeric_columns', []),
                    'categorical_columns': validation.get('categorical_columns', []),
                    'memory_mb': float(df.memory_usage(deep=True).sum() / 1024**2)
                }
            }
            
            with open(args.load_metadata, 'w') as f:
                json.dump(metadata, f, indent=2)
            logger.info(f"  ‚úì Metadata: {args.load_metadata}")
            
            # Final summary
            print("\n" + "=" * 80)
            print("PI SCHEMA DATA LOADER COMPLETE")
            print("=" * 80)
            print(f"Schema ID: {args.schema_id}")
            print(f"Records loaded: {len(records)}")
            print(f"Final dataset: {df.shape[0]} rows √ó {df.shape[1]} columns")
            print(f"Numeric columns: {len(validation.get('numeric_columns', []))}")
            print(f"Categorical columns: {len(validation.get('categorical_columns', []))}")
            print(f"Memory usage: {metadata['statistics']['memory_mb']:.2f} MB")
            print("=" * 80 + "\n")
            
        except Exception as e:
            print(f"\n[ERROR] {str(e)}", file=sys.stderr)
            import traceback
            traceback.print_exc()
            sys.exit(1)

    args:
      - --api_base_url
      - {inputValue: api_base_url}
      - --access_token
      - {inputPath: access_token}
      - --schema_id
      - {inputValue: schema_id}
      - --feature_list
      - {inputValue: feature_list}
      - --show_referenced_data
      - {inputValue: show_referenced_data}
      - --show_dbaas_keywords
      - {inputValue: show_dbaas_keywords}
      - --show_pageable_metadata
      - {inputValue: show_pageable_metadata}
      - --page_size
      - {inputValue: page_size}
      - --max_pages
      - {inputValue: max_pages}
      - --start_page
      - {inputValue: start_page}
      - --drop_metadata_columns
      - {inputValue: drop_metadata_columns}
      - --validate_clustering_readiness
      - {inputValue: validate_clustering_readiness}
      - --loaded_data
      - {outputPath: loaded_data}
      - --load_metadata
      - {outputPath: load_metadata}
