name: Generic Outlier Detection & Handling Component
description: Comprehensive outlier detection and handling for clustering with 4 detection methods and 7 handling strategies. Supports method-specific parameters via JSON. Detection methods include Z-score, IQR, LOF, and Isolation Forest. Handling methods include remove, clip, winsorize, transforms, and flagging.

inputs:
  - name: input_data
    type: Data
    description: 'Input dataset (CSV or Parquet)'
  - name: detection_method
    type: String
    description: 'Outlier detection method: zscore, iqr, lof, isolation_forest'
    default: 'zscore'
  - name: detection_params
    type: String
    description: 'Detection method parameters as JSON string. Examples: {"threshold":3.0}, {"factor":1.5}, {"n_neighbors":20}'
    default: '{}'
  - name: handling_method
    type: String
    description: 'Outlier handling method: remove, clip, cap, winsorize, log_transform, sqrt_transform, flag'
    default: 'clip'
  - name: handling_params
    type: String
    description: 'Handling method parameters as JSON string. Examples: {"use_iqr":true,"factor":1.5}, {"limits":[0.05,0.05]}'
    default: '{}'

outputs:
  - name: data
    type: Data
    description: 'Dataset with outliers handled (CSV format)'
  - name: outlier_info
    type: Model
    description: 'Outlier detection information and masks (PKL format)'
  - name: report
    type: Data
    description: 'Comprehensive outlier detection and handling report (JSON format)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pickle
        import pandas as pd
        import numpy as np
        from scipy import stats
        from scipy.stats import mstats
        from sklearn.neighbors import LocalOutlierFactor
        from sklearn.ensemble import IsolationForest
        from pathlib import Path
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        logger = logging.getLogger('outlier_handler')
        
        
        def ensure_directory_exists(file_path):
            # Create directory if it doesn't exist
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
                logger.info(f"Created directory: {directory}")
        
        
        def load_data(input_path):
            # Load data from CSV or Parquet file
            logger.info(f"Loading dataset from: {input_path}")
            
            ext = Path(input_path).suffix.lower()
            
            try:
                if ext in ['.parquet', '.pq']:
                    df = pd.read_parquet(input_path)
                    logger.info("Loaded Parquet file")
                else:
                    df = pd.read_csv(input_path)
                    logger.info("Loaded CSV file")
                
                logger.info(f"Shape: {df.shape[0]} rows x {df.shape[1]} columns")
                return df
                
            except Exception as e:
                logger.error(f"Error loading data: {str(e)}")
                raise
        
        
        def detect_outliers(df, method, detection_params):
            # Detect outliers using specified method
            # Parameters: df, method, detection_params
            # Returns: outlier_mask (boolean series), detection_info (dict)
            logger.info("="*80)
            logger.info("DETECTING OUTLIERS")
            logger.info("="*80)
            logger.info(f"Method: {method}")
            
            if detection_params:
                logger.info(f"Parameters: {detection_params}")
            
            logger.info("")
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) == 0:
                logger.warning("No numeric columns for outlier detection")
                logger.warning("Returning no outliers")
                return pd.Series(False, index=df.index), {
                    'method': method,
                    'n_outliers': 0,
                    'outlier_percentage': 0.0,
                    'warning': 'No numeric columns found'
                }
            
            logger.info(f"Analyzing {len(numeric_cols)} numeric columns")
            
            detection_info = {
                'method': method,
                'params': detection_params,
                'numeric_columns': numeric_cols,
                'numeric_column_count': len(numeric_cols)
            }
            
            # METHOD 1: Z-Score Detection
            if method == 'zscore':
                threshold = detection_params.get('threshold', 3.0)
                logger.info(f"Z-score threshold: {threshold}")
                
                # Calculate Z-scores for all numeric columns
                z_scores = np.abs(stats.zscore(df[numeric_cols], nan_policy='omit'))
                z_scores_df = pd.DataFrame(z_scores, columns=numeric_cols, index=df.index)
                
                # Mark outliers (any column exceeds threshold)
                outlier_mask = (z_scores > threshold).any(axis=1)
                
                detection_info['threshold'] = threshold
                detection_info['max_z_score_per_sample'] = z_scores.max(axis=1).tolist()[:100]
                
                # Per-column outlier counts
                logger.info("Per-column outlier counts:")
                for col in numeric_cols:
                    col_outliers = (z_scores_df[col] > threshold).sum()
                    if col_outliers > 0:
                        logger.info(f"  {col}: {col_outliers} outliers")
            
            # METHOD 2: IQR (Interquartile Range) Detection
            elif method == 'iqr':
                factor = detection_params.get('factor', 1.5)
                logger.info(f"IQR factor: {factor}")
                
                outlier_mask = pd.Series(False, index=df.index)
                bounds = {}
                
                logger.info("Calculating IQR bounds per column:")
                for col in numeric_cols:
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    
                    lower_bound = Q1 - factor * IQR
                    upper_bound = Q3 + factor * IQR
                    
                    bounds[col] = {
                        'lower': float(lower_bound),
                        'upper': float(upper_bound),
                        'Q1': float(Q1),
                        'Q3': float(Q3),
                        'IQR': float(IQR)
                    }
                    
                    col_outliers = (df[col] < lower_bound) | (df[col] > upper_bound)
                    outlier_mask = outlier_mask | col_outliers
                    
                    n_col_outliers = col_outliers.sum()
                    if n_col_outliers > 0:
                        logger.info(
                            f"  {col}: {n_col_outliers} outliers "
                            f"(bounds: [{lower_bound:.3f}, {upper_bound:.3f}])"
                        )
                
                detection_info['bounds'] = bounds
                detection_info['factor'] = factor
            
            # METHOD 3: LOF (Local Outlier Factor) Detection
            elif method == 'lof':
                n_neighbors = detection_params.get('n_neighbors', 20)
                contamination = detection_params.get('contamination', 0.1)
                
                logger.info(f"LOF parameters:")
                logger.info(f"  n_neighbors: {n_neighbors}")
                logger.info(f"  contamination: {contamination}")
                
                X = df[numeric_cols].values
                
                # Check if we have enough samples
                if len(df) < n_neighbors:
                    logger.warning(
                        f"Not enough samples ({len(df)}) for n_neighbors={n_neighbors}"
                    )
                    n_neighbors = max(2, len(df) - 1)
                    logger.warning(f"Reduced n_neighbors to {n_neighbors}")
                
                lof = LocalOutlierFactor(
                    n_neighbors=n_neighbors,
                    contamination=contamination
                )
                outlier_labels = lof.fit_predict(X)
                lof_scores = lof.negative_outlier_factor_
                
                outlier_mask = pd.Series(outlier_labels == -1, index=df.index)
                
                detection_info['lof_scores_sample'] = lof_scores.tolist()[:100]
                detection_info['n_neighbors'] = n_neighbors
                detection_info['contamination'] = contamination
                detection_info['lof_score_mean'] = float(lof_scores.mean())
                detection_info['lof_score_std'] = float(lof_scores.std())
            
            # METHOD 4: Isolation Forest Detection
            elif method == 'isolation_forest':
                contamination = detection_params.get('contamination', 0.1)
                random_state = detection_params.get('random_state', 42)
                n_estimators = detection_params.get('n_estimators', 100)
                
                logger.info(f"Isolation Forest parameters:")
                logger.info(f"  contamination: {contamination}")
                logger.info(f"  n_estimators: {n_estimators}")
                logger.info(f"  random_state: {random_state}")
                
                X = df[numeric_cols].values
                
                iso_forest = IsolationForest(
                    contamination=contamination,
                    random_state=random_state,
                    n_estimators=n_estimators,
                    n_jobs=-1,
                    verbose=0
                )
                
                outlier_labels = iso_forest.fit_predict(X)
                anomaly_scores = iso_forest.score_samples(X)
                
                outlier_mask = pd.Series(outlier_labels == -1, index=df.index)
                
                detection_info['anomaly_scores_sample'] = anomaly_scores.tolist()[:100]
                detection_info['contamination'] = contamination
                detection_info['n_estimators'] = n_estimators
                detection_info['random_state'] = random_state
                detection_info['anomaly_score_mean'] = float(anomaly_scores.mean())
                detection_info['anomaly_score_std'] = float(anomaly_scores.std())
            
            else:
                raise ValueError(
                    f"Unknown outlier detection method: {method}. "
                    f"Available: zscore, iqr, lof, isolation_forest"
                )
            
            # Calculate summary statistics
            n_outliers = outlier_mask.sum()
            pct_outliers = (n_outliers / len(df)) * 100 if len(df) > 0 else 0
            detection_info['n_outliers'] = int(n_outliers)
            detection_info['outlier_percentage'] = float(pct_outliers)
            
            logger.info("")
            logger.info(f"Found {n_outliers} outliers ({pct_outliers:.2f}%)")
            logger.info("")
            
            return outlier_mask, detection_info
        
        
        def handle_outliers(df, outlier_mask, method, handling_params, detection_info):
            # Handle detected outliers
            # Parameters: df, outlier_mask, method, handling_params, detection_info
            # Returns: df_handled, handling_stats (dict)
            logger.info("="*80)
            logger.info("HANDLING OUTLIERS")
            logger.info("="*80)
            logger.info(f"Method: {method}")
            
            if handling_params:
                logger.info(f"Parameters: {handling_params}")
            
            logger.info("")
            
            df_handled = df.copy()
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            stats = {
                'method': method,
                'params': handling_params,
                'n_outliers_detected': int(outlier_mask.sum()),
                'n_rows_removed': 0,
                'columns_processed': numeric_cols,
                'handling_details': {}
            }
            
            # METHOD 1: Remove outlier rows
            if method == 'remove':
                removed_indices = df[outlier_mask].index.tolist()
                df_handled = df[~outlier_mask].reset_index(drop=True)
                stats['n_rows_removed'] = len(removed_indices)
                stats['removed_indices_sample'] = removed_indices[:100]
                logger.info(f"Removed {len(removed_indices)} outlier rows")
                logger.info(f"Remaining rows: {len(df_handled)}")
            
            # METHOD 2: Clip/Cap values to bounds
            elif method in ['clip', 'cap']:
                use_iqr = handling_params.get('use_iqr', True)
                factor = handling_params.get('factor', 1.5)
                
                # Check if we have bounds from IQR detection
                if detection_info and 'bounds' in detection_info:
                    bounds = detection_info['bounds']
                    logger.info("Using bounds from IQR detection")
                elif use_iqr:
                    logger.info(f"Calculating IQR bounds (factor={factor})")
                    bounds = {}
                    for col in numeric_cols:
                        Q1 = df[col].quantile(0.25)
                        Q3 = df[col].quantile(0.75)
                        IQR = Q3 - Q1
                        bounds[col] = {
                            'lower': float(Q1 - factor * IQR),
                            'upper': float(Q3 + factor * IQR)
                        }
                else:
                    # Use min/max (no effective clipping)
                    logger.info("Using min/max as bounds (no effective clipping)")
                    bounds = {}
                    for col in numeric_cols:
                        bounds[col] = {
                            'lower': float(df[col].min()),
                            'upper': float(df[col].max())
                        }
                
                total_clipped = 0
                for col in numeric_cols:
                    if col in bounds:
                        lower = bounds[col]['lower']
                        upper = bounds[col]['upper']
                        
                        # Count values that will be clipped
                        values_clipped = ((df[col] < lower) | (df[col] > upper)).sum()
                        total_clipped += values_clipped
                        
                        # Apply clipping
                        df_handled[col] = df_handled[col].clip(lower=lower, upper=upper)
                        
                        stats['handling_details'][col] = {
                            'lower_bound': lower,
                            'upper_bound': upper,
                            'values_clipped': int(values_clipped)
                        }
                        
                        if values_clipped > 0:
                            logger.info(
                                f"  {col}: clipped {values_clipped} values "
                                f"to [{lower:.3f}, {upper:.3f}]"
                            )
                
                logger.info(f"Total values clipped: {total_clipped}")
            
            # METHOD 3: Winsorize (replace extremes with percentiles)
            elif method == 'winsorize':
                limits = handling_params.get('limits', [0.05, 0.05])
                logger.info(f"Winsorizing with limits: {limits}")
                
                for col in numeric_cols:
                    original_values = df_handled[col].copy()
                    df_handled[col] = mstats.winsorize(df_handled[col], limits=limits)
                    
                    values_changed = (original_values != df_handled[col]).sum()
                    stats['handling_details'][col] = {
                        'limits': limits,
                        'values_changed': int(values_changed)
                    }
                    
                    logger.info(f"  {col}: {values_changed} values winsorized")
            
            # METHOD 4: Log transformation
            elif method == 'log_transform':
                transformed_count = 0
                skipped_count = 0
                
                for col in numeric_cols:
                    if (df_handled[col] > 0).all():
                        df_handled[col] = np.log1p(df_handled[col])
                        stats['handling_details'][col] = {'transform': 'log1p', 'status': 'success'}
                        transformed_count += 1
                        logger.info(f"  {col}: log transformed")
                    else:
                        logger.warning(f"  {col}: skipped (contains non-positive values)")
                        stats['handling_details'][col] = {
                            'transform': 'skipped',
                            'reason': 'non-positive values'
                        }
                        skipped_count += 1
                
                logger.info(f"Transformed: {transformed_count}, Skipped: {skipped_count}")
            
            # METHOD 5: Square root transformation
            elif method == 'sqrt_transform':
                transformed_count = 0
                skipped_count = 0
                
                for col in numeric_cols:
                    if (df_handled[col] >= 0).all():
                        df_handled[col] = np.sqrt(df_handled[col])
                        stats['handling_details'][col] = {'transform': 'sqrt', 'status': 'success'}
                        transformed_count += 1
                        logger.info(f"  {col}: sqrt transformed")
                    else:
                        logger.warning(f"  {col}: skipped (contains negative values)")
                        stats['handling_details'][col] = {
                            'transform': 'skipped',
                            'reason': 'negative values'
                        }
                        skipped_count += 1
                
                logger.info(f"Transformed: {transformed_count}, Skipped: {skipped_count}")
            
            # METHOD 6: Flag outliers with indicator column
            elif method == 'flag':
                flag_column = handling_params.get('flag_column', 'is_outlier')
                df_handled[flag_column] = outlier_mask.astype(int)
                stats['handling_details']['flag_column_added'] = flag_column
                stats['handling_details']['n_flagged'] = int(outlier_mask.sum())
                logger.info(f"Added '{flag_column}' flag column")
                logger.info(f"  Flagged {outlier_mask.sum()} outliers as 1")
            
            else:
                raise ValueError(
                    f"Unknown outlier handling method: {method}. "
                    f"Available: remove, clip, cap, winsorize, log_transform, sqrt_transform, flag"
                )
            
            stats['initial_shape'] = list(df.shape)
            stats['final_shape'] = list(df_handled.shape)
            
            logger.info("")
            logger.info(f"Final shape: {df_handled.shape}")
            logger.info("")
            
            return df_handled, stats
        
        
        def main():
            parser = argparse.ArgumentParser(
                description="Generic Outlier Detection & Handling Component"
            )
            parser.add_argument("--input_data", required=True,
                               help="Path to input dataset")
            parser.add_argument("--detection_method", default='zscore',
                               help="Outlier detection method")
            parser.add_argument("--detection_params", type=str, default='{}',
                               help="Detection method parameters as JSON")
            parser.add_argument("--handling_method", default='clip',
                               help="Outlier handling method")
            parser.add_argument("--handling_params", type=str, default='{}',
                               help="Handling method parameters as JSON")
            parser.add_argument("--output_data", required=True,
                               help="Output path for processed dataset")
            parser.add_argument("--output_outlier_info", required=True,
                               help="Output path for outlier information")
            parser.add_argument("--output_report", required=True,
                               help="Output path for outlier report")
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("OUTLIER DETECTION & HANDLING COMPONENT")
            logger.info("="*80)
            logger.info(f"Input: {args.input_data}")
            logger.info(f"Detection method: {args.detection_method}")
            logger.info(f"Handling method: {args.handling_method}")
            logger.info("")
            
            try:
                # Ensure output directories
                ensure_directory_exists(args.output_data)
                ensure_directory_exists(args.output_outlier_info)
                ensure_directory_exists(args.output_report)
                
                # Parse parameters
                try:
                    detection_params = json.loads(args.detection_params)
                    handling_params = json.loads(args.handling_params)
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON in parameters: {e}")
                    logger.error(f"Detection params: {args.detection_params}")
                    logger.error(f"Handling params: {args.handling_params}")
                    logger.error("Expected format: '{\"param\": value}' (use double quotes)")
                    sys.exit(1)
                
                # Load data
                df = load_data(args.input_data)
                
                if df.empty:
                    logger.error("ERROR: Dataset is empty")
                    sys.exit(1)
                
                # Detect outliers
                outlier_mask, detection_info = detect_outliers(
                    df=df,
                    method=args.detection_method,
                    detection_params=detection_params
                )
                
                # Handle outliers
                df_handled, handling_stats = handle_outliers(
                    df=df,
                    outlier_mask=outlier_mask,
                    method=args.handling_method,
                    handling_params=handling_params,
                    detection_info=detection_info
                )
                
                # Save processed data
                df_handled.to_csv(args.output_data, index=False)
                logger.info(f"Processed data saved: {args.output_data}")
                
                # Save outlier information (including mask for reference)
                detection_info['outlier_mask'] = outlier_mask
                with open(args.output_outlier_info, 'wb') as f:
                    pickle.dump(detection_info, f)
                logger.info(f"Outlier info saved: {args.output_outlier_info}")
                
                # Create comprehensive report
                report = {
                    'detection': {
                        'method': args.detection_method,
                        'params': detection_params,
                        'n_outliers': detection_info['n_outliers'],
                        'outlier_percentage': detection_info['outlier_percentage'],
                        'numeric_columns': detection_info.get('numeric_columns', [])
                    },
                    'handling': {
                        'method': args.handling_method,
                        'params': handling_params,
                        'stats': handling_stats
                    },
                    'initial_shape': list(df.shape),
                    'final_shape': list(df_handled.shape),
                    'rows_removed': handling_stats.get('n_rows_removed', 0)
                }
                
                with open(args.output_report, 'w') as f:
                    json.dump(report, f, indent=2)
                logger.info(f"Report saved: {args.output_report}")
                
                logger.info("")
                logger.info("="*80)
                logger.info("OUTLIER HANDLING COMPLETED")
                logger.info("="*80)
                logger.info(f"Detection method: {args.detection_method}")
                logger.info(f"Handling method: {args.handling_method}")
                logger.info(f"Outliers detected: {detection_info['n_outliers']} ({detection_info['outlier_percentage']:.2f}%)")
                logger.info(f"Rows removed: {handling_stats['n_rows_removed']}")
                logger.info(f"Initial shape: {df.shape}")
                logger.info(f"Final shape: {df_handled.shape}")
                logger.info("="*80)
                
            except ValueError as e:
                logger.error(f"VALIDATION ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
            except Exception as e:
                logger.error(f"ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        
        if __name__ == "__main__":
            main()
    args:
      - --input_data
      - {inputPath: input_data}
      - --detection_method
      - {inputValue: detection_method}
      - --detection_params
      - {inputValue: detection_params}
      - --handling_method
      - {inputValue: handling_method}
      - --handling_params
      - {inputValue: handling_params}
      - --output_data
      - {outputPath: data}
      - --output_outlier_info
      - {outputPath: outlier_info}
      - --output_report
      - {outputPath: report}
