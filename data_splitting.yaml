name: Generic Data Splitting Component
description: Splits dataset into train and test sets for clustering evaluation. Supports stratified splitting with ground truth labels to maintain class distributions. Handles cases with or without ground truth labels. Outputs comprehensive split statistics.

inputs:
  - name: input_data
    type: Data
    description: 'Input dataset to split (CSV or Parquet)'
  - name: ground_truth
    type: Data
    description: 'Ground truth labels for stratified splitting (NPY or CSV, optional)'
    optional: true
  - name: test_size
    type: String
    description: 'Proportion of test data (0-1). Use 0 for no test split.'
    default: '0.2'
  - name: random_state
    type: String
    description: 'Random seed for reproducibility'
    default: '42'
  - name: stratify
    type: String
    description: 'Use stratified splitting if ground truth available (true/false)'
    default: 'true'

outputs:
  - name: train_data
    type: Data
    description: 'Training dataset (CSV format)'
  - name: test_data
    type: Data
    description: 'Test dataset (CSV format)'
  - name: train_ground_truth
    type: Data
    description: 'Training ground truth labels (NPY format or empty)'
  - name: test_ground_truth
    type: Data
    description: 'Test ground truth labels (NPY format or empty)'
  - name: split_info
    type: Data
    description: 'Split statistics and class distributions (JSON format)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import shutil
        import pandas as pd
        import numpy as np
        from sklearn.model_selection import train_test_split
        from pathlib import Path
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        logger = logging.getLogger('data_splitter')
        
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
                logger.info("Created directory: " + str(directory))
        
        
        def load_data(input_path):
            logger.info("Loading dataset from: " + str(input_path))
            
            ext = Path(input_path).suffix.lower()
            
            try:
                if ext in ['.parquet', '.pq']:
                    df = pd.read_parquet(input_path)
                    logger.info("Loaded Parquet file")
                else:
                    df = pd.read_csv(input_path)
                    logger.info("Loaded CSV file")
                
                logger.info("Shape: " + str(df.shape[0]) + " rows x " + str(df.shape[1]) + " columns")
                return df
                
            except Exception as e:
                logger.error("Error loading data: " + str(e))
                raise
        
        
        def load_ground_truth(ground_truth_path):
            if not ground_truth_path or ground_truth_path.lower() == 'none':
                logger.info("No ground truth provided")
                return None
            
            logger.info("Loading ground truth from: " + str(ground_truth_path))
            
            try:
                if not os.path.exists(ground_truth_path):
                    logger.info("Ground truth file not found")
                    return None
                
                if os.path.getsize(ground_truth_path) == 0:
                    logger.info("Ground truth file is empty")
                    return None
                
                ext = Path(ground_truth_path).suffix.lower()
                logger.info("Ground truth file extension: '" + str(ext) + "'")
                
                # Try loading as NPY first (works even without .npy extension)
                if ext == '.npy' or ext == '':
                    try:
                        ground_truth = np.load(ground_truth_path, allow_pickle=False)
                        logger.info("Loaded ground truth as NPY: " + str(len(ground_truth)) + " labels")
                        logger.info("Ground truth dtype: " + str(ground_truth.dtype))
                        logger.info("Unique labels: " + str(np.unique(ground_truth).tolist()))
                        return ground_truth
                    except Exception as e:
                        if ext == '.npy':
                            logger.error("Failed to load NPY file: " + str(e))
                            raise
                        logger.info("Not a valid NPY file, trying CSV...")
                
                # Try CSV format
                if ext == '.csv' or ext == '':
                    try:
                        df_gt = pd.read_csv(ground_truth_path)
                        
                        if len(df_gt.columns) == 1:
                            ground_truth = df_gt.iloc[:, 0].values
                        else:
                            ground_truth = df_gt.iloc[:, -1].values
                        
                        logger.info("Loaded ground truth as CSV: " + str(len(ground_truth)) + " labels")
                        logger.info("Unique labels: " + str(np.unique(ground_truth).tolist()))
                        return ground_truth
                    except Exception as e:
                        if ext == '.csv':
                            logger.error("Failed to load CSV file: " + str(e))
                            raise
                        logger.warning("Could not load as CSV either")
                
                logger.warning("Could not determine ground truth format for: " + str(ground_truth_path))
                return None
                
            except FileNotFoundError:
                logger.info("Ground truth file not found")
                return None
            except Exception as e:
                logger.warning("Could not load ground truth: " + str(e))
                import traceback
                traceback.print_exc()
                return None
        
        
        def split_data(df, ground_truth, test_size, random_state, stratify):
            logger.info("="*80)
            logger.info("SPLITTING DATA")
            logger.info("="*80)
            logger.info("Test size: " + str(test_size))
            logger.info("Random state: " + str(random_state))
            logger.info("Stratify: " + str(stratify))
            logger.info("")
            
            if test_size == 0 or test_size is None:
                logger.info("No test split requested (test_size=0)")
                empty_df = pd.DataFrame(columns=df.columns)
                
                split_info = {
                    'test_size': 0,
                    'train_samples': int(len(df)),
                    'test_samples': 0,
                    'stratified': False,
                    'train_percentage': 100.0,
                    'test_percentage': 0.0
                }
                
                if ground_truth is not None:
                    return df, empty_df, ground_truth, np.array([]), split_info
                else:
                    return df, empty_df, None, None, split_info
            
            if not (0 < test_size < 1):
                raise ValueError("test_size must be between 0 and 1, got " + str(test_size))
            
            split_info = {
                'test_size': float(test_size),
                'random_state': int(random_state),
                'stratified': False
            }
            
            # CASE 1: Stratified split with ground truth
            if ground_truth is not None and stratify:
                logger.info("Performing stratified split using ground truth")
                
                if len(ground_truth) != len(df):
                    raise ValueError(
                        "Ground truth length (" + str(len(ground_truth)) + ") does not match "
                        "dataset length (" + str(len(df)) + ")"
                    )
                
                try:
                    train_df, test_df, train_gt, test_gt = train_test_split(
                        df,
                        ground_truth,
                        test_size=test_size,
                        random_state=random_state,
                        stratify=ground_truth
                    )
                    
                    split_info['stratified'] = True
                    split_info['n_classes'] = int(len(np.unique(ground_truth)))
                    
                    unique_train, counts_train = np.unique(train_gt, return_counts=True)
                    unique_test, counts_test = np.unique(test_gt, return_counts=True)
                    
                    train_dist = {int(k): int(v) for k, v in zip(unique_train, counts_train)}
                    test_dist = {int(k): int(v) for k, v in zip(unique_test, counts_test)}
                    
                    split_info['train_class_distribution'] = train_dist
                    split_info['test_class_distribution'] = test_dist
                    
                    logger.info("Stratified split completed successfully")
                    logger.info("Number of classes: " + str(split_info['n_classes']))
                    logger.info("Train class distribution: " + str(train_dist))
                    logger.info("Test class distribution: " + str(test_dist))
                    
                except ValueError as e:
                    logger.warning("Stratified split failed: " + str(e))
                    logger.warning("Falling back to random split")
                    
                    train_df, test_df = train_test_split(
                        df,
                        test_size=test_size,
                        random_state=random_state
                    )
                    
                    train_indices = train_df.index.values
                    test_indices = test_df.index.values
                    train_gt = ground_truth[train_indices]
                    test_gt = ground_truth[test_indices]
                    
                    split_info['stratified'] = False
                    split_info['stratify_fallback'] = True
            
            # CASE 2: Random split with ground truth (no stratification)
            elif ground_truth is not None and not stratify:
                logger.info("Performing random split (ground truth available but not stratifying)")
                
                if len(ground_truth) != len(df):
                    raise ValueError(
                        "Ground truth length (" + str(len(ground_truth)) + ") does not match "
                        "dataset length (" + str(len(df)) + ")"
                    )
                
                train_df, test_df, train_gt, test_gt = train_test_split(
                    df,
                    ground_truth,
                    test_size=test_size,
                    random_state=random_state
                )
                
                logger.info("Random split completed")
            
            # CASE 3: Random split without ground truth
            else:
                logger.info("Performing random split (no ground truth)")
                
                train_df, test_df = train_test_split(
                    df,
                    test_size=test_size,
                    random_state=random_state
                )
                
                train_gt = None
                test_gt = None
                
                logger.info("Random split completed")
            
            train_df = train_df.reset_index(drop=True)
            test_df = test_df.reset_index(drop=True)
            
            split_info['train_samples'] = int(len(train_df))
            split_info['test_samples'] = int(len(test_df))
            
            total_samples = len(train_df) + len(test_df)
            split_info['train_percentage'] = float(len(train_df) / total_samples * 100)
            split_info['test_percentage'] = float(len(test_df) / total_samples * 100)
            
            logger.info("")
            logger.info("Train samples: " + str(len(train_df)) + " (" + str(round(split_info['train_percentage'], 1)) + "%)")
            logger.info("Test samples: " + str(len(test_df)) + " (" + str(round(split_info['test_percentage'], 1)) + "%)")
            
            if train_gt is not None:
                logger.info("Train ground truth shape: " + str(train_gt.shape))
                logger.info("Test ground truth shape: " + str(test_gt.shape))
            
            logger.info("")
            
            return train_df, test_df, train_gt, test_gt, split_info
        
        
        def main():
            parser = argparse.ArgumentParser(
                description="Generic Data Splitting Component"
            )
            parser.add_argument("--input_data", required=True,
                               help="Path to input dataset")
            parser.add_argument("--ground_truth", default='',
                               help="Path to ground truth labels")
            parser.add_argument("--test_size", type=str, default='0.2',
                               help="Proportion of test data")
            parser.add_argument("--random_state", type=str, default='42',
                               help="Random seed")
            parser.add_argument("--stratify", default='true',
                               help="Use stratified splitting")
            parser.add_argument("--output_train_data", required=True,
                               help="Output path for training data")
            parser.add_argument("--output_test_data", required=True,
                               help="Output path for test data")
            parser.add_argument("--output_train_ground_truth", required=True,
                               help="Output path for training ground truth")
            parser.add_argument("--output_test_ground_truth", required=True,
                               help="Output path for test ground truth")
            parser.add_argument("--output_split_info", required=True,
                               help="Output path for split information")
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("DATA SPLITTING COMPONENT")
            logger.info("="*80)
            logger.info("Input: " + str(args.input_data))
            logger.info("Test size: " + str(args.test_size))
            logger.info("Stratify: " + str(args.stratify))
            logger.info("")
            
            try:
                ensure_directory_exists(args.output_train_data)
                ensure_directory_exists(args.output_test_data)
                ensure_directory_exists(args.output_train_ground_truth)
                ensure_directory_exists(args.output_test_ground_truth)
                ensure_directory_exists(args.output_split_info)
                
                df = load_data(args.input_data)
                
                if df.empty:
                    logger.error("ERROR: Dataset is empty")
                    sys.exit(1)
                
                ground_truth = load_ground_truth(args.ground_truth)
                
                test_size = float(args.test_size)
                random_state = int(args.random_state)
                stratify = args.stratify.lower() == 'true'
                
                train_df, test_df, train_gt, test_gt, split_info = split_data(
                    df=df,
                    ground_truth=ground_truth,
                    test_size=test_size,
                    random_state=random_state,
                    stratify=stratify
                )
                
                train_df.to_csv(args.output_train_data, index=False)
                logger.info("Training data saved: " + str(args.output_train_data))
                
                if not test_df.empty:
                    test_df.to_csv(args.output_test_data, index=False)
                else:
                    pd.DataFrame(columns=train_df.columns).to_csv(
                        args.output_test_data,
                        index=False
                    )
                logger.info("Test data saved: " + str(args.output_test_data))
                
                if train_gt is not None and len(train_gt) > 0:
                    np.save(args.output_train_ground_truth, train_gt, allow_pickle=False)
                    npy_path = args.output_train_ground_truth + '.npy'
                    if os.path.exists(npy_path):
                        shutil.move(npy_path, args.output_train_ground_truth)
                    logger.info("Training ground truth saved: " + str(args.output_train_ground_truth))
                    logger.info("  Shape: " + str(train_gt.shape))
                    logger.info("  Unique labels: " + str(np.unique(train_gt).tolist()))
                else:
                    with open(args.output_train_ground_truth, 'w') as f:
                        f.write("")
                    logger.info("No training ground truth (empty marker file created)")
                
                if test_gt is not None and len(test_gt) > 0:
                    np.save(args.output_test_ground_truth, test_gt, allow_pickle=False)
                    npy_path = args.output_test_ground_truth + '.npy'
                    if os.path.exists(npy_path):
                        shutil.move(npy_path, args.output_test_ground_truth)
                    logger.info("Test ground truth saved: " + str(args.output_test_ground_truth))
                    logger.info("  Shape: " + str(test_gt.shape))
                    logger.info("  Unique labels: " + str(np.unique(test_gt).tolist()))
                else:
                    with open(args.output_test_ground_truth, 'w') as f:
                        f.write("")
                    logger.info("No test ground truth (empty marker file created)")
                
                with open(args.output_split_info, 'w') as f:
                    json.dump(split_info, f, indent=2)
                logger.info("Split information saved: " + str(args.output_split_info))
                
                logger.info("")
                logger.info("="*80)
                logger.info("DATA SPLITTING COMPLETED")
                logger.info("="*80)
                logger.info("Training samples: " + str(split_info['train_samples']))
                logger.info("Test samples: " + str(split_info['test_samples']))
                logger.info("Stratified: " + str(split_info['stratified']))
                
                if split_info.get('n_classes'):
                    logger.info("Number of classes: " + str(split_info['n_classes']))
                
                logger.info("="*80)
                
            except ValueError as e:
                logger.error("VALIDATION ERROR: " + str(e))
                import traceback
                traceback.print_exc()
                sys.exit(1)
            except Exception as e:
                logger.error("ERROR: " + str(e))
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        
        if __name__ == "__main__":
            main()
    args:
      - --input_data
      - {inputPath: input_data}
      - --ground_truth
      - {inputPath: ground_truth}
      - --test_size
      - {inputValue: test_size}
      - --random_state
      - {inputValue: random_state}
      - --stratify
      - {inputValue: stratify}
      - --output_train_data
      - {outputPath: train_data}
      - --output_test_data
      - {outputPath: test_data}
      - --output_train_ground_truth
      - {outputPath: train_ground_truth}
      - --output_test_ground_truth
      - {outputPath: test_ground_truth}
      - --output_split_info
      - {outputPath: split_info}
