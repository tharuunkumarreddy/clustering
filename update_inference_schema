name: Clustering Inference Schema Updater
description: Updates the inference schema with clustering inference results

inputs:
  - name: schema_id
    type: String
    description: "The ID of the inference schema to update"
  - name: predictions
    type: Data  # ✅ CORRECT TYPE - matches inference output
    description: "Predictions JSON from clustering inference component"
  - name: model_id
    type: String
    description: "The ID of the model"
  - name: execution_id
    type: String
    description: "The ID of the execution"
  - name: tenant_id
    type: String
    description: "The ID of the tenant"
  - name: project_id
    type: String
    description: "The ID of the project"
  - name: bearer_auth_token
    type: String
    description: "Bearer token for authentication"
  - name: domain
    type: String
    description: "The domain for the API endpoint"

outputs:
  - name: inference_schema_updated
    type: String
    description: "Confirmation of inference schema update"

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import json
        import argparse
        import requests
        import time
        from datetime import datetime
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        import os

        parser = argparse.ArgumentParser()
        parser.add_argument('--schema_id', type=str, required=True)
        parser.add_argument('--predictions', type=str, required=True)  # File path
        parser.add_argument('--model_id', type=str, required=True)
        parser.add_argument('--execution_id', type=str, required=True)
        parser.add_argument('--tenant_id', type=str, required=True)
        parser.add_argument('--project_id', type=str, required=True)
        parser.add_argument('--bearer_auth_token', type=str, required=True)
        parser.add_argument('--domain', type=str, required=True)
        parser.add_argument('--inference_schema_updated', type=str, required=True)
        args = parser.parse_args()

        print("="*80)
        print("CLUSTERING INFERENCE SCHEMA UPDATER")
        print("="*80)

        # Create output directory
        output_dir = os.path.dirname(args.inference_schema_updated)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)

        # Read input files
        print("")
        print("Loading inputs...")
        
        # ✅ Read predictions as FILE (type: Data)
        with open(args.predictions, 'r') as f:
            result = json.load(f)
        print("Predictions loaded successfully")
        
        # ✅ Read bearer token as FILE
        with open(args.bearer_auth_token, 'r') as f:
            bearer_auth_token = f.read().strip()
        
        # ✅ Read tenant_id as FILE
        with open(args.tenant_id, 'r') as f:
            tenant_id = f.read().strip()

        # Parse execution_id to int
        try:
            execution_id_int = int(args.execution_id)
        except (ValueError, TypeError):
            print(f"Warning: execution_id '{args.execution_id}' is not a valid integer. Using 0.")
            execution_id_int = 0

        # Generate unique inference ID (timestamp-based)
        inference_id = f"inf_{int(time.time() * 1000)}"

        # Extract data from predictions
        model_info = result.get('model_info', {})
        predictions_data = result.get('predictions', {})
        quality_eval = result.get('quality_evaluation', {})
        
        algorithm = model_info.get('algorithm', 'Unknown')
        model_version = model_info.get('model_version', 'unknown')
        n_clusters = model_info.get('n_clusters', 0)
        n_samples = predictions_data.get('n_samples', 0)
        input_type = predictions_data.get('input_type', 'unknown')
        cluster_labels = predictions_data.get('cluster_labels', [])
        
        # Quality evaluation (may be absent for single samples)
        quality_evaluated = quality_eval.get('evaluated', False)
        evaluation_note = quality_eval.get('note', '')
        
        # Extract metrics (if available)
        internal_metrics = quality_eval.get('internal_metrics', {})
        quality_assessment = quality_eval.get('quality_assessment', {})
        cluster_distribution = quality_eval.get('cluster_distribution', {})
        
        silhouette = internal_metrics.get('silhouette_score')
        davies_bouldin = internal_metrics.get('davies_bouldin_score')
        calinski_harabasz = internal_metrics.get('calinski_harabasz_score')
        separation_ratio = internal_metrics.get('separation_ratio')
        overall_quality = quality_assessment.get('overall_quality', 'N/A')

        # Convert timestamp
        timestamp_str = result.get('timestamp', datetime.now().isoformat())
        try:
            inference_timestamp = int(datetime.fromisoformat(timestamp_str).timestamp() * 1000)
        except:
            inference_timestamp = int(time.time() * 1000)

        print(f"  Algorithm: {algorithm}")
        print(f"  Samples: {n_samples}")
        print(f"  Input Type: {input_type}")
        print(f"  Quality Evaluated: {quality_evaluated}")
        if quality_evaluated:
            print(f"  Overall Quality: {overall_quality}")

        # Helper function for safe float conversion
        def safe_float(value):
            if value is None:
                return None
            try:
                return float(value)
            except:
                return None

        # Create inference row
        inference_row = {
            # === PRIMARY KEYS ===
            "inference_id": inference_id,
            "execution_id": execution_id_int,
            "projectId": args.project_id,
            "model_id": args.model_id,
            "tenant_id": tenant_id,
            
            # === MODEL INFO ===
            "algorithm": algorithm,
            "model_version": model_version,
            "n_clusters": n_clusters,
            
            # === INFERENCE INFO ===
            "total_samples": n_samples,
            "input_type": input_type,
            
            # === PREDICTIONS ===
            "cluster_labels": cluster_labels,  # JSON array
            "cluster_distribution": cluster_distribution if cluster_distribution else {},  # JSON object
            
            # === QUALITY METRICS (may be None) ===
            "silhouette_score": safe_float(silhouette),
            "davies_bouldin_score": safe_float(davies_bouldin),
            "calinski_harabasz_score": safe_float(calinski_harabasz),
            "separation_ratio": safe_float(separation_ratio),
            "overall_quality": overall_quality if quality_evaluated else "Not Evaluated",
            
            # === EVALUATION STATUS ===
            "quality_evaluated": quality_evaluated,
            "evaluation_note": evaluation_note,
            
            # === TIMESTAMP ===
            "inference_timestamp": inference_timestamp
        }

        print("")
        print("Inference row prepared:")
        print(f"  inference_id: {inference_row['inference_id']}")
        print(f"  algorithm: {inference_row['algorithm']}")
        print(f"  total_samples: {inference_row['total_samples']}")
        print(f"  quality_evaluated: {inference_row['quality_evaluated']}")

        # Set up API call
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {bearer_auth_token}'
        }

        retry_strategy = Retry(
            total=3,
            status_forcelist=[408, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "PUT", "POST", "DELETE", "OPTIONS", "TRACE"],
            backoff_factor=1
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        http = requests.Session()
        http.mount("https://", adapter)
        http.mount("http://", adapter)

        # Check if row exists (using primary key)
        check_url = f"{args.domain}/pi-entity-instances-service/v3.0/schemas/{args.schema_id}/instances/list"
        check_payload = {
            "dbType": "TIDB",
            "ownedOnly": True,
            "filter": {
                "inference_id": inference_id,
                "execution_id": execution_id_int,
                "projectId": args.project_id
            }
        }

        print("")
        print(f"Checking for existing inference record...")
        
        try:
            response = http.post(check_url, headers=headers, json=check_payload, timeout=60)
            response.raise_for_status()
            response_data = response.json()
            
            # Since inference_id is timestamp-based, it should always be new
            # But we check anyway
            if response_data.get("content"):
                print("Inference record found: Updating row")
                update_url = f"{args.domain}/pi-entity-instances-service/v2.0/schemas/{args.schema_id}/instances"
                update_payload = {
                    "dbType": "TIDB",
                    "conditionalFilter": {
                        "conditions": [
                            {"field": "inference_id", "operator": "EQUAL", "value": inference_id},
                            {"field": "execution_id", "operator": "EQUAL", "value": execution_id_int},
                            {"field": "projectId", "operator": "EQUAL", "value": args.project_id}
                        ]
                    },
                    "partialUpdateRequests": [{
                        "patch": [
                            {"operation": "REPLACE", "path": "algorithm", "value": inference_row["algorithm"]},
                            {"operation": "REPLACE", "path": "total_samples", "value": inference_row["total_samples"]},
                            {"operation": "REPLACE", "path": "cluster_labels", "value": inference_row["cluster_labels"]},
                            {"operation": "REPLACE", "path": "overall_quality", "value": inference_row["overall_quality"]}
                        ]
                    }]
                }
                
                response = http.patch(update_url, headers=headers, json=update_payload, timeout=60)
                response.raise_for_status()
                print("Successfully updated inference record")
            else:
                print("No inference record found: Creating new row")
                create_url = f"{args.domain}/pi-entity-instances-service/v2.0/schemas/{args.schema_id}/instances"
                create_payload = {"data": [inference_row]}
                
                response = http.post(create_url, headers=headers, json=create_payload, timeout=60)
                response.raise_for_status()
                print("Successfully created new inference record")
            
            print(f"Response: {response.json()}")
            
            # Write success confirmation
            with open(args.inference_schema_updated, 'w') as f:
                json.dump({
                    "status": "success",
                    "message": "Inference schema updated successfully",
                    "inference_id": inference_id,
                    "algorithm": algorithm,
                    "samples": n_samples
                }, f)

            print("")
            print("="*80)
            print("INFERENCE SCHEMA UPDATE COMPLETED")
            print("="*80)

        except requests.exceptions.RequestException as e:
            print(f"Error: {e}")
            if e.response is not None:
                print(f"Response Status Code: {e.response.status_code}")
                print(f"Response Content: {e.response.text}")
            
            with open(args.inference_schema_updated, 'w') as f:
                json.dump({
                    "status": "error",
                    "message": str(e),
                    "row_data": inference_row
                }, f)
            exit(1)

    args:
      - --schema_id
      - {inputValue: schema_id}
      - --predictions
      - {inputPath: predictions}  # ✅ CRITICAL: Must be inputPath (not inputValue)
      - --model_id
      - {inputValue: model_id}
      - --execution_id
      - {inputValue: execution_id}
      - --tenant_id
      - {inputPath: tenant_id}  # ✅ inputPath
      - --project_id
      - {inputValue: project_id}
      - --bearer_auth_token
      - {inputPath: bearer_auth_token}  # ✅ inputPath
      - --domain
      - {inputValue: domain}
      - --inference_schema_updated
      - {outputPath: inference_schema_updated}
