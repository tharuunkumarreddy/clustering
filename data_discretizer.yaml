name: Generic Discretization Component
description: Comprehensive discretization (binning) for clustering with 4 methods. Converts continuous features into discrete bins using equal-width, equal-frequency, K-means, or custom binning. Supports ordinal and one-hot encoding. Optional column selection.

inputs:
  - name: input_data
    type: Data
    description: 'Input dataset (CSV or Parquet)'
  - name: method
    type: String
    description: 'Discretization method: equal_width, uniform, equal_frequency, quantile, kmeans, custom, none'
    default: 'equal_width'
  - name: method_params
    type: String
    description: 'Method-specific parameters as JSON string. Examples: {"n_bins":5,"encode":"ordinal"}, {"columns":["col1","col2"]}'
    default: '{}'

outputs:
  - name: data
    type: Data
    description: 'Discretized dataset (CSV format)'
  - name: discretizer
    type: Model
    description: 'Fitted discretizer for test data transformation (PKL format)'
  - name: metadata
    type: Data
    description: 'Discretization metadata including bin edges (JSON format)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pickle
        import pandas as pd
        import numpy as np
        from sklearn.preprocessing import KBinsDiscretizer
        from pathlib import Path
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        logger = logging.getLogger('discretizer')
        
        
        def ensure_directory_exists(file_path):
            # Create directory if it doesn't exist
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
                logger.info(f"Created directory: {directory}")
        
        
        def load_data(input_path):
            # Load data from CSV or Parquet file
            logger.info(f"Loading dataset from: {input_path}")
            
            ext = Path(input_path).suffix.lower()
            
            try:
                if ext in ['.parquet', '.pq']:
                    df = pd.read_parquet(input_path)
                    logger.info("Loaded Parquet file")
                else:
                    df = pd.read_csv(input_path)
                    logger.info("Loaded CSV file")
                
                logger.info(f"Shape: {df.shape[0]} rows x {df.shape[1]} columns")
                return df
                
            except Exception as e:
                logger.error(f"Error loading data: {str(e)}")
                raise
        
        
        def apply_discretization(df, method, method_params):
            # Apply discretization (binning) to numeric columns
            # Parameters: df, method, method_params
            # Returns: df_discretized, discretizer object, metadata dict
            logger.info("="*80)
            logger.info("APPLYING DISCRETIZATION")
            logger.info("="*80)
            logger.info(f"Method: {method}")
            
            if method_params:
                logger.info(f"Parameters: {method_params}")
            
            logger.info("")
            
            df_discretized = df.copy()
            
            # Get parameters
            n_bins = method_params.get('n_bins', 5)
            encode = method_params.get('encode', 'ordinal')
            columns = method_params.get('columns', None)
            
            # Determine columns to discretize
            if columns is None:
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                logger.info("Auto-selecting all numeric columns")
            else:
                # Validate specified columns exist
                numeric_cols = [col for col in columns if col in df.columns]
                missing_cols = [col for col in columns if col not in df.columns]
                
                if missing_cols:
                    logger.warning(f"Specified columns not found: {missing_cols}")
                
                if not numeric_cols:
                    logger.warning("No valid columns to discretize")
                else:
                    logger.info(f"Using specified columns: {numeric_cols}")
            
            if len(numeric_cols) == 0:
                logger.warning("No numeric columns to discretize")
                return df_discretized, None, {
                    'method': 'none',
                    'reason': 'no_numeric_columns',
                    'n_bins': 0
                }
            
            logger.info(f"Discretizing {len(numeric_cols)} columns into {n_bins} bins")
            logger.info(f"Encoding: {encode}")
            logger.info("")
            
            metadata = {
                'method': method,
                'params': method_params,
                'n_bins': int(n_bins),
                'encode': encode,
                'discretized_columns': numeric_cols,
                'n_discretized_columns': len(numeric_cols),
                'bin_edges': {}
            }
            
            discretizer = None
            
            try:
                # METHOD 1: Equal-Width Binning (Uniform)
                if method in ['equal_width', 'uniform']:
                    discretizer = KBinsDiscretizer(
                        n_bins=n_bins,
                        encode=encode,
                        strategy='uniform'
                    )
                    
                    transformed = discretizer.fit_transform(df[numeric_cols])
                    
                    if encode == 'ordinal':
                        df_discretized[numeric_cols] = transformed
                        logger.info("Ordinal encoding: replacing original columns")
                    elif encode == 'onehot-dense':
                        # Create new columns for one-hot encoded bins
                        feature_names = []
                        for i, col in enumerate(numeric_cols):
                            for bin_idx in range(n_bins):
                                feature_names.append(f"{col}_bin_{bin_idx}")
                        
                        # Remove original numeric columns
                        df_discretized = df_discretized.drop(columns=numeric_cols)
                        
                        # Add one-hot encoded columns
                        df_onehot = pd.DataFrame(
                            transformed,
                            columns=feature_names,
                            index=df.index
                        )
                        df_discretized = pd.concat([df_discretized, df_onehot], axis=1)
                        
                        metadata['onehot_columns'] = feature_names
                        logger.info(f"One-hot encoding: created {len(feature_names)} new columns")
                    
                    # Store bin edges
                    for i, col in enumerate(numeric_cols):
                        metadata['bin_edges'][col] = discretizer.bin_edges_[i].tolist()
                    
                    logger.info("Equal-width discretization completed")
                
                # METHOD 2: Equal-Frequency Binning (Quantile)
                elif method in ['equal_frequency', 'quantile']:
                    discretizer = KBinsDiscretizer(
                        n_bins=n_bins,
                        encode=encode,
                        strategy='quantile'
                    )
                    
                    transformed = discretizer.fit_transform(df[numeric_cols])
                    
                    if encode == 'ordinal':
                        df_discretized[numeric_cols] = transformed
                        logger.info("Ordinal encoding: replacing original columns")
                    elif encode == 'onehot-dense':
                        feature_names = []
                        for i, col in enumerate(numeric_cols):
                            for bin_idx in range(n_bins):
                                feature_names.append(f"{col}_bin_{bin_idx}")
                        
                        df_discretized = df_discretized.drop(columns=numeric_cols)
                        df_onehot = pd.DataFrame(
                            transformed,
                            columns=feature_names,
                            index=df.index
                        )
                        df_discretized = pd.concat([df_discretized, df_onehot], axis=1)
                        
                        metadata['onehot_columns'] = feature_names
                        logger.info(f"One-hot encoding: created {len(feature_names)} new columns")
                    
                    # Store bin edges
                    for i, col in enumerate(numeric_cols):
                        metadata['bin_edges'][col] = discretizer.bin_edges_[i].tolist()
                    
                    logger.info("Equal-frequency discretization completed")
                
                # METHOD 3: K-Means Binning
                elif method == 'kmeans':
                    discretizer = KBinsDiscretizer(
                        n_bins=n_bins,
                        encode=encode,
                        strategy='kmeans'
                    )
                    
                    transformed = discretizer.fit_transform(df[numeric_cols])
                    
                    if encode == 'ordinal':
                        df_discretized[numeric_cols] = transformed
                        logger.info("Ordinal encoding: replacing original columns")
                    elif encode == 'onehot-dense':
                        feature_names = []
                        for i, col in enumerate(numeric_cols):
                            for bin_idx in range(n_bins):
                                feature_names.append(f"{col}_bin_{bin_idx}")
                        
                        df_discretized = df_discretized.drop(columns=numeric_cols)
                        df_onehot = pd.DataFrame(
                            transformed,
                            columns=feature_names,
                            index=df.index
                        )
                        df_discretized = pd.concat([df_discretized, df_onehot], axis=1)
                        
                        metadata['onehot_columns'] = feature_names
                        logger.info(f"One-hot encoding: created {len(feature_names)} new columns")
                    
                    # Store bin edges
                    for i, col in enumerate(numeric_cols):
                        metadata['bin_edges'][col] = discretizer.bin_edges_[i].tolist()
                    
                    logger.info("K-Means discretization completed")
                
                # METHOD 4: Custom Binning (using pandas cut)
                elif method == 'custom':
                    logger.info("Using pandas cut for custom binning")
                    
                    for col in numeric_cols:
                        # Use pandas cut for binning
                        df_discretized[col], bin_edges = pd.cut(
                            df[col],
                            bins=n_bins,
                            labels=False,
                            retbins=True,
                            duplicates='drop'
                        )
                        
                        # Store bin edges
                        metadata['bin_edges'][col] = bin_edges.tolist()
                    
                    logger.info("Custom discretization completed")
                
                # METHOD 5: No Discretization
                elif method == 'none':
                    logger.info("No discretization applied")
                    return df_discretized, None, {
                        'method': 'none',
                        'n_bins': 0
                    }
                
                else:
                    raise ValueError(
                        f"Unknown discretization method: {method}. "
                        f"Available: equal_width, uniform, equal_frequency, quantile, "
                        f"kmeans, custom, none"
                    )
                
                # Calculate bin distribution statistics
                logger.info("")
                logger.info("Bin distribution per column:")
                for col in numeric_cols:
                    if col in df_discretized.columns:
                        value_counts = df_discretized[col].value_counts().sort_index()
                        metadata[f'{col}_bin_distribution'] = {
                            int(k): int(v) for k, v in value_counts.to_dict().items()
                        }
                        
                        logger.info(f"  {col}:")
                        for bin_idx, count in value_counts.items():
                            pct = count / len(df) * 100
                            logger.info(f"    Bin {int(bin_idx)}: {count} samples ({pct:.1f}%)")
                
                metadata['initial_shape'] = list(df.shape)
                metadata['final_shape'] = list(df_discretized.shape)
                
                logger.info("")
                logger.info(f"Initial shape: {tuple(metadata['initial_shape'])}")
                logger.info(f"Final shape: {tuple(metadata['final_shape'])}")
                
                if encode == 'onehot-dense':
                    logger.info(
                        f"Features expanded: {df.shape[1]} â†’ {df_discretized.shape[1]} "
                        f"({df_discretized.shape[1] - df.shape[1]:+d})"
                    )
                
                logger.info("")
                
                return df_discretized, discretizer, metadata
                
            except Exception as e:
                logger.error(f"Error during discretization: {str(e)}")
                raise
        
        
        def main():
            parser = argparse.ArgumentParser(
                description="Generic Discretization Component"
            )
            parser.add_argument("--input_data", required=True,
                               help="Path to input dataset")
            parser.add_argument("--method", default='equal_width',
                               help="Discretization method")
            parser.add_argument("--method_params", type=str, default='{}',
                               help="Method-specific parameters as JSON")
            parser.add_argument("--output_data", required=True,
                               help="Output path for discretized dataset")
            parser.add_argument("--output_discretizer", required=True,
                               help="Output path for fitted discretizer")
            parser.add_argument("--output_metadata", required=True,
                               help="Output path for discretization metadata")
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("DISCRETIZATION COMPONENT")
            logger.info("="*80)
            logger.info(f"Input: {args.input_data}")
            logger.info(f"Method: {args.method}")
            logger.info("")
            
            try:
                # Ensure output directories
                ensure_directory_exists(args.output_data)
                ensure_directory_exists(args.output_discretizer)
                ensure_directory_exists(args.output_metadata)
                
                # Parse method parameters
                try:
                    method_params = json.loads(args.method_params)
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON in method_params: {e}")
                    logger.error(f"Received: {args.method_params}")
                    logger.error("Expected format: '{\"param\": value}' (use double quotes)")
                    sys.exit(1)
                
                # Load data
                df = load_data(args.input_data)
                
                if df.empty:
                    logger.error("ERROR: Dataset is empty")
                    sys.exit(1)
                
                # Apply discretization
                df_discretized, discretizer, metadata = apply_discretization(
                    df=df,
                    method=args.method,
                    method_params=method_params
                )
                
                # Save discretized data
                df_discretized.to_csv(args.output_data, index=False)
                logger.info(f"Discretized data saved: {args.output_data}")
                
                # Save discretizer
                with open(args.output_discretizer, 'wb') as f:
                    pickle.dump(discretizer, f)
                logger.info(f"Discretizer saved: {args.output_discretizer}")
                
                # Save metadata
                with open(args.output_metadata, 'w') as f:
                    json.dump(metadata, f, indent=2)
                logger.info(f"Metadata saved: {args.output_metadata}")
                
                logger.info("")
                logger.info("="*80)
                logger.info("DISCRETIZATION COMPLETED")
                logger.info("="*80)
                logger.info(f"Method: {args.method}")
                
                if 'n_bins' in metadata and metadata['n_bins'] > 0:
                    logger.info(f"Bins: {metadata['n_bins']}")
                    logger.info(f"Columns discretized: {metadata['n_discretized_columns']}")
                
                if 'final_shape' in metadata:
                    logger.info(f"Final shape: {tuple(metadata['final_shape'])}")
                
                logger.info("="*80)
                
            except ValueError as e:
                logger.error(f"VALIDATION ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
            except Exception as e:
                logger.error(f"ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        
        if __name__ == "__main__":
            main()
    args:
      - --input_data
      - {inputPath: input_data}
      - --method
      - {inputValue: method}
      - --method_params
      - {inputValue: method_params}
      - --output_data
      - {outputPath: data}
      - --output_discretizer
      - {outputPath: discretizer}
      - --output_metadata
      - {outputPath: metadata}
